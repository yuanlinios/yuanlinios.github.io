<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Lin&#39;s Notes</title>
  
  <subtitle>All About IT Infra and Cloud</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://notes.yuanlinios.me/"/>
  <updated>2020-05-30T09:07:00.666Z</updated>
  <id>https://notes.yuanlinios.me/</id>
  
  <author>
    <name>L. Yuan</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>使用 tlog 在 CentOS 7 上实施 ssh 会话记录</title>
    <link href="https://notes.yuanlinios.me/2020-05-29/%E4%BD%BF%E7%94%A8-tlog-%E5%9C%A8-CentOS-7-%E4%B8%8A%E5%AE%9E%E6%96%BD-ssh-%E4%BC%9A%E8%AF%9D%E8%AE%B0%E5%BD%95/"/>
    <id>https://notes.yuanlinios.me/2020-05-29/%E4%BD%BF%E7%94%A8-tlog-%E5%9C%A8-CentOS-7-%E4%B8%8A%E5%AE%9E%E6%96%BD-ssh-%E4%BC%9A%E8%AF%9D%E8%AE%B0%E5%BD%95/</id>
    <published>2020-05-29T16:24:15.000Z</published>
    <updated>2020-05-30T09:07:00.666Z</updated>
    
    <content type="html"><![CDATA[<p>Native Session Recording 是 RHEL 8 / CentOS 8 引入的新功能, 可以方便的录制用户会话用以审计. 按照<a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/recording_sessions/index" target="_blank" rel="external nofollow noopener noreferrer">官方文档</a>的步骤, 在 RHEL 8 或者 CentOS 8 上配置非常容易. 由于目前我的大部分服务器还是跑在 CentOS 7 上, 很自然的想能否将这套方案在 CentOS 7 上部署出来. 原本以为很容易, 实际做下来才发现坑不少. 这里做一个简要的记录 <a id="more"></a></p><h2 id="tlog-会话记录的组件和原理">tlog 会话记录的组件和原理</h2><p>之前也有不少开源工具可以提供 ssh 会话录制的功能, 比如 script + scriptreplay. 不过也就是玩具的水平, 要真正生产可用还需要大量的 DIY 脚本来辅助. 而 tlog 算是一个相对比较成熟的开源方案了: 使用简便, 搭配 cockpit 可以方便的管理会话记录, 针对重放也提供了丰富的控制.</p><h3 id="基本原理">基本原理</h3><p>核心组件是 tlog + sssd</p><ul><li>用户通过 pam 机制登入</li><li>sssd-session-recording 会将 nss response 中的用户 shell 替换为 tlog-rec-session</li><li>启动 tlog-rec-session</li><li>tlog-rec-session 通过配置文件获取用户的原始 shell</li><li>tlog-rec-session 启动用户的原始 shell</li><li>tlog-rec-session 位于 user terminal 和 user shell 中间, 记录所有通过的数据, 发送给 journal</li></ul><h3 id="web-ui">Web UI</h3><p>cockpit-session-recording 提供一个 web 界面来管理记录的会话, 并提供一个简易的回放播放器.</p><p>针对记录会话功能, Web UI 并不是必须的组件. 然而直接通过命令行 <code>journalctl -o verbose | grep -i '"rec"'</code> 来过滤感兴趣会话, 体验并不是很友好</p><h2 id="编译-rpm-包">编译 rpm 包</h2><p>CentOS 7 的源默认不要带 tlog, 可以使用 tar ball 源码编译为 rpm. Cockpit 软件包有一部分, 但是不包含 cockpit-session-recording 组件, 因此也需要单独编译 rpm</p><p>当前 tlog 最新版本为 v8-2, cockpit-session-recording 最新版本为 v4</p><h3 id="tlog">tlog</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">wget -c https://github.com/Scribery/tlog/releases/download/v8-2/tlog-8.tar.gz</span><br><span class="line">yum install -y gcc systemd-devel json-c-devel libcurl-devel rpm-build autoconf automake libtool</span><br><span class="line">rpmbuild -tb tlog-8.tar.gz</span><br></pre></td></tr></table></figure><h3 id="cockpit-session-recording">cockpit-session-recording</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">wget -c https://github.com/Scribery/cockpit-session-recording/releases/download/4/cockpit-session-recording-4.tar.gz</span><br><span class="line">yum install -y libappstream-glib-devel</span><br><span class="line">rpmbuild -tb cockpit-session-recording-4.tar.gz</span><br></pre></td></tr></table></figure><h2 id="方案部署">方案部署</h2><h3 id="配置持久化-journal">配置持久化 journal</h3><p>默认 tlog 将记录的会话保存在 journal 中 (也支持发送给 Elasticsearch). 如果不存在 /var/log/journal 目录, 则 journal 将保存在 /run/log/journal 下, 为非持久存储, 重启丢失. 因此首先需要将 journal 保存到持久存储:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /var/<span class="built_in">log</span>/journal /etc/systemd/journald.conf.d</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; /etc/systemd/journald.conf.d/99-persistent.conf</span><br><span class="line">[Journal]</span><br><span class="line">Storage=persistent</span><br><span class="line">Compress=yes</span><br><span class="line">SyncIntervalSec=5m</span><br><span class="line">RateLimitInterval=30s</span><br><span class="line">RateLimitBurst=1000</span><br><span class="line">SystemMaxUse=10G</span><br><span class="line">SystemMaxFileSize=200M</span><br><span class="line">MaxRetentionSec=7week</span><br><span class="line">ForwardToSyslog=no</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl restart systemd-journald.service</span><br></pre></td></tr></table></figure><h3 id="安装-tlog-和-cockpit-session-recording">安装 tlog 和 cockpit-session-recording</h3><p>直接安装前面编译出来的 rpm 即可 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">yum install -y cockpit</span><br><span class="line">yum localinstall -y tlog-8-2.el7.x86_64.rpm cockpit-session-recording-4-1.el7.noarch.rpm</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> --now cockpit.socket</span><br></pre></td></tr></table></figure></p><h3 id="配置-sssd">配置 sssd</h3><p>CentOS 7 使用 1.16.x 的 sssd, 而 RHEL 8/CentOS 8 使用 2.2.x 版本. 部分行为有所差别.</p><p>2.2.x 版本的 sssd 使了 <code>--enable-files-domain</code> 编译参数, 允许在没有 /etc/sssd/sssd.conf 时启动, 而 1.16.x 使用了 <code>--disable-files-domain</code>, 要求 /etc/sssd/sssd.conf 必须存在.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">yum install sssd-common sssd-client -y</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; /etc/sssd/sssd.conf</span><br><span class="line">[sssd]</span><br><span class="line">services = nss, pam</span><br><span class="line">domains = <span class="built_in">local</span></span><br><span class="line"></span><br><span class="line">[nss]</span><br><span class="line"></span><br><span class="line">[pam]</span><br><span class="line"></span><br><span class="line">[domain/<span class="built_in">local</span>]</span><br><span class="line">id_provider = files</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># pay attention to the permission</span></span><br><span class="line">chmod 600 /etc/sssd/sssd.conf</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; /etc/sssd/conf.d/sssd-session-recording.conf</span><br><span class="line">[session_recording]</span><br><span class="line">scope=all</span><br><span class="line">users=</span><br><span class="line">groups=</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># pay attention to the permission</span></span><br><span class="line">chmod 600 /etc/sssd/conf.d/sssd-session-recording.conf</span><br><span class="line">systemctl restart sssd</span><br></pre></td></tr></table></figure><h3 id="修正-nsswitch">修正 nsswitch</h3><p>CentOS 8 / RHEL 8 的 /etc/nsswitch.conf 相对 CentOS 7 / RHEL 7 发生了细微的变化: 对 passwd 和 group, sss 排到了 files 前面 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">passwd:     sss files</span><br><span class="line">group:      sss files</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p><p>这里也需要把 passwd 和 group 条目中的 sss 排到 files 前面, 否则用户登录时会话记录无法生效</p><p>用下面命令检查一个允许登录的本地账号, 确认设置生效. 命令显示的用户 login shell 应该是 /usr/bin/tlog-rec-session, 虽然在 /etc/passwd 中是 /bin/bash</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">getent passwd alice</span><br><span class="line">getent passwd -s sss alice</span><br></pre></td></tr></table></figure><h3 id="测试会话记录">测试会话记录</h3><p>用本地账号 (alice) 通过 ssh 登录系统确认会话记录已经生效 <img src="alice-ssh-session-recorded.png"></p><p>进行一些操作后, 退出登录. 通过 cockpit 界面查看记录的会话 <img src="alice-session-cockpit.png"></p><p>测试重放 <img src="alice-session-replay-web.png"></p><h3 id="测试本地命令行回放">测试本地命令行回放</h3><p>从 cockpit 界面上选择一个保存的会话, 记录其 ID. 或者通过 <code>journalctl -o verbose | grep -i '"rec"'</code>来找.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">REC_ID=2f84bc33bce0486394706de9be91e72e-965-a3cc</span><br><span class="line">tlog-play -r journal -M TLOG_REC=<span class="variable">$REC_ID</span></span><br></pre></td></tr></table></figure><h3 id="测试异地命令行回放">测试异地命令行回放</h3><p>需要通过 systemd-journal-remote 命令将 journal 导出为文件, 复制到目标机器上. 在 CentOS 7 中 systemd-journal-remote 命令由 systemd-journal-gateway 包提供</p><p>从源服务器导出 journal <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">yum -y install systemd-journal-gateway</span><br><span class="line"></span><br><span class="line">REC_ID=2f84bc33bce0486394706de9be91e72e-965-a3cc</span><br><span class="line">journalctl -o <span class="built_in">export</span> TLOG_REC=<span class="variable">$REC_ID</span> | /usr/lib/systemd/systemd-journal-remote -o /tmp/mysession.journal -</span><br></pre></td></tr></table></figure></p><p>将 /tmp/mysession.journal 复制到到目标服务器, 需要放到 /var/log/journal 下. 在目标服务器上实现重放只需要安装 tlog <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">yum localinstall -y tlog-8-2.el7.x86_64.rpm</span><br><span class="line"></span><br><span class="line">mkdir -p /var/<span class="built_in">log</span>/journal</span><br><span class="line">mv /tmp/mysession.journal /var/<span class="built_in">log</span>/journal/</span><br><span class="line"></span><br><span class="line">REC_ID=2f84bc33bce0486394706de9be91e72e-965-a3cc</span><br><span class="line">tlog-play -r journal --file-path=mysession.journal -M TLOG_REC=<span class="variable">$REC_ID</span></span><br></pre></td></tr></table></figure> <img src="alice-session-remote-replay.png"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Native Session Recording 是 RHEL 8 / CentOS 8 引入的新功能, 可以方便的录制用户会话用以审计. 按照&lt;a href=&quot;https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/recording_sessions/index&quot; target=&quot;_blank&quot; rel=&quot;external nofollow noopener noreferrer&quot;&gt;官方文档&lt;/a&gt;的步骤, 在 RHEL 8 或者 CentOS 8 上配置非常容易. 由于目前我的大部分服务器还是跑在 CentOS 7 上, 很自然的想能否将这套方案在 CentOS 7 上部署出来. 原本以为很容易, 实际做下来才发现坑不少. 这里做一个简要的记录
    
    </summary>
    
    
      <category term="CentOS" scheme="https://notes.yuanlinios.me/categories/CentOS/"/>
    
      <category term="Administration" scheme="https://notes.yuanlinios.me/categories/CentOS/Administration/"/>
    
    
      <category term="Linux" scheme="https://notes.yuanlinios.me/tags/Linux/"/>
    
      <category term="CentOS" scheme="https://notes.yuanlinios.me/tags/CentOS/"/>
    
  </entry>
  
  <entry>
    <title>定制 Openshift 3.11 监控系统 Cluster Monitoring Operator</title>
    <link href="https://notes.yuanlinios.me/2020-05-07/%E5%AE%9A%E5%88%B6-Openshift-3-11-%E7%9B%91%E6%8E%A7%E7%B3%BB%E7%BB%9F-Cluster-Monitoring-Operator/"/>
    <id>https://notes.yuanlinios.me/2020-05-07/%E5%AE%9A%E5%88%B6-Openshift-3-11-%E7%9B%91%E6%8E%A7%E7%B3%BB%E7%BB%9F-Cluster-Monitoring-Operator/</id>
    <published>2020-05-07T17:03:49.000Z</published>
    <updated>2020-05-14T13:40:25.555Z</updated>
    
    <content type="html"><![CDATA[<p>本文介绍如何对 Openshift (OCP/OKD) 3.11 的监控系统 cluster monitoring operator 进行简单定制, 扩展平台监控的范围. <a id="more"></a></p><h2 id="cluster-monitoring-operator-介绍">Cluster Monitoring Operator 介绍</h2><p>Openshift 3.11 以及后续的 4.x 平台都使用预定义配置的 cluster monitoring operator 来做平台监控. Cluster monitoring operator 管理 prometheus operator, grafana, kube-state-metrics 和 node-exporter. 而 prometheus operator 再负责管理 prometheus 和 alertmanager. 各组件之间的关系如下图所示:</p><p><img src="cluster-monitoring-operator.png"></p><p>通过检查 prometheus 的配置可以发现其 scrape 对象包括</p><ul><li>prometheus</li><li>prometheus operator</li><li>cluster monitoring operator</li><li>alertmanager</li><li>kubernete apiserver</li><li>kubelet</li><li>kube-controller</li><li>kube-state-metrics</li><li>node-exporter</li><li>etcd (可选择启用)</li></ul><p>对于一个中小型 Openshift 集群的平台层面 (不包含应用特定 metrics), 用这些对象的 metrics 来做平台监控其实也差不多够用了. 然而 Openshift 监控套件的预定义配置并没有覆盖非系统级的 namespace.</p><p>举个例子, 我的应用 pod 跑在 myproj project/namespace 下. 虽然 kube-state-metrics 会收集 myproj 名字空间下资源 (deployment/statefulset/pod) 的状态信息, 但是预定义的 alert rules 只选取 <code>openshift-.\*|kube-.\*|default|logging</code> 这些 namespace 的 metrics 来进行评估. 导致的结果是: 虽然收集了应用 namespace 下的资源状态信息, 却无法实现告警.</p><p>遗憾的是 Openshift 并不支持用户扩展其平台监控套装的监控范围. 文档原文如下</p><blockquote><p>Explicitly unsupported cases include:</p><ul><li>Creating additional ServiceMonitor objects in the openshift-monitoring namespace, thereby extending the targets the cluster monitoring Prometheus instance scrapes. This can cause collisions and load differences that cannot be accounted for, therefore the Prometheus setup can be unstable.</li><li>Creating additional ConfigMap objects, that cause the cluster monitoring Prometheus instance to include additional alerting and recording rules. Note that this behavior is known to cause a breaking behavior if applied, as Prometheus 2.0 will ship with a new rule file syntax.</li></ul></blockquote><p>如果想让监控覆盖到用户的 namespace, RedHat 给出的建议是另外单独部署 prometheus 套件. 对于大型集群, 这可能是一个合理的建议. 但是对于一个小型集群, 如果本身就只有几个用户级别的 namespace, 总共十几或几十个 pod, 你让我再单独来一套 prometheus? 实在是很蛋疼.</p><p>如果你使用的是订阅版本的 Openshift Container Platform, 为了避免维保/技术支持的麻烦, 请遵循 RedHat 的建议. 如果你使用的是社区版本的 OKD, 可以继续往下看: 如何改造 cluster monitoring operator 来满足定制化需求.</p><h2 id="定制化方案">定制化方案</h2><h3 id="需求">需求</h3><p>下面这些应该是非常基本的定制化需求</p><ul><li>调整部分默认告警规则的覆盖范围. 例如 KubePodNotReady, 除了覆盖系统级别的 namespace, 还需要覆盖所有应用 namespace</li><li>调整部分默认告警规则的触发状态持续时间, 像 KubePodCrashLooping, KubeNodeNotReady 默认设置状态持续 1h 才会触发告警, 黄花菜都凉了吧</li><li>调整默认 target scrape interval. 例如调整 kube-state-metrics 的刮取间隔为 1m</li><li>添加自定义告警规则</li><li>添加自定义 service monitor 来包含用户应用</li></ul><p>再次吐槽 RedHat 的产品, 经常弄一些跟玩具一样的东西出来, 性能/稳定性/扩展性/可定制性上有诸多限制. 类似的还有 Openshift 平台的 EFK 日志系统.</p><h3 id="添加自定义配置">添加自定义配置</h3><p>上面需求中的第四和第五项属于添加自定义配置, 比较容易实现.</p><p>Prometheus 规则和监控 target 配置都是通过自定义资源 (Custom Resource) 来管理</p><ul><li>prometheusrules.monitoring.coreos.com 类型资源 针对 prometheus rules</li><li>servicemonitors.monitoring.coreos.com 类型资源针对 target 配置</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">oc get prometheusrules.monitoring.coreos.com</span><br><span class="line">NAME                   AGE</span><br><span class="line">prometheus-k8s-rules   1d</span><br><span class="line"></span><br><span class="line">oc get servicemonitors.monitoring.coreos.com</span><br><span class="line">NAME                          AGE</span><br><span class="line">alertmanager                  1d</span><br><span class="line">cluster-monitoring-operator   1d</span><br><span class="line">kube-apiserver                1d</span><br><span class="line">kube-controllers              1d</span><br><span class="line">kube-state-metrics            1d</span><br><span class="line">kubelet                       1d</span><br><span class="line">node-exporter                 1d</span><br><span class="line">prometheus                    1d</span><br><span class="line">prometheus-operator           1d</span><br></pre></td></tr></table></figure><p>如果需要添加自定义规则和自定义监控目标, 创建相应的 CR 即可. 不过要注意, 不要试图修改默认 CR 来实现自定义配置, 具体原因后面会分析.</p><h3 id="修改默认配置">修改默认配置</h3><p>需求中的 1-3 项属于修改默认配置. 如果尝试修改默认 CR, 你会发现你的更改会被 cluster monitoring operator 自动重置. 一个很自然的想法是能不能重新构建 cluster monitoring operator 镜像, 覆盖掉这些默认 CR 的配置文件来实现需求?</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取 cluster monitoring operator 源代码</span></span><br><span class="line">go get -u github.com/openshift/cluster-monitoring-operator/...</span><br><span class="line"><span class="built_in">cd</span> <span class="variable">$GOPATH</span>/src/github.com/openshift/cluster-monitoring-operator/</span><br><span class="line">git checkout release-3.11</span><br></pre></td></tr></table></figure><p>简单检查一下 cluster monitoring operator 的源码就可以发现其实并不可行, 原因是这些预定义配置文件会被 go-bindata 用来生成 go 代码, 最终会成为 operator binary 的一部分. 即这些配置文件 "固化" 在二进制文件中了. 我猜想这也是 RedHat 不提供针对 cluster monitoring operator 定制化配置的原因之一.</p><p>查看<code>Makefile</code>可以大致了解: <code>${ASSERTS}</code> 这个目标会执行 <code>./hack/build-jsonnet.sh</code>来产生所有预定义配置, 而 <code>pkg/manifests/bindata.go</code> 这个目标会利用预定义配置生成 go 代码. 因此只需要修改 <code>./hack/build-jsonnet.sh</code> 脚本替换预定义配置即可实现需求.</p><p>在<code>hack</code>目录下建立路径来存放需要定制的配置文件. 把预定义文件复制过来, 然后按照需要修改 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p hack/customization/assets/&#123;prometheus-k8s,kube-state-metrics&#125;</span><br><span class="line">cp assets/prometheus-k8s/rules.yaml hack/customization/assets/prometheus-k8s/rules.yaml</span><br><span class="line">cp assets/kube-state-metrics/service-monitor.yaml hack/customization/assets/kube-state-metrics/service-monitor.yaml</span><br></pre></td></tr></table></figure></p><p>需求三修改 kube-state-metrics target 的刮取时间间隔需要修改<code>service-monitor.yaml</code>的 interval 字段.</p><p>需求一和二修改默认告警规则触发的状态持续时间和 namespace 覆盖范围需要修改<code>rules.yaml</code>中 alert rule 的定义. 例如我调整了下面告警的状态持续时间, 并且将原来针对系统级 namepace 的过滤条件删除:</p><ul><li>kube-state-metrics job 相关<ul><li>KubePodCrashLooping</li><li>KubePodNotReady</li><li>KubeDeploymentGenerationMismatch</li><li>KubeDeploymentReplicasMismatch</li><li>KubeStatefulSetReplicasMismatch</li><li>KubeStatefulSetGenerationMismatch</li><li>KubeDaemonSetRolloutStuck</li><li>KubeDaemonSetNotScheduled</li><li>KubeDaemonSetMisScheduled</li><li>KubeCPUOvercommit</li><li>KubeMemOvercommit</li><li>KubeQuotaExceeded</li><li>KubeNodeNotReady</li></ul></li><li>kubelet job 相关<ul><li>KubeletDown</li></ul></li><li>node-export job 相关<ul><li>NodeExportDown</li></ul></li><li>其他<ul><li>TargetDown</li></ul></li></ul><p>然后修改<code>hack/build-jsonnet.sh</code>文件, 添加脚本代码用自定义配置替换默认配置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># hack: my customized files</span></span><br><span class="line">cat hack/customization/assets/prometheus-k8s/rules.yaml &gt; assets/prometheus-k8s/rules.yaml</span><br><span class="line">cat hack/customization/assets/kube-state-metrics/service-monitor.yaml &gt; assets/kube-state-metrics/service-monitor.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># These manifests are generated by kube-prmoetheus, but are not necessary in</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure><h3 id="构建定制化镜像">构建定制化镜像</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># build operator binary</span></span><br><span class="line">make generate</span><br><span class="line">make crossbuild</span><br><span class="line"></span><br><span class="line"><span class="comment"># build image and push</span></span><br><span class="line">docker build -t quay.svc.vopsdev.com/openshift3/ose-cluster-monitoring-operator:v3.11-rev1 .</span><br><span class="line">docker push quay.svc.vopsdev.com/openshift3/ose-cluster-monitoring-operator:v3.11-rev1</span><br></pre></td></tr></table></figure><h2 id="测试">测试</h2><p>在 inventory 文件中通过<code>openshift_cluster_monitoring_operator_image</code>参数指定定制镜像 quay.svc.vopsdev.com/openshift3/ose-cluster-monitoring-operator:v3.11-rev1. 重新部署集群监控套装. 部署完成后, 创建 CR 添加自定义告警规则和监控对象 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ansible-playbook -i inventory playbooks/openshift-monitoring/config.yml</span><br><span class="line"></span><br><span class="line">oc apply -f customized-servicemonitor.yaml -n openshift-monitoring</span><br><span class="line">oc apply -f customized-rules.yaml -n openshift-monitoring</span><br></pre></td></tr></table></figure></p><p>登录 prometheus 界面确认自定义配置已经生效</p><ol type="1"><li><p>调整 kube-state-metrics 的 metrics 收集周期: <img src="kube-state-metrics-interval.png"></p></li><li><p>扩展默认告警规则到所有 namespace 并调整持续时间: <img src="alert-rule.png"></p></li><li><p>自定义 target 配置: <img src="custom-target.png"></p></li><li><p>自定义告警规则 <img src="custom-alert-rule.png"></p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文介绍如何对 Openshift (OCP/OKD) 3.11 的监控系统 cluster monitoring operator 进行简单定制, 扩展平台监控的范围.
    
    </summary>
    
    
      <category term="RedHat" scheme="https://notes.yuanlinios.me/categories/RedHat/"/>
    
      <category term="Openshift" scheme="https://notes.yuanlinios.me/categories/RedHat/Openshift/"/>
    
    
      <category term="RedHat" scheme="https://notes.yuanlinios.me/tags/RedHat/"/>
    
      <category term="Openshift" scheme="https://notes.yuanlinios.me/tags/Openshift/"/>
    
  </entry>
  
  <entry>
    <title>离线部署 Openshift Container Platform 4.3 - 6: 配置持久存储</title>
    <link href="https://notes.yuanlinios.me/2020-03-21/%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B2-Openshift-Container-Platform-4-3-6-%E9%85%8D%E7%BD%AE%E6%8C%81%E4%B9%85%E5%AD%98%E5%82%A8/"/>
    <id>https://notes.yuanlinios.me/2020-03-21/%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B2-Openshift-Container-Platform-4-3-6-%E9%85%8D%E7%BD%AE%E6%8C%81%E4%B9%85%E5%AD%98%E5%82%A8/</id>
    <published>2020-03-21T11:58:14.000Z</published>
    <updated>2020-03-21T12:13:40.995Z</updated>
    
    <content type="html"><![CDATA[<p>本文为 OCP 平台配置动态持久存储, 使用 vSphere Volume 和 Ceph RBD CSI <a id="more"></a></p><h2 id="vsphere-volume">vSphere Volume</h2><p>如果前面配置都正确, vSphere Volume 此时已经可用. 目前系统里应该只有一种 storageclass: thin(default), 其 provisioner 为 kubernetes.io/vsphere-volume. 建一个 pod 测试一下 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看现有的 storageclass</span></span><br><span class="line">oc get sc</span><br><span class="line">NAME             PROVISIONER                    AGE</span><br><span class="line">thin (default)   kubernetes.io/vsphere-volume   2d19h</span><br><span class="line"></span><br><span class="line">oc project sandbox-1</span><br><span class="line"><span class="comment"># 我这里用的是私有仓库里的离线镜像</span></span><br><span class="line">oc apply -f - &lt;&lt;EOF</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: vsphere-volume-demo-pod</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">    - name: web-server</span><br><span class="line">      image: quay.svc.vopsdev.com/devinfra/nginx:latest</span><br><span class="line">      volumeMounts:</span><br><span class="line">        - name: www-data</span><br><span class="line">          mountPath: /var/lib/www/html</span><br><span class="line">  volumes:</span><br><span class="line">    - name: www-data</span><br><span class="line">      persistentVolumeClaim:</span><br><span class="line">        claimName: vsphere-demo-pvc</span><br><span class="line">        readOnly: <span class="literal">false</span></span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">metadata:</span><br><span class="line">  name: vsphere-demo-pvc</span><br><span class="line">spec:</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteOnce</span><br><span class="line">  volumeMode: Filesystem</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 1Gi</span><br><span class="line">  storageClassName: thin</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查一下结果是否正常</span></span><br><span class="line">oc get pod,pvc</span><br><span class="line">NAME                          READY   STATUS    RESTARTS   AGE</span><br><span class="line">pod/vsphere-volume-demo-pod   1/1     Running   0          20s</span><br><span class="line"></span><br><span class="line">NAME                                     STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE</span><br><span class="line">persistentvolumeclaim/vsphere-demo-pvc   Bound    pvc-ae6dbae7-bb1f-460c-87d8-79ccbc01173d   1Gi        RWO            thin           20s</span><br></pre></td></tr></table></figure></p><p>前往 vSphere Datastore 下查看产生的 vmdk <img src="ocp-vsphere-volume.png"></p><p>Provisioner 会在 install-config.yaml 文件中指定的 platform.vsphere.defaultDatastore 下创建 kubevols 目录, 并在此目录下创建 vmdk.</p><p>如果 pvc 出现长时间 pending, <code>oc describe</code> 提示</p><blockquote><p>Failed to provision volume with StorageClass "thin": folder 'xxx' not found</p></blockquote><p>请检查是否在 vSphere Datacenter 下创建了 metadata.name 对应的目录.</p><p>vSphere volume 的性能取决于你的 datastore 的性质.</p><h2 id="ceph-rbd">Ceph RBD</h2><div class="note info">            <p>此处仅仅展示 OCP 和现有 Ceph 集群通过 CSI 对接的配置过程. 如果你在裸机上部署 OCP 同时又有现存的 Ceph 集群, 可以考虑这种方式整合. 如果你的 OCP 本身跑在 vSphere 虚拟化平台上, 那还是请使用 vSphere Volume 以确保稳定和性能</p>          </div><p>原先在 OCP 3.11, Atomic Host 自带 ceph-common 工具可以访问 Ceph 集群. 但是到了 OCP 4, RHCOS 上不再提供 ceph-common, 此时需要通过 CSI 来和现有的 Ceph 集群对接.</p><h3 id="ceph-端准备工作">Ceph 端准备工作</h3><p>现有的 Ceph 集群版本 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph version 14.2.4 (75f4de193b3ea58512f204623e6c5a16e6c1e1ba) nautilus (stable)</span><br></pre></td></tr></table></figure> 建一个 rbd pool 给 Openshift 使用, 同时创建 cephx 用户和密钥 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool create okd 32</span><br><span class="line">ceph osd pool application <span class="built_in">enable</span> okd rbd</span><br><span class="line">ceph auth get-or-create client.okd mon <span class="string">'profile rbd'</span> osd <span class="string">'profile rbd pool=okd'</span> -o ceph.client.okd.keyring</span><br></pre></td></tr></table></figure></p><h3 id="ocp-端">OCP 端</h3><p>下载 ceph-csi.git 根据实际环境信息配置后部署 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/ceph/ceph-csi.git</span><br><span class="line"><span class="built_in">cd</span> ceph-csi</span><br><span class="line"><span class="comment"># 当前最新 release 为 2.0.0</span></span><br><span class="line">git checkout v2.0.0</span><br><span class="line"><span class="built_in">cd</span> deploy/rbd/kubernetes</span><br><span class="line"></span><br><span class="line">oc new-project ceph-csi</span><br><span class="line"><span class="comment"># 参考 https://docs.ceph.com/docs/master/rbd/rbd-kubernetes/</span></span><br><span class="line"><span class="comment"># clusterID 可以使用 ceph mon dump 里的 fsid</span></span><br><span class="line">oc apply -f - &lt;&lt;EOF</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">data:</span><br><span class="line">  config.json: |-</span><br><span class="line">    [</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"clusterID"</span>: <span class="string">"2e44e736-d686-4c86-8cf5-ecb7d91ccf2b"</span>,</span><br><span class="line">            <span class="string">"monitors"</span>: [</span><br><span class="line">                <span class="string">"192.168.11.21:6789"</span>,</span><br><span class="line">                <span class="string">"192.168.11.22:6789"</span>,</span><br><span class="line">                <span class="string">"192.168.11.23:6789"</span></span><br><span class="line">            ]</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">metadata:</span><br><span class="line">  name: ceph-csi-config</span><br><span class="line">  namespace: ceph-csi</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 secret 保存 cephx keyring 认证方式的 user/key </span></span><br><span class="line">oc apply -f - &lt;&lt;EOF</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Secret</span><br><span class="line">metadata:</span><br><span class="line">  name: csi-rbd-secret</span><br><span class="line">  namespace: ceph-csi</span><br><span class="line">stringData:</span><br><span class="line">  userID: okd</span><br><span class="line">  userKey: xxx</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意调整 namespace</span></span><br><span class="line">oc apply -f csi-provisioner-rbac.yaml</span><br><span class="line">oc apply -f csi-nodeplugin-rbac.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调整 scc</span></span><br><span class="line">oc adm policy add-scc-to-user privileged system:serviceaccount:ceph-csi:rbd-csi-nodeplugin</span><br><span class="line">oc adm policy add-scc-to-user privileged system:serviceaccount:ceph-csi:rbd-csi-provisioner</span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意调整 namespace</span></span><br><span class="line"><span class="comment"># 如果是隔离环境, 需要把下面几个镜像离线到私有镜像仓库, 然后调整镜像的位置</span></span><br><span class="line"><span class="comment">#  quay.io/cephcsi/cephcsi:v2.0.0</span></span><br><span class="line"><span class="comment">#  quay.io/k8scsi/csi-provisioner:v1.4.0</span></span><br><span class="line"><span class="comment">#  quay.io/k8scsi/csi-snapshotter:v1.2.2</span></span><br><span class="line"><span class="comment">#  quay.io/k8scsi/csi-attacher:v2.1.0</span></span><br><span class="line"><span class="comment">#  quay.io/k8scsi/csi-resizer:v0.4.0</span></span><br><span class="line"><span class="comment">#  quay.io/k8scsi/csi-node-driver-registrar:v1.2.0</span></span><br><span class="line">oc apply -f csi-rbdplugin-provisioner.yaml</span><br><span class="line">oc apply -f csi-rbdplugin.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 确定 csi-rbdplugin 和 csi-rbdplugin-provisioner pod 都正常起来了</span></span><br><span class="line">oc get all</span><br><span class="line">NAME                                             READY   STATUS    RESTARTS   AGE</span><br><span class="line">pod/csi-rbdplugin-7qqtp                          3/3     Running   0          5m</span><br><span class="line">pod/csi-rbdplugin-f8grj                          3/3     Running   0          5m</span><br><span class="line">pod/csi-rbdplugin-provisioner-77497b775c-25624   6/6     Running   0          5m</span><br><span class="line">pod/csi-rbdplugin-provisioner-77497b775c-2crxf   6/6     Running   0          5m</span><br><span class="line">pod/csi-rbdplugin-provisioner-77497b775c-lhgtl   6/6     Running   0          5m</span><br><span class="line">pod/csi-rbdplugin-x59fc                          3/3     Running   0          5m</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p><h3 id="测试">测试</h3><p>先创建 storageclass</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">oc apply -f - &lt;&lt;EOF</span><br><span class="line">apiVersion: storage.k8s.io/v1</span><br><span class="line">kind: StorageClass</span><br><span class="line">metadata:</span><br><span class="line">   name: csi-rbd-sc</span><br><span class="line">provisioner: rbd.csi.ceph.com</span><br><span class="line">parameters:</span><br><span class="line">   clusterID: 2e44e736-d686-4c86-8cf5-ecb7d91ccf2b</span><br><span class="line">   pool: okd</span><br><span class="line">   csi.storage.k8s.io/provisioner-secret-name: csi-rbd-secret</span><br><span class="line">   csi.storage.k8s.io/provisioner-secret-namespace: ceph-csi</span><br><span class="line">   csi.storage.k8s.io/node-stage-secret-name: csi-rbd-secret</span><br><span class="line">   csi.storage.k8s.io/node-stage-secret-namespace: ceph-csi</span><br><span class="line">reclaimPolicy: Delete</span><br><span class="line">mountOptions:</span><br><span class="line">   - discard</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>然后再创建一个使用 pvc 的 pod 测试 ceph rbd storageclass</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">oc project sandbox-1</span><br><span class="line"></span><br><span class="line">oc apply -f - &lt;&lt;EOF</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: csi-rbd-demo-pod</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">    - name: web-server</span><br><span class="line">      image: quay.svc.vopsdev.com&#x2F;devinfra&#x2F;nginx:latest</span><br><span class="line">      volumeMounts:</span><br><span class="line">        - name: www-data</span><br><span class="line">          mountPath: &#x2F;var&#x2F;lib&#x2F;www&#x2F;html</span><br><span class="line">  volumes:</span><br><span class="line">    - name: www-data</span><br><span class="line">      persistentVolumeClaim:</span><br><span class="line">        claimName: rbd-pvc</span><br><span class="line">        readOnly: false</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">metadata:</span><br><span class="line">  name: rbd-pvc</span><br><span class="line">spec:</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteOnce</span><br><span class="line">  volumeMode: Filesystem</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 1Gi</span><br><span class="line">  storageClassName: csi-rbd-sc</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">oc get pod,pvc</span><br><span class="line">NAME                   READY   STATUS    RESTARTS   AGE</span><br><span class="line">pod&#x2F;csi-rbd-demo-pod   1&#x2F;1     Running   0          43s</span><br><span class="line"></span><br><span class="line">NAME                            STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE</span><br><span class="line">persistentvolumeclaim&#x2F;rbd-pvc   Bound    pvc-57efa24d-acc8-40f5-bb0c-18b77f0d5ec6   1Gi        RWO            csi-rbd-sc     43s</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文为 OCP 平台配置动态持久存储, 使用 vSphere Volume 和 Ceph RBD CSI
    
    </summary>
    
    
      <category term="RedHat" scheme="https://notes.yuanlinios.me/categories/RedHat/"/>
    
      <category term="Openshift" scheme="https://notes.yuanlinios.me/categories/RedHat/Openshift/"/>
    
    
      <category term="RedHat" scheme="https://notes.yuanlinios.me/tags/RedHat/"/>
    
      <category term="Openshift" scheme="https://notes.yuanlinios.me/tags/Openshift/"/>
    
  </entry>
  
  <entry>
    <title>离线部署 Openshift Container Platform 4.3 - 5: 配置 registry 使用持久存储</title>
    <link href="https://notes.yuanlinios.me/2020-03-20/%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B2-Openshift-Container-Platform-4-3-5-%E9%85%8D%E7%BD%AE-registry-%E4%BD%BF%E7%94%A8%E6%8C%81%E4%B9%85%E5%AD%98%E5%82%A8/"/>
    <id>https://notes.yuanlinios.me/2020-03-20/%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B2-Openshift-Container-Platform-4-3-5-%E9%85%8D%E7%BD%AE-registry-%E4%BD%BF%E7%94%A8%E6%8C%81%E4%B9%85%E5%AD%98%E5%82%A8/</id>
    <published>2020-03-21T02:43:54.000Z</published>
    <updated>2020-03-21T11:50:06.341Z</updated>
    
    <content type="html"><![CDATA[<p>默认安装完成时 OCP 集成的镜像仓库没有配置持久存储, registry pod 会被自动清理, 管理状态为 Removed. 本文将为 OCP 集成镜像仓库配置持久存储. 这里使用基于 minio 的对象存储</p><a id="more"></a><h2 id="关于-registry-持久存储">关于 registry 持久存储</h2><p>OCP <a href="https://docs.openshift.com/container-platform/4.3/scalability_and_performance/optimizing-storage.html" target="_blank" rel="external nofollow noopener noreferrer">官方文档</a>建议使用对象存储作为 registry 的后端, 并且一再强调不建议在生产环境中使用 NFS. 然而在 OCP 官方文档的 vSphere 和 Baremetal 的<a href="https://docs.openshift.com/container-platform/4.3/registry/configuring_registry_storage/configuring-registry-storage-vsphere.html" target="_blank" rel="external nofollow noopener noreferrer">配置样例</a>里又一直采用 NFS 做为 registry 的持久存储, 实在是让人无语.</p><p>我的环境有 minio 集群, 通过 endpoint https://s3.svc.vopsdev.com 对外提供服务. 这里将使用 minio 提供的对象存储作为 registry 的后端.</p><h2 id="配置-minio">配置 minio</h2><h3 id="创建-bucket">创建 bucket</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mc mb minio/ocp-registry</span><br></pre></td></tr></table></figure><h3 id="创建用户和策略">创建用户和策略</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># IAM policy</span></span><br><span class="line">cat &gt; ocp-registry-policy.json &lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"Version"</span>: <span class="string">"2012-10-17"</span>,</span><br><span class="line">  <span class="string">"Statement"</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"Action"</span>: [</span><br><span class="line">        <span class="string">"s3:GetObject"</span>,</span><br><span class="line">        <span class="string">"s3:PutObject"</span>,</span><br><span class="line">        <span class="string">"s3:DeleteObject"</span>,</span><br><span class="line">        <span class="string">"s3:ListMultipartUploadParts"</span>,</span><br><span class="line">        <span class="string">"s3:AbortMultipartUpload"</span></span><br><span class="line">      ],</span><br><span class="line">      <span class="string">"Effect"</span>: <span class="string">"Allow"</span>,</span><br><span class="line">      <span class="string">"Resource"</span>: [</span><br><span class="line">        <span class="string">"arn:aws:s3:::ocp-registry/*"</span></span><br><span class="line">      ],</span><br><span class="line">      <span class="string">"Sid"</span>: <span class="string">""</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"Action"</span>: [</span><br><span class="line">        <span class="string">"s3:ListBucket"</span>,</span><br><span class="line">        <span class="string">"s3:GetBucketLocation"</span>,</span><br><span class="line">        <span class="string">"s3:ListBucketMultipartUploads"</span></span><br><span class="line">      ],</span><br><span class="line">      <span class="string">"Effect"</span>: <span class="string">"Allow"</span>,</span><br><span class="line">      <span class="string">"Resource"</span>: [</span><br><span class="line">        <span class="string">"arn:aws:s3:::ocp-registry"</span></span><br><span class="line">      ],</span><br><span class="line">      <span class="string">"Sid"</span>: <span class="string">""</span></span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建用户, 关联策略</span></span><br><span class="line">mc admin policy add minio ocp-registry ocp-registry-policy.json</span><br><span class="line">mc admin policy info minio ocp-registry</span><br><span class="line">mc admin user add minio ocp-registry-id ocp-registry-secret</span><br><span class="line">mc admin policy <span class="built_in">set</span> minio ocp-registry user=ocp-registry-id</span><br><span class="line">mc admin user info minio ocp-registry-id</span><br></pre></td></tr></table></figure><h2 id="配置-ocp-registry">配置 OCP registry</h2><p>初始状态, 在 openshift-image-registry 下只有 registry operator pod 在运行, 并没有 registry pod</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">oc get pod -n openshift-image-registry</span><br><span class="line">NAME                                               READY   STATUS    RESTARTS   AGE</span><br><span class="line">cluster-image-registry-operator-5d7d64d769-q4tw2   2/2     Running   1          2d1h</span><br></pre></td></tr></table></figure><p>在 openshift-image-registry 项目下创建 secret 保存访问对象存储所使用的 access_key 和 secret_key</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">oc create secret generic image-registry-private-configuration-user --from-literal=REGISTRY_STORAGE_S3_ACCESSKEY=ocp-registry-id --from-literal=REGISTRY_STORAGE_S3_SECRETKEY=ocp-registry-secret -n openshift-image-registry</span><br></pre></td></tr></table></figure><p>通过<code>oc edit configs.imageregistry.operator.openshift.io/cluster</code>编辑 image registry 的配置, 将管理状态设置为 Managed, 并指定后端对象存储信息:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">spec:</span><br><span class="line">  managementState: Managed</span><br><span class="line">  storage:</span><br><span class="line">    s3:</span><br><span class="line">      bucket: ocp-registry</span><br><span class="line">      region: local</span><br><span class="line">      regionEndpoint: https:&#x2F;&#x2F;s3.svc.vopsdev.com</span><br><span class="line">  routes:</span><br><span class="line">  - name: internal-registry-route</span><br><span class="line">    hostname: registry-int.apps.ocp.vopsdev.com</span><br></pre></td></tr></table></figure><p>同时这里也指定了 route 将集成镜像库的服务通过 ingress 暴露出来. 然后就可以看到 image-registry pod 已经起来了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">oc get pod -n openshift-image-registry</span><br><span class="line">NAME                                               READY   STATUS    RESTARTS   AGE</span><br><span class="line">cluster-image-registry-operator-5d7d64d769-q4tw2   2/2     Running   1          2d1h</span><br><span class="line">image-registry-5cdb47886d-cq6l5                    1/1     Running   0          115s</span><br><span class="line">node-ca-2w9jf                                      1/1     Running   0          116s</span><br><span class="line">node-ca-5mccq                                      1/1     Running   0          116s</span><br><span class="line">node-ca-5vv95                                      1/1     Running   0          116s</span><br><span class="line">node-ca-fqk8q                                      1/1     Running   0          116s</span><br><span class="line">node-ca-gfhvl                                      1/1     Running   0          116s</span><br><span class="line">node-ca-hmr4w                                      1/1     Running   0          116s</span><br></pre></td></tr></table></figure><h2 id="测试">测试</h2><h3 id="创建本地账户">创建本地账户</h3><p>为了测试方便, 给 OCP 配置 HTPasswd 的认证源, 创建几个本地账户</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建 htpasswd 文件</span></span><br><span class="line">htpasswd -cBb /tmp/htpasswd admin admin</span><br><span class="line">htpasswd -Bb /tmp/htpasswd user1 user1</span><br><span class="line">htpasswd -Bb /tmp/htpasswd user2 user2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建包含此 htpsswd 的 secret</span></span><br><span class="line">oc create secret generic htpass-secret --from-file=htpasswd=/tmp/htpasswd -n openshift-config</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置 HTPasswd IDP</span></span><br><span class="line">oc apply -f - &lt;&lt;EOF</span><br><span class="line">apiVersion: config.openshift.io/v1</span><br><span class="line">kind: OAuth</span><br><span class="line">metadata:</span><br><span class="line">  name: cluster</span><br><span class="line">spec:</span><br><span class="line">  identityProviders:</span><br><span class="line">  - name: htpasswd_provider</span><br><span class="line">    mappingMethod: claim</span><br><span class="line">    <span class="built_in">type</span>: HTPasswd</span><br><span class="line">    htpasswd:</span><br><span class="line">      fileData:</span><br><span class="line">        name: htpass-secret</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 给用户授予角色. 允许 user1 往 sandbox-1 名字空间下推送镜像</span></span><br><span class="line">oc adm policy add-cluster-role-to-user cluster-admin admin</span><br><span class="line">oc new-project sandbox-1</span><br><span class="line">oc policy add-role-to-user registry-editor user1</span><br><span class="line">oc login -u user1</span><br></pre></td></tr></table></figure><h3 id="测试-registry">测试 registry</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker login -u user1 -p $(oc whoami -t) registry-int.apps.ocp.vopsdev.com</span><br><span class="line">docker pull alpine</span><br><span class="line">docker tag alpine:latest registry-int.apps.ocp.vopsdev.com/sandbox-1/alpine:latest</span><br><span class="line">docker push registry-int.apps.ocp.vopsdev.com/sandbox-1/alpine:latest</span><br></pre></td></tr></table></figure><p>在 minio 的浏览器界面上也可以看到结果</p><p><img src="integrated-registry-test.png"></p><p>使用本地帐号 admin 登录 OCP web console, 可以发现针对 image registry 被删除的状态提示已经消失. 目前还提示 Alertmanager 的 notification 没有正确设置, 这部分会在后面监控配置中解决.</p><p><img src="ocp-status-after-registry-setup.png"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;默认安装完成时 OCP 集成的镜像仓库没有配置持久存储, registry pod 会被自动清理, 管理状态为 Removed. 本文将为 OCP 集成镜像仓库配置持久存储. 这里使用基于 minio 的对象存储&lt;/p&gt;
    
    </summary>
    
    
      <category term="RedHat" scheme="https://notes.yuanlinios.me/categories/RedHat/"/>
    
      <category term="Openshift" scheme="https://notes.yuanlinios.me/categories/RedHat/Openshift/"/>
    
    
      <category term="RedHat" scheme="https://notes.yuanlinios.me/tags/RedHat/"/>
    
      <category term="Openshift" scheme="https://notes.yuanlinios.me/tags/Openshift/"/>
    
  </entry>
  
  <entry>
    <title>离线部署 Openshift Container Platform 4.3 - 4: 配置 ingress</title>
    <link href="https://notes.yuanlinios.me/2020-03-19/%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B2-Openshift-Container-Platform-4-3-4-%E9%85%8D%E7%BD%AE-ingress/"/>
    <id>https://notes.yuanlinios.me/2020-03-19/%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B2-Openshift-Container-Platform-4-3-4-%E9%85%8D%E7%BD%AE-ingress/</id>
    <published>2020-03-20T02:35:23.000Z</published>
    <updated>2020-03-21T12:20:19.429Z</updated>
    
    <content type="html"><![CDATA[<p>本文简单配置一下 ingress router, 解决默认设置的一些问题</p><ul><li>固定 router pod 的放置节点: 默认没有 placement 规则, ingress router pod 可以跑在任意 worker 节点上, 给外部负载均衡器的配置带来麻烦</li><li>替换默认证书为私有 CA 签发证书, 避免内网访问时提示证书不受信任的警告</li></ul><a id="more"></a><h2 id="ingress-router-placement">Ingress Router Placement</h2><p>我这个环境里只有三个 worker 节点, 配置外部负载均衡器访问 ingress 暴露的应用时也就是用这三个节点作为后端 server pool. 但是实际情况中可能有很多 worker 节点, 因此需要对 ingress router pod 的放置策略加以限制, 便于配置外部负载均衡器.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看 IngressController 的 API spec. 可以看到放置策略支持 nodeSelector 模式</span></span><br><span class="line">oc explain ingresscontroller.spec.nodePlacement --api-version=operator.openshift.io/v1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 给目标 worker 打标签, 标记其角色为 infra</span></span><br><span class="line">oc label node ocp-node-3.int.vopsdev.com node-role.kubernetes.io/worker=infra --overwrite</span><br><span class="line">oc label node ocp-node-4.int.vopsdev.com node-role.kubernetes.io/worker=infra --overwrite</span><br><span class="line">oc label node ocp-node-5.int.vopsdev.com node-role.kubernetes.io/worker=infra --overwrite</span><br><span class="line"></span><br><span class="line"><span class="comment"># 给 ingresscontroller 设置放置策略</span></span><br><span class="line">oc patch ingresscontroller default --<span class="built_in">type</span>=merge -p <span class="string">'&#123;"spec":&#123;"nodePlacement":&#123;"nodeSelector":&#123;"matchLabels":&#123;"node-role.kubernetes.io/worker":"infra"&#125;&#125;&#125;&#125;&#125;'</span> -n openshift-ingress-operator</span><br><span class="line"></span><br><span class="line"><span class="comment"># router pod 进行了重建, 并且调度到目标节点上</span></span><br><span class="line">oc get pod -n openshift-ingress -o wide</span><br></pre></td></tr></table></figure><h2 id="替换-ingress-默认证书">替换 ingress 默认证书</h2><p>使用私有 PKI 的二级 CA (issuing CA) 签发一个 wildcard 证书作为 ingress 默认证书, 包含下面名字</p><ul><li>ocp.vopsdev.com</li><li>*.ocp.vopsdev.com</li><li>*.apps.ocp.vopsdev.com</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 让 haproxy router 信任私有根 CA</span></span><br><span class="line">oc create configmap vopsdev-root-ca --from-file=ca-bundle.crt=/etc/pki/ca-trust/<span class="built_in">source</span>/anchors/vopsdev-root-ca-g1.crt -n openshift-config</span><br><span class="line">oc patch proxy/cluster --<span class="built_in">type</span>=merge --patch=<span class="string">'&#123;"spec":&#123;"trustedCA":&#123;"name":"vopsdev-root-ca"&#125;&#125;&#125;'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里的证书使用证书链形式, 包含服务器证书 + 二级 CA 证书</span></span><br><span class="line"><span class="comment"># 由于内网机器都已经信任了根 CA, 结合这里的服务器证书链可以构成完整的 TLS 验证路径</span></span><br><span class="line">oc create secret tls wildcard-ocp-tls --cert=private.wildcard.vopsdev.com.chained.crt --key=private.wildcard.vopsdev.com.key -n openshift-ingress</span><br><span class="line">oc patch ingresscontroller default --<span class="built_in">type</span>=merge -p <span class="string">'&#123;"spec":&#123;"defaultCertificate":&#123;"name":"wildcard-ocp-tls"&#125;&#125;&#125;'</span> -n openshift-ingress-operator</span><br></pre></td></tr></table></figure><p>等待 router pod 重建完成后, 测试登录 web console, 可以发现证书替换已经生效 <img src="ingress-default-certificate.png"></p><p>对于内网环境, 这种 wildcard 形式的默认证书基本够用. 签发一个时效足够长的证书后也不用操心续期的问题. 但是如果想把应用发布到公网上, 还是得需要公网 CA 签发的证书. 目前用的比较多的是 Let's Encrypt, 此时可以结合 Openshift ACME Controller 实现为 ingress 资源自动申请 Let's Encrypt 证书并自动续期.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文简单配置一下 ingress router, 解决默认设置的一些问题&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;固定 router pod 的放置节点: 默认没有 placement 规则, ingress router pod 可以跑在任意 worker 节点上, 给外部负载均衡器的配置带来麻烦&lt;/li&gt;
&lt;li&gt;替换默认证书为私有 CA 签发证书, 避免内网访问时提示证书不受信任的警告&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="RedHat" scheme="https://notes.yuanlinios.me/categories/RedHat/"/>
    
      <category term="Openshift" scheme="https://notes.yuanlinios.me/categories/RedHat/Openshift/"/>
    
    
      <category term="RedHat" scheme="https://notes.yuanlinios.me/tags/RedHat/"/>
    
      <category term="Openshift" scheme="https://notes.yuanlinios.me/tags/Openshift/"/>
    
  </entry>
  
  <entry>
    <title>离线部署 Openshift Container Platform 4.3 - 3: 配置本地 ImageStream 和 OperatorHub</title>
    <link href="https://notes.yuanlinios.me/2020-03-18/%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B2-Openshift-Container-Platform-4-3-3-%E9%85%8D%E7%BD%AE%E6%9C%AC%E5%9C%B0-ImageStream-%E5%92%8C-OperatorHub/"/>
    <id>https://notes.yuanlinios.me/2020-03-18/%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B2-Openshift-Container-Platform-4-3-3-%E9%85%8D%E7%BD%AE%E6%9C%AC%E5%9C%B0-ImageStream-%E5%92%8C-OperatorHub/</id>
    <published>2020-03-18T14:20:23.000Z</published>
    <updated>2020-03-21T03:04:27.547Z</updated>
    
    <content type="html"><![CDATA[<p>本文将使用前面保存私有镜像库的离线资源替换默认的 ImageStream 和 OperatorHub</p><a id="more"></a><h2 id="替换-sample-images">替换 Sample Images</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 私有镜像库使用私有 PKI 签发的证书提供 https 服务, 需要让 samples operator 信任根 CA</span></span><br><span class="line">oc create configmap registry-config --from-file=/etc/pki/ca-trust/<span class="built_in">source</span>/anchors/vopsdev-root-ca-g1.crt -n openshift-config</span><br><span class="line">oc patch image.config.openshift.io cluster --patch <span class="string">'&#123;"spec":&#123;"additionalTrustedCA":&#123;"name":"registry-config"&#125;&#125;&#125;'</span> --<span class="built_in">type</span>=merge</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加私有镜像库地址</span></span><br><span class="line">oc patch configs.samples.operator.openshift.io cluster --patch <span class="string">'&#123;"spec":&#123;"samplesRegistry":"quay.svc.vopsdev.com"&#125;&#125;'</span> --<span class="built_in">type</span>=merge</span><br><span class="line"></span><br><span class="line"><span class="comment"># 清除现有的 imagestreams</span></span><br><span class="line">oc patch configs.samples.operator.openshift.io cluster --patch <span class="string">'[&#123;"op": "replace", "path": "/spec/managementState", "value":"Removed"&#125;]'</span> --<span class="built_in">type</span>=json</span><br><span class="line">oc get is -n openshift</span><br><span class="line"></span><br><span class="line"><span class="comment"># 再重新创建出来</span></span><br><span class="line">oc patch configs.samples.operator.openshift.io cluster --patch <span class="string">'[&#123;"op": "replace", "path": "/spec/managementState", "value":"Managed"&#125;]'</span> --<span class="built_in">type</span>=json</span><br><span class="line">oc get is -n openshift</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查 dockerImageReference 可以发现已经指向私有镜像库了  </span></span><br><span class="line">oc get is apicast-gateway -n openshift -o yaml</span><br></pre></td></tr></table></figure><p>在 openshift namespace 下的这几个 imagestream 不受 samples operator 管理, 而是来自离线安装介质</p><ul><li>cli</li><li>cli-artifacts</li><li>installer</li><li>installer-artifacts</li><li>must-gather</li><li>tests</li><li>jenkins</li><li>jenkins-agent-maven</li><li>jenkins-agent-nodejs</li></ul><h2 id="替换-operatorhub">替换 OperatorHub</h2><p>通过 <code>oc get catalogsource -n openshift-marketplace</code> 可以发现在 openshift-marketplace namespace 下有 redhat-operators, community-operators 和 certified-operators 三个 catalog source. 先禁用这些默认的源, 然后再添加本地离线资源</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">oc patch OperatorHub cluster --<span class="built_in">type</span> json -p <span class="string">'[&#123;"op": "add", "path": "/spec/disableAllDefaultSources", "value": true&#125;]'</span></span><br></pre></td></tr></table></figure><p>此时在 web console 上查看 operatorHub 时可以发现已经没有内容了.</p><p><img src="ocp-operatorhub-empty.png"></p><p>通过 <code>oc get catalogsource -n openshift-marketplace</code> 查看显示内容为空.</p><div class="note info">            <p>此处发现各个节点 kubelet 会依次重启. 如果遇到这个情况, 等各个节点都 Ready 再进行后面的操作</p>          </div><p>然后添加本地 catalog source. 这里需要用到前面 <code>oc adm catalog mirror</code> 产生的目录 redhat-operators-manifests</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建 imageContentSourcePolicy</span></span><br><span class="line">oc apply -f redhat-operators-manifests/</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 catalogSource 引用离线资源</span></span><br><span class="line">oc apply -f - &lt;&lt;EOF</span><br><span class="line">apiVersion: operators.coreos.com/v1alpha1</span><br><span class="line">kind: CatalogSource</span><br><span class="line">metadata:</span><br><span class="line">  name: vopsdev-redhat-operators</span><br><span class="line">  namespace: openshift-marketplace</span><br><span class="line">spec:</span><br><span class="line">  sourceType: grpc</span><br><span class="line">  image: quay.svc.vopsdev.com/devinfra/redhat-operators:v1</span><br><span class="line">  displayName: VOPSDEV RedHat Operator Catalog</span><br><span class="line">  publisher: grpc</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 会看到自定义的 catalog pod 和 catalog source</span></span><br><span class="line">oc get pods -n openshift-marketplace</span><br><span class="line">oc get catalogsource -n openshift-marketplace</span><br></pre></td></tr></table></figure><p>再登录 web console 就可以看到 OperatorHub 下的内容了</p><p><img src="ocp-operatorhub-customized.png"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将使用前面保存私有镜像库的离线资源替换默认的 ImageStream 和 OperatorHub&lt;/p&gt;
    
    </summary>
    
    
      <category term="RedHat" scheme="https://notes.yuanlinios.me/categories/RedHat/"/>
    
      <category term="Openshift" scheme="https://notes.yuanlinios.me/categories/RedHat/Openshift/"/>
    
    
      <category term="RedHat" scheme="https://notes.yuanlinios.me/tags/RedHat/"/>
    
      <category term="Openshift" scheme="https://notes.yuanlinios.me/tags/Openshift/"/>
    
  </entry>
  
  <entry>
    <title>离线部署 Openshift Container Platform 4.3 - 2: 初始安装</title>
    <link href="https://notes.yuanlinios.me/2020-03-16/%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B2-Openshift-Container-Platform-4-3-2-%E5%88%9D%E5%A7%8B%E5%AE%89%E8%A3%85/"/>
    <id>https://notes.yuanlinios.me/2020-03-16/%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B2-Openshift-Container-Platform-4-3-2-%E5%88%9D%E5%A7%8B%E5%AE%89%E8%A3%85/</id>
    <published>2020-03-17T01:23:46.000Z</published>
    <updated>2020-03-21T03:02:31.183Z</updated>
    
    <content type="html"><![CDATA[<p>本文记录了离线环境下以 UPI (User Provisioned Infrastructure) 模式初始安装 OCP 4.3.5 集群的步骤, 包括地址/DNS 名称规划, DHCP, 负载均衡配置, ignition 文件生成, 到最后的集群部署.</p><p>IaaS 平台为 VMware vSphere 6.7U2. DDI 方案使用 Infoblox NIOS. 负载均衡方案使用 HAProxy</p><a id="more"></a><h2 id="dns-及-ip-地址规划">DNS 及 IP 地址规划</h2><table><colgroup><col style="width: 31%"><col style="width: 5%"><col style="width: 27%"><col style="width: 35%"></colgroup><thead><tr class="header"><th>DNS 记录</th><th>类型</th><th>值</th><th>说明</th></tr></thead><tbody><tr class="odd"><td>ocp-bootstrap.int.vopsdev.com</td><td>A</td><td>192.168.11.119</td><td>bootstrap 节点. A 记录对应的 PTR 记录也同时创建出来, 下同</td></tr><tr class="even"><td>ocp-node-0.int.vopsdev.com</td><td>A</td><td>192.168.11.120</td><td>master 节点</td></tr><tr class="odd"><td>ocp-node-1.int.vopsdev.com</td><td>A</td><td>192.168.11.121</td><td>master 节点</td></tr><tr class="even"><td>ocp-node-2.int.vopsdev.com</td><td>A</td><td>192.168.11.122</td><td>master 节点</td></tr><tr class="odd"><td>ocp-node-3.int.vopsdev.com</td><td>A</td><td>192.168.11.123</td><td>worker 节点</td></tr><tr class="even"><td>ocp-node-4.int.vopsdev.com</td><td>A</td><td>192.168.11.124</td><td>worker 节点</td></tr><tr class="odd"><td>ocp-node-5.int.vopsdev.com</td><td>A</td><td>192.168.11.125</td><td>worker 节点</td></tr><tr class="even"><td>api.ocp.vopsdev.com</td><td>CNAME</td><td>lb-if-192-168-11-249.int.vopsdev.com</td><td>别名, 指向现有的 LB 记录</td></tr><tr class="odd"><td>api-int.ocp.vopsdev.com</td><td>CNAME</td><td>lb-if-192-168-11-249.int.vopsdev.com</td><td>如果有需要, api 和 api-int 可以分别指向内部/外部 LB</td></tr><tr class="even"><td>*.apps.ocp.vopsdev.com</td><td>CNAME</td><td>lb-if-192-168-11-249.int.vopsdev.com</td><td></td></tr><tr class="odd"><td>etcd-0.ocp.vopsdev.com</td><td>CNAME</td><td>ocp-node-0.int.vopsdev.com</td><td></td></tr><tr class="even"><td>etcd-1.ocp.vopsdev.com</td><td>CNAME</td><td>ocp-node-1.int.vopsdev.com</td><td></td></tr><tr class="odd"><td>etcd-2.ocp.vopsdev.com</td><td>CNAME</td><td>ocp-node-2.int.vopsdev.com</td><td></td></tr><tr class="even"><td>_etcd-server-ssl._tcp.ocp.vopsdev.com</td><td>SRV</td><td>0 10 2380 etcd-0.ocp.vopsdev.com</td><td></td></tr><tr class="odd"><td>_etcd-server-ssl._tcp.ocp.vopsdev.com</td><td>SRV</td><td>0 10 2380 etcd-1.ocp.vopsdev.com</td><td></td></tr><tr class="even"><td>_etcd-server-ssl._tcp.ocp.vopsdev.com</td><td>SRV</td><td>0 10 2380 etcd-2.ocp.vopsdev.com</td><td></td></tr></tbody></table><h2 id="dhcp">DHCP</h2><p>所有 OCP 节点都通过 DHCP 自动获取 IP. DHCP 上做预留 (在 Infoblox 里叫 fixed address), 确保每个节点都可以获取预先规划的地址.</p><p><img src="infoblox-ocp-dhcp-fixed-addresses.png"></p><p>由于节点 MAC 地址要在虚拟机部署之后才能确定, 这里先随便填写. 后面部署节点时会通过 Infoblox API 动态更新 fixed address 记录的 MAC 字段.</p><h2 id="负载均衡">负载均衡</h2><p>负载均衡器目前使用是 HAProxy, 简单写几个四层 LB 配置 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">frontend vs_ocp_master_6443</span><br><span class="line">    mode tcp</span><br><span class="line">    option tcplog</span><br><span class="line">    <span class="built_in">bind</span> 192.168.11.249:6443</span><br><span class="line">    default_backend pl_ocp_master_6443</span><br><span class="line"></span><br><span class="line">frontend vs_ocp_master_22623</span><br><span class="line">    mode tcp</span><br><span class="line">    option tcplog</span><br><span class="line">    <span class="built_in">bind</span> 192.168.11.249:22623</span><br><span class="line">    default_backend pl_ocp_master_22623</span><br><span class="line"></span><br><span class="line">frontend vs_ocp_router_80</span><br><span class="line">    mode tcp</span><br><span class="line">    option tcplog</span><br><span class="line">    <span class="built_in">bind</span> 192.168.11.249:80</span><br><span class="line">    default_backend pl_ocp_router_80</span><br><span class="line"></span><br><span class="line">frontend vs_ocp_router_443</span><br><span class="line">    mode tcp</span><br><span class="line">    option tcplog</span><br><span class="line">    <span class="built_in">bind</span> 192.168.11.249:443</span><br><span class="line">    default_backend pl_ocp_router_443</span><br><span class="line"></span><br><span class="line">backend pl_ocp_master_6443</span><br><span class="line">    mode tcp</span><br><span class="line">    balance <span class="built_in">source</span></span><br><span class="line">    server ocp-node-0 ocp-node-0.int.vopsdev.com:6443 check</span><br><span class="line">    server ocp-node-1 ocp-node-1.int.vopsdev.com:6443 check</span><br><span class="line">    server ocp-node-2 ocp-node-2.int.vopsdev.com:6443 check</span><br><span class="line">    server ocp-bootstrap ocp-bootstrap.int.vopsdev.com:6443 check</span><br><span class="line"></span><br><span class="line">backend pl_ocp_master_22623</span><br><span class="line">    mode tcp</span><br><span class="line">    balance <span class="built_in">source</span></span><br><span class="line">    server ocp-node-0 ocp-node-0.int.vopsdev.com:22623 check</span><br><span class="line">    server ocp-node-1 ocp-node-1.int.vopsdev.com:22623 check</span><br><span class="line">    server ocp-node-2 ocp-node-2.int.vopsdev.com:22623 check</span><br><span class="line">    server ocp-bootstrap ocp-bootstrap.int.vopsdev.com:22623 check</span><br><span class="line"></span><br><span class="line">backend pl_ocp_router_80</span><br><span class="line">    mode tcp</span><br><span class="line">    balance <span class="built_in">source</span></span><br><span class="line">    server ocp-node-3 ocp-node-3.int.vopsdev.com:80 check</span><br><span class="line">    server ocp-node-4 ocp-node-4.int.vopsdev.com:80 check</span><br><span class="line">    server ocp-node-5 ocp-node-5.int.vopsdev.com:80 check</span><br><span class="line"></span><br><span class="line">backend pl_ocp_router_443</span><br><span class="line">    mode tcp</span><br><span class="line">    balance <span class="built_in">source</span></span><br><span class="line">    server ocp-node-3 ocp-node-3.int.vopsdev.com:443 check</span><br><span class="line">    server ocp-node-4 ocp-node-4.int.vopsdev.com:443 check</span><br><span class="line">    server ocp-node-5 ocp-node-5.int.vopsdev.com:443 check</span><br></pre></td></tr></table></figure></p><h2 id="准备-ignition-文件">准备 ignition 文件</h2><h3 id="编写-install-config.yaml">编写 install-config.yaml</h3><p>根据环境的实际情况编写 install-config.yaml. 下面是我的实验环境所使用的版本 <figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">baseDomain:</span> <span class="string">vopsdev.com</span></span><br><span class="line"><span class="attr">compute:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">hyperthreading:</span> <span class="string">Enabled</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">worker</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">0</span></span><br><span class="line"><span class="attr">controlPlane:</span></span><br><span class="line">  <span class="attr">hyperthreading:</span> <span class="string">Enabled</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">master</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">3</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">ocp</span></span><br><span class="line"><span class="attr">platform:</span></span><br><span class="line">  <span class="attr">vsphere:</span></span><br><span class="line">    <span class="attr">vcenter:</span> <span class="string">vc.int.vopsdev.com</span></span><br><span class="line">    <span class="attr">username:</span> <span class="string">xxx</span></span><br><span class="line">    <span class="attr">password:</span> <span class="string">xxx</span></span><br><span class="line">    <span class="attr">datacenter:</span> <span class="string">HDC</span></span><br><span class="line">    <span class="attr">defaultDatastore:</span> <span class="string">DS-SHARED</span></span><br><span class="line"><span class="attr">fips:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">networking:</span></span><br><span class="line">  <span class="attr">networkType:</span> <span class="string">OpenShiftSDN</span></span><br><span class="line">  <span class="attr">clusterNetworks:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">cidr:</span> <span class="number">10.9</span><span class="number">.0</span><span class="number">.0</span><span class="string">/20</span></span><br><span class="line">    <span class="attr">hostPrefix:</span> <span class="number">24</span></span><br><span class="line">  <span class="attr">serviceNetwork:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="number">172.16</span><span class="number">.0</span><span class="number">.0</span><span class="string">/20</span></span><br><span class="line"><span class="attr">sshKey:</span> <span class="string">'ssh-rsa ...'</span></span><br><span class="line"><span class="attr">additionalTrustBundle:</span> <span class="string">|</span></span><br><span class="line">  <span class="string">-----BEGIN</span> <span class="string">CERTIFICATE-----</span></span><br><span class="line">  <span class="string">...</span></span><br><span class="line">  <span class="string">-----END</span> <span class="string">CERTIFICATE-----</span></span><br><span class="line"><span class="attr">imageContentSources:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">mirrors:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">quay.svc.vopsdev.com/openshift-release-dev/ocp-v4.0-art-dev</span></span><br><span class="line">  <span class="attr">source:</span> <span class="string">quay.io/openshift-release-dev/ocp-release</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">mirrors:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">quay.svc.vopsdev.com/openshift-release-dev/ocp-v4.0-art-dev</span></span><br><span class="line">  <span class="attr">source:</span> <span class="string">quay.io/openshift-release-dev/ocp-v4.0-art-dev</span></span><br><span class="line"><span class="attr">pullSecret:</span> <span class="string">'&#123;"auths":&#123;...&#125;&#125;'</span></span><br></pre></td></tr></table></figure></p><p>一些注释</p><ul><li>metadata.name + baseDomain 组成集群的基础域名, 这里是 ocp.vopsdev.com</li><li>需要在 vSphere Datacenter 下按照 metadata.name 建立一个虚拟机目录, 将来创建 vSphere 动态卷需要, 具体原因看<a href="https://github.com/vmware-archive/kubernetes/issues/499" target="_blank" rel="external nofollow noopener noreferrer">这里</a>. 然而你并不需要将 OCP 节点放到这个虚拟机目录下</li><li>platform 下的 vcenter 信息主要用于动态卷的创建, 确保这个用户有足够的权限, 具体看<a href="https://vmware.github.io/vsphere-storage-for-kubernetes/documentation/vcp-roles.html" target="_blank" rel="external nofollow noopener noreferrer">这里</a></li><li>我的环境有私有 PKI, 所有的服务器证书由二级 CA 签发, 因此把根 CA 的证书放到 additionalTrustBundle 里, 确保所有 OCP 节点信任私有根 CA</li><li>imageContentSources 来自前面<code>oc adm release mirror</code> 的输出结果</li><li>我的 quay 私有镜像库不允许匿名拉取, 因此提供了 pullSecret</li></ul><h3 id="生成-ignition">生成 ignition</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line">rm -rf assets; mkdir assets</span><br><span class="line">cp install-config.yaml assets/</span><br><span class="line">./openshift-install create manifests --dir=assets</span><br><span class="line">sed -i <span class="string">'s/mastersSchedulable: true/mastersSchedulable: false/'</span> assets/manifests/cluster-scheduler-02-config.yml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置一下时间同步服务. 我这里两个 ntp 服务器是 192.168.11.2 和 192.168.255.249</span></span><br><span class="line">chrony_config=$(<span class="built_in">echo</span> -n <span class="string">"server 192.168.11.2 iburst</span></span><br><span class="line"><span class="string">server 192.168.255.249 iburst</span></span><br><span class="line"><span class="string">driftfile /var/lib/chrony/drift</span></span><br><span class="line"><span class="string">makestep 1.0 3</span></span><br><span class="line"><span class="string">rtcsync</span></span><br><span class="line"><span class="string">logdir /var/log/chrony"</span> | base64 -w0)</span><br><span class="line"></span><br><span class="line">cat &gt; assets/openshift/99_master-chrony.yaml &lt;&lt;EOF</span><br><span class="line">apiVersion: machineconfiguration.openshift.io/v1</span><br><span class="line">kind: MachineConfig</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    machineconfiguration.openshift.io/role: master</span><br><span class="line">  name: 50-vopsdev-master-chrony</span><br><span class="line">spec:</span><br><span class="line">  config:</span><br><span class="line">    ignition:</span><br><span class="line">      version: 2.2.0</span><br><span class="line">    storage:</span><br><span class="line">      files:</span><br><span class="line">      - contents:</span><br><span class="line">          <span class="built_in">source</span>: data:text/plain;charset=utf-8;base64,<span class="variable">$chrony_config</span></span><br><span class="line">        filesystem: root</span><br><span class="line">        mode: 0644</span><br><span class="line">        path: /etc/chrony.conf</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &gt; assets/openshift/99_worker-chrony.yaml &lt;&lt;EOF</span><br><span class="line">apiVersion: machineconfiguration.openshift.io/v1</span><br><span class="line">kind: MachineConfig</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    machineconfiguration.openshift.io/role: worker</span><br><span class="line">  name: 50-vopsdev-worker-chrony</span><br><span class="line">spec:</span><br><span class="line">  config:</span><br><span class="line">    ignition:</span><br><span class="line">      version: 2.2.0</span><br><span class="line">    storage:</span><br><span class="line">      files:</span><br><span class="line">      - contents:</span><br><span class="line">          <span class="built_in">source</span>: data:text/plain;charset=utf-8;base64,<span class="variable">$chrony_config</span></span><br><span class="line">        filesystem: root</span><br><span class="line">        mode: 0644</span><br><span class="line">        path: /etc/chrony.conf</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成 ignition 配置</span></span><br><span class="line">./openshift-install create ignition-configs --dir=assets</span><br><span class="line"></span><br><span class="line"><span class="comment"># 跳板机上运行了 httpd, 提供静态文件服务</span></span><br><span class="line"><span class="comment"># 后面节点可以通过 http://static.svc.vopsdev.com/ocp/bootstrap.ign 下载</span></span><br><span class="line">sudo cp -f assets/bootstrap.ign /var/www/static/ocp/</span><br><span class="line"></span><br><span class="line">cat &gt; ./assets/append-bootstrap.ign &lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"ignition"</span>: &#123;</span><br><span class="line">    <span class="string">"config"</span>: &#123;</span><br><span class="line">      <span class="string">"append"</span>: [</span><br><span class="line">        &#123;</span><br><span class="line">          <span class="string">"source"</span>: <span class="string">"http://static.svc.vopsdev.com/ocp/bootstrap.ign"</span>, </span><br><span class="line">          <span class="string">"verification"</span>: &#123;&#125;</span><br><span class="line">        &#125;</span><br><span class="line">      ]</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"timeouts"</span>: &#123;&#125;,</span><br><span class="line">    <span class="string">"version"</span>: <span class="string">"2.1.0"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"networkd"</span>: &#123;&#125;,</span><br><span class="line">  <span class="string">"passwd"</span>: &#123;&#125;,</span><br><span class="line">  <span class="string">"storage"</span>: &#123;&#125;,</span><br><span class="line">  <span class="string">"systemd"</span>: &#123;&#125;</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># base64 编码的 ignition</span></span><br><span class="line">base64 -w0 assets/master.ign &gt; assets/master.64</span><br><span class="line">base64 -w0 assets/worker.ign &gt; assets/worker.64</span><br><span class="line">base64 -w0 assets/append-bootstrap.ign &gt; assets/append-bootstrap.64</span><br></pre></td></tr></table></figure><h2 id="部署节点">部署节点</h2><p>对于 vSphere 环境下的部署, ignition 数据需要通过 guestinfo.ignition.config.data 虚拟机属性注入, 手工操作显然不可接受. 我这里采用的方法是 ovftool + govc. 用 ansible 或者 terraform 之类应该也可以做, 但是在这儿显得杀鸡用牛刀了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line">BOOTSTRAP_B64_CONF=<span class="string">"<span class="variable">$(cat assets/append-bootstrap.64)</span>"</span></span><br><span class="line">MASTER_B64_CONF=<span class="string">"<span class="variable">$(cat assets/master.64)</span>"</span></span><br><span class="line">WORKER_B64_CONF=<span class="string">"<span class="variable">$(cat assets/worker.64)</span>"</span></span><br><span class="line">RHCOS_OVA=<span class="string">"/mnt/driver/RedHat/openshift-4.3/rhcos-4.3.0-x86_64-vmware.ova"</span></span><br><span class="line">USER=<span class="string">'administrator@vsphere.local'</span></span><br><span class="line">PASS=<span class="string">'xxx'</span></span><br><span class="line">VC=<span class="string">"vc.int.vopsdev.com"</span></span><br><span class="line">VMFOLDER=<span class="string">"Lab/OCP"</span></span><br><span class="line">NETWORK=<span class="string">"SG-SVC-11"</span></span><br><span class="line">DATASTORE=<span class="string">"DS-SHARED"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># infoblox api access</span></span><br><span class="line">IBUSER=<span class="string">"xxx"</span></span><br><span class="line">IBPASS=<span class="string">"xxx"</span></span><br><span class="line">IBAPI_BASE=<span class="string">"https://infoblox.int.vopsdev.com/wapi/v2.1"</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> GOVC_URL=<span class="string">"<span class="variable">$USER</span>:<span class="variable">$PASS</span>@<span class="variable">$VC</span>"</span></span><br><span class="line"><span class="built_in">export</span> GOVC_DATACENTER=HDC</span><br><span class="line"></span><br><span class="line">OCP_MASTERS=()</span><br><span class="line">OCP_WORKERS=()</span><br><span class="line"></span><br><span class="line"><span class="comment"># bootstrap node: 4 vCPU + 16G RAM</span></span><br><span class="line">VM=<span class="string">"ocp-bootstrap"</span></span><br><span class="line">ovftool --vmFolder=<span class="string">"<span class="variable">$VMFOLDER</span>"</span> --datastore=<span class="string">"<span class="variable">$DATASTORE</span>"</span> --diskMode=<span class="string">"thin"</span> --net:<span class="string">"VM Network"</span>=<span class="string">"<span class="variable">$NETWORK</span>"</span> --name=<span class="string">"<span class="variable">$VM</span>"</span>  --numberOfCpus:<span class="string">'*'</span>=4 --memorySize:<span class="string">'*'</span>=16384 --allowExtraConfig --extraConfig:disk.EnableUUID=<span class="literal">true</span> --extraConfig:guestinfo.ignition.config.data.encoding=<span class="string">"base64"</span> --extraConfig:guestinfo.ignition.config.data=<span class="string">"<span class="variable">$BOOTSTRAP_B64_CONF</span>"</span> <span class="variable">$RHCOS_OVA</span> <span class="string">"vi://<span class="variable">$USER</span>:<span class="variable">$PASS</span>@<span class="variable">$VC</span>/HDC/host/compute-cluster"</span></span><br><span class="line">govc vm.disk.change -vm <span class="variable">$VM</span> -disk.label <span class="string">"Hard disk 1"</span> -size 120G</span><br><span class="line"></span><br><span class="line"><span class="comment"># master nodes: 4 vCPU + 16G RAM</span></span><br><span class="line"><span class="keyword">for</span> idx <span class="keyword">in</span> 0 1 2; <span class="keyword">do</span></span><br><span class="line">  VM=<span class="string">"ocp-node-<span class="variable">$idx</span>"</span></span><br><span class="line">  ovftool --vmFolder=<span class="string">"<span class="variable">$VMFOLDER</span>"</span> --datastore=<span class="string">"<span class="variable">$DATASTORE</span>"</span> --diskMode=<span class="string">"thin"</span> --net:<span class="string">"VM Network"</span>=<span class="string">"<span class="variable">$NETWORK</span>"</span> --name=<span class="string">"<span class="variable">$VM</span>"</span>  --numberOfCpus:<span class="string">'*'</span>=4 --memorySize:<span class="string">'*'</span>=16384 --allowExtraConfig --extraConfig:disk.EnableUUID=<span class="literal">true</span> --prop:guestinfo.ignition.config.data.encoding=<span class="string">"base64"</span> --prop:guestinfo.ignition.config.data=<span class="string">"<span class="variable">$MASTER_B64_CONF</span>"</span> <span class="variable">$RHCOS_OVA</span> <span class="string">"vi://<span class="variable">$USER</span>:<span class="variable">$PASS</span>@<span class="variable">$VC</span>/HDC/host/compute-cluster"</span></span><br><span class="line">  govc vm.disk.change -vm <span class="variable">$VM</span> -disk.label <span class="string">"Hard disk 1"</span> -size 120G</span><br><span class="line">  OCP_MASTERS=(<span class="string">"<span class="variable">$&#123;OCP_MASTERS[@]&#125;</span>"</span> <span class="variable">$VM</span>)</span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># worker nodes: 4 vCPU + 32G RAM</span></span><br><span class="line"><span class="keyword">for</span> idx <span class="keyword">in</span> 3 4 5; <span class="keyword">do</span></span><br><span class="line">  VM=<span class="string">"ocp-node-<span class="variable">$idx</span>"</span></span><br><span class="line">  ovftool --vmFolder=<span class="string">"<span class="variable">$VMFOLDER</span>"</span> --datastore=<span class="string">"<span class="variable">$DATASTORE</span>"</span> --diskMode=<span class="string">"thin"</span> --net:<span class="string">"VM Network"</span>=<span class="string">"<span class="variable">$NETWORK</span>"</span> --name=<span class="string">"<span class="variable">$VM</span>"</span>  --numberOfCpus:<span class="string">'*'</span>=4 --memorySize:<span class="string">'*'</span>=32768 --allowExtraConfig --extraConfig:disk.EnableUUID=<span class="literal">true</span> --prop:guestinfo.ignition.config.data.encoding=<span class="string">"base64"</span> --prop:guestinfo.ignition.config.data=<span class="string">"<span class="variable">$WORKER_B64_CONF</span>"</span> <span class="variable">$RHCOS_OVA</span> <span class="string">"vi://<span class="variable">$USER</span>:<span class="variable">$PASS</span>@<span class="variable">$VC</span>/HDC/host/compute-cluster"</span></span><br><span class="line">  govc vm.disk.change -vm <span class="variable">$VM</span> -disk.label <span class="string">"Hard disk 1"</span> -size 120G</span><br><span class="line">  OCP_WORKERS=(<span class="string">"<span class="variable">$&#123;OCP_WORKERS[@]&#125;</span>"</span> <span class="variable">$VM</span>)</span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据新的 mac 地址, 更新 dhcp fixed address 记录</span></span><br><span class="line">ALL_NODES=(ocp-bootstrap <span class="string">"<span class="variable">$&#123;OCP_MASTERS[@]&#125;</span>"</span> <span class="string">"<span class="variable">$&#123;OCP_WORKERS[@]&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">for</span> node <span class="keyword">in</span> <span class="string">"<span class="variable">$&#123;ALL_NODES[@]&#125;</span>"</span>; <span class="keyword">do</span></span><br><span class="line">  mac=$(govc device.info -vm <span class="variable">$node</span> -json ethernet-0 | jq -r .Devices[].MacAddress)</span><br><span class="line">  ip=$(host <span class="variable">$node</span> | cut -d<span class="string">' '</span> -f4)</span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"<span class="variable">$node</span>: <span class="variable">$ip</span>, <span class="variable">$mac</span>"</span></span><br><span class="line">  api_path=$(curl -s -k -u <span class="variable">$IBUSER</span>:<span class="variable">$IBPASS</span> -H <span class="string">"Content-Type: application/json"</span> -X GET <span class="string">"<span class="variable">$IBAPI_BASE</span>/fixedaddress?ipv4addr=<span class="variable">$ip</span>"</span> | jq -r <span class="string">'.[] | ._ref'</span>)  </span><br><span class="line">  curl -s -k -u <span class="variable">$IBUSER</span>:<span class="variable">$IBPASS</span> -H <span class="string">"Content-Type: application/json"</span> -X PUT  <span class="string">"<span class="variable">$IBAPI_BASE</span>/<span class="variable">$&#123;api_path&#125;</span>"</span> -d <span class="string">"&#123;\"mac\":\"<span class="variable">$mac</span>\"&#125;"</span> &gt; /dev/null</span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 必要时 reload infoblox DHCP 服务</span></span><br><span class="line">grid_path=$(curl -s -k -u <span class="variable">$IBUSER</span>:<span class="variable">$IBPASS</span> -H <span class="string">"Content-Type: application/json"</span> <span class="string">"<span class="variable">$IBAPI_BASE</span>/grid"</span> | jq -r <span class="string">'.[] | ._ref'</span>)</span><br><span class="line">curl -s -k -u <span class="variable">$IBUSER</span>:<span class="variable">$IBPASS</span> -H <span class="string">"Content-Type: application/json"</span> -X POST <span class="string">"<span class="variable">$IBAPI_BASE</span>/<span class="variable">$&#123;grid_path&#125;</span>?_function=restartservices"</span> -d <span class="string">'&#123;"member_order":"SIMULTANEOUSLY","restart_option":"RESTART_IF_NEEDED","service_option":"DHCP"&#125;'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动 bootstrap</span></span><br><span class="line">govc vm.power -on <span class="string">"ocp-bootstrap"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 观察负载均衡器, 下面两个没问题了再进行下去</span></span><br><span class="line"><span class="comment"># curl -v -k https://api-int.ocp.vopsdev.com:22623/config/master</span></span><br><span class="line"><span class="comment"># curl -v -k https://api-int.ocp.vopsdev.com:22623/config/worker </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动所有 master 节点. 节点会去 bootstrap 上下载配置</span></span><br><span class="line"><span class="keyword">for</span> node <span class="keyword">in</span> <span class="string">"<span class="variable">$&#123;OCP_MASTERS[@]&#125;</span>"</span>; <span class="keyword">do</span></span><br><span class="line">  govc vm.power -on <span class="variable">$node</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 观察负载均衡器, 3 个 master 都起来了再进行下去</span></span><br><span class="line"><span class="comment"># curl -v -k https://api-int.ocp.vopsdev.com:22623/config/master</span></span><br><span class="line"><span class="comment"># curl -v -k https://api-int.ocp.vopsdev.com:22623/config/worker </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动所有 worker 节点. 节点会去 master 上下载配置</span></span><br><span class="line"><span class="keyword">for</span> node <span class="keyword">in</span> <span class="string">"<span class="variable">$&#123;OCP_WORKERS[@]&#125;</span>"</span>; <span class="keyword">do</span></span><br><span class="line">  govc vm.power -on <span class="variable">$node</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 到这儿这步一般已经结束了</span></span><br><span class="line">./openshift-install --dir=assets <span class="built_in">wait</span>-for bootstrap-complete --<span class="built_in">log</span>-level=debug</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> KUBECONFIG=<span class="string">'./assets/auth/kubeconfig'</span></span><br><span class="line">oc whoami</span><br><span class="line">oc version</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果有 pending 的证书请求就批准</span></span><br><span class="line">oc get csr</span><br><span class="line">oc get csr -o json | jq -r <span class="string">'.items[] | select(.status == &#123;&#125; ) | .metadata.name'</span> | xargs oc adm certificate approve</span><br><span class="line"></span><br><span class="line"><span class="comment"># 等待 worker 节点起来, 确保所有节点都是 Ready</span></span><br><span class="line">oc get nodes</span><br><span class="line"></span><br><span class="line"><span class="comment"># 等所有 cluster operator 状态 AVAILABLE: TRUE</span></span><br><span class="line">oc get clusteroperators</span><br><span class="line"></span><br><span class="line"><span class="comment"># 等待安装结束, 获取集群管理员密码</span></span><br><span class="line">./openshift-install --dir=assets <span class="built_in">wait</span>-for install-complete</span><br></pre></td></tr></table></figure><p>使用集群管理员 kubeadmin 登录 web console <img src="ocp-console-initial.png"></p><p>至此集群初步搭建完成, 后面将针对各个基础服务进行完善.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文记录了离线环境下以 UPI (User Provisioned Infrastructure) 模式初始安装 OCP 4.3.5 集群的步骤, 包括地址/DNS 名称规划, DHCP, 负载均衡配置, ignition 文件生成, 到最后的集群部署.&lt;/p&gt;
&lt;p&gt;IaaS 平台为 VMware vSphere 6.7U2. DDI 方案使用 Infoblox NIOS. 负载均衡方案使用 HAProxy&lt;/p&gt;
    
    </summary>
    
    
      <category term="RedHat" scheme="https://notes.yuanlinios.me/categories/RedHat/"/>
    
      <category term="Openshift" scheme="https://notes.yuanlinios.me/categories/RedHat/Openshift/"/>
    
    
      <category term="RedHat" scheme="https://notes.yuanlinios.me/tags/RedHat/"/>
    
      <category term="Openshift" scheme="https://notes.yuanlinios.me/tags/Openshift/"/>
    
  </entry>
  
  <entry>
    <title>实施 AWS Landing Zone 4: 定制 Landing Zone CodePipeline</title>
    <link href="https://notes.yuanlinios.me/2020-03-16/%E5%AE%9E%E6%96%BD-AWS-Landing-Zone-4-%E5%AE%9A%E5%88%B6-Landing-Zone-CodePipeline/"/>
    <id>https://notes.yuanlinios.me/2020-03-16/%E5%AE%9E%E6%96%BD-AWS-Landing-Zone-4-%E5%AE%9A%E5%88%B6-Landing-Zone-CodePipeline/</id>
    <published>2020-03-16T14:35:43.000Z</published>
    <updated>2020-03-17T04:26:10.244Z</updated>
    
    <content type="html"><![CDATA[<p>Landing Zone CodePipeline 默认使用一个启用了版本的 S3 bucket 作为代码来源: 当有新的配置文件 aws-landing-zone-configuration.zip 上传时, 触发 CodePipeline. 然而使用 versioned bucket 作为版本控制显然算不上最佳实践. 这里将修改 Landing Zone Codepipeline 使用 CodeCommit repo 作为代码来源. 这样就可以通过 git 来管理组织下所有 AWS 账号的配置基线.</p><a id="more"></a><p>进一步, 企业一般都有自己的代码管理平台 (bitbucket/gitlab/github), 并不见得愿意单单为了管理 Landing Zone 的配置而引入额外的 CodeCommit 代码仓库的维护成本. 实际上可以利用现有代码平台来保存 Landing Zone 配置, 其基本原理是做仓库镜像: 利用 CI 工具将特定分支的更新自动推送到 CodeCommit, 进而触发 CodePipeline.</p><h2 id="配置-codecommit">配置 CodeCommit</h2><h3 id="创建-codecommit-代码仓库">创建 CodeCommit 代码仓库</h3><p>到 Developer Tools, CodeCommit 下创建一个名为 landing-zone-configuration 的代码仓库 <img src="create-lz-codecommit-repo.png"></p><h3 id="创建-iam-policy-和服务账号">创建 IAM Policy 和服务账号</h3><p>到 IAM 下创建自定义 Policy: LandingZoneCodeCommitRepoAccess <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;Version&quot;: &quot;2012-10-17&quot;,</span><br><span class="line">    &quot;Statement&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;Effect&quot;: &quot;Allow&quot;,</span><br><span class="line">            &quot;Action&quot;: [</span><br><span class="line">                &quot;codecommit:BatchGet*&quot;,</span><br><span class="line">                &quot;codecommit:Create*&quot;,</span><br><span class="line">                &quot;codecommit:DeleteBranch&quot;,</span><br><span class="line">                &quot;codecommit:Get*&quot;,</span><br><span class="line">                &quot;codecommit:List*&quot;,</span><br><span class="line">                &quot;codecommit:Describe*&quot;,</span><br><span class="line">                &quot;codecommit:Put*&quot;,</span><br><span class="line">                &quot;codecommit:Post*&quot;,</span><br><span class="line">                &quot;codecommit:Merge*&quot;,</span><br><span class="line">                &quot;codecommit:Test*&quot;,</span><br><span class="line">                &quot;codecommit:Update*&quot;,</span><br><span class="line">                &quot;codecommit:GitPull&quot;,</span><br><span class="line">                &quot;codecommit:GitPush&quot;</span><br><span class="line">            ],</span><br><span class="line">            &quot;Resource&quot;: [</span><br><span class="line">                &quot;arn:aws:codecommit:&lt;region&gt;:&lt;accountid&gt;:landing-zone-configuration&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure> 接着创建一个 IAM user 作为服务账号: landingzone-repo-mirror, 附加 LandingZoneCodeCommitRepoAccess 策略. 配置这个用户的 Security credentials: HTTPS Git credentials for AWS CodeCommit <img src="https-codecommit-credentials.png"></p><p>我们并不会直接使用这个账户来修改 Landing Zone 配置. 这个账号仅仅用于同步私有代码仓库到 CodeCommit.</p><h2 id="使用-codecommit-作为-landing-zone-pipeline-构建源">使用 CodeCommit 作为 Landing Zone Pipeline 构建源</h2><p>进入 CodePipeline, 编辑 AWS-Landing-Zone-CodePipeline. Edit Source, Edit stage <img src="codepipeline-edit-source.png"></p><p>编辑原有的 Source</p><ul><li>Action provider: AWS CodeCommit</li><li>Repository name: landing-zone-configuration</li><li>Branch name: master</li><li>Change detection options: Amazon Cloudwatch Events</li><li>Output artifacts: SourceApp</li></ul><p><img src="pipeline-codecommit-source.png"></p><p>保存退出. 此时会提示将自动添加 AWS Cloudwatch Events rule. 这里我们需要 Events rule 来跟踪 CodeCommit 的变化进而触发 CodePipeline, 因此保持 "No resource updates needed for this source action change" 为默认的不勾选状态, 让 AWS 自动创建 Cloudwatch Events rule.</p><p><img src="pipeline-event-rule.png"></p><p>如果感兴趣可以到 Cloudwatch Events 下查看自动产生的规则.</p><h2 id="使用私有代码仓库保存-landing-zone-配置">使用私有代码仓库保存 Landing Zone 配置</h2><p>下面是使用 gitlab CI 的例子: Landing Zone 配置保存在企业内部的 gitlab 代码仓库中, 当 master 分支发生变更时触发 gitlab CI, 将代码库镜像到 AWS CodeCommit, 进而触发 Landing Zone CodePipeline.</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">image:</span> <span class="string">rtcamp/gitlab-aws-codecommit-mirror</span></span><br><span class="line"><span class="attr">stages:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">deploy</span></span><br><span class="line"></span><br><span class="line"><span class="attr">deploy to production:</span></span><br><span class="line">  <span class="attr">tags:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">aws</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">codecommit</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">landingzone</span></span><br><span class="line">  <span class="attr">stage:</span> <span class="string">deploy</span></span><br><span class="line">  <span class="attr">only:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">master</span></span><br><span class="line">  <span class="attr">script:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">git</span> <span class="string">clone</span> <span class="string">--mirror</span> <span class="string">https://$TOKEN_NAME:$TOKEN_VALUE@$SOURCE_REPO</span> <span class="string">rtSync</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">cd</span> <span class="string">rtSync</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">git</span> <span class="string">branch</span> <span class="string">-a</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">git</span> <span class="string">push</span> <span class="string">--mirror</span> <span class="string">https://$USERNAME:$SECRET@$DEST_REPO</span></span><br></pre></td></tr></table></figure><p>这里用到的几个 CI 变量:</p><ul><li>USERNAME: 就是前面配置的 IAM 服务账号 landing-zone-repo-mirror 下 HTTPS Git credentials for AWS CodeCommit 的用户名</li><li>DEST_REPO: AWS CodeCommit 代码库位置, 例如 git-codecommit.us-west-2.amazonaws.com/v1/repos/landing-zone-configuration</li><li>SECRET: landing-zone-repo-mirror 下 HTTPS Git credentials for AWS CodeCommit 的密码. 注意特殊字符转义</li><li>SOURCE_REPO: 内部代码库位置, 例如 gitlab.svc.vopsdev.com/devinfra/landing-zone-configuration.git</li><li>TOKEN_NAME, TOKEN_VALUE: 这里使用 deploy token 来获取私有代码库</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Landing Zone CodePipeline 默认使用一个启用了版本的 S3 bucket 作为代码来源: 当有新的配置文件 aws-landing-zone-configuration.zip 上传时, 触发 CodePipeline. 然而使用 versioned bucket 作为版本控制显然算不上最佳实践. 这里将修改 Landing Zone Codepipeline 使用 CodeCommit repo 作为代码来源. 这样就可以通过 git 来管理组织下所有 AWS 账号的配置基线.&lt;/p&gt;
    
    </summary>
    
    
      <category term="AWS" scheme="https://notes.yuanlinios.me/categories/AWS/"/>
    
      <category term="Landing Zone" scheme="https://notes.yuanlinios.me/categories/AWS/Landing-Zone/"/>
    
    
      <category term="AWS" scheme="https://notes.yuanlinios.me/tags/AWS/"/>
    
      <category term="Landing Zone" scheme="https://notes.yuanlinios.me/tags/Landing-Zone/"/>
    
  </entry>
  
  <entry>
    <title>离线部署 Openshift Container Platform 4.3 - 1: 准备离线资源</title>
    <link href="https://notes.yuanlinios.me/2020-03-15/%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B2-Openshift-Container-Platform-4-3-1-%E5%87%86%E5%A4%87%E7%A6%BB%E7%BA%BF%E8%B5%84%E6%BA%90/"/>
    <id>https://notes.yuanlinios.me/2020-03-15/%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B2-Openshift-Container-Platform-4-3-1-%E5%87%86%E5%A4%87%E7%A6%BB%E7%BA%BF%E8%B5%84%E6%BA%90/</id>
    <published>2020-03-15T23:11:31.000Z</published>
    <updated>2020-05-27T09:56:37.784Z</updated>
    
    <content type="html"><![CDATA[<p>本系列文章记录了 Openshift Container Platform (OCP) 4.3.5 离线部署的过程. 离线资源包括安装镜像, 所有样例 Image Stream, OperatorHub 下的 RedHat Operators. 虽然这是一个实验系列, 我会尽可能的按照生产实践来进行. <a id="more"></a></p><h2 id="实验环境">实验环境</h2><h3 id="跳板机">跳板机</h3><p>可以同时访问内外网, 同时具备科学上网的能力, 用来执行安装任务和离线材料准备</p><h3 id="私有镜像库">私有镜像库</h3><p>离线镜像会保存到私有镜像库中, 以供 OCP 安装和运行时使用, <strong>要求支持 version 2 schema 2 (manifest list)</strong>. 我这里选择的是 <strong>Quay 3</strong></p><ul><li><del>Nexus 目前还不支持 manifest list: <a href="https://issues.sonatype.org/browse/NEXUS-18546" target="_blank" rel="external nofollow noopener noreferrer">NEXUS-18546</a></del></li><li><del>Harbor 目前还不支持 manifest list: <a href="https://github.com/goharbor/harbor/issues/6522" target="_blank" rel="external nofollow noopener noreferrer">6522</a></del></li></ul><h2 id="关于镜像的获取">关于镜像的获取</h2><p>发现很多人误以为必须联系红帽销售, 签单之后才可以试用 OCP4, 实际上并不是这样. 注册一个<a href="https://developers.redhat.com" target="_blank" rel="external nofollow noopener noreferrer">开发者账号</a> 后就可以获得 quay.io, registry.redhat.io 的 pull secret 来进行测试实验了.</p><h2 id="准备离线安装介质">准备离线安装介质</h2><h3 id="获取目前的版本信息">获取目前的版本信息</h3><p>目前最新的 OCP 版本是 4.3.5. 从这里下载 oc 客户端</p><ul><li><a href="https://mirror.openshift.com/pub/openshift-v4/clients/ocp/4.3.5/openshift-client-linux-4.3.5.tar.gz" target="_blank" rel="external nofollow noopener noreferrer">https://mirror.openshift.com/pub/openshift-v4/clients/ocp/4.3.5/openshift-client-linux-4.3.5.tar.gz</a></li></ul><p>解压出来的二进制文件放到跳板机 PATH 下. 先看一下当前 4.3.5 的版本信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">oc adm release info quay.io/openshift-release-dev/ocp-release:4.3.5-x86_64</span><br><span class="line"></span><br><span class="line">Name:      4.3.5</span><br><span class="line">Digest:    sha256:64320fbf95d968fc6b9863581a92d373bc75f563a13ae1c727af37450579f61a</span><br><span class="line">Created:   2020-03-06T12:05:47Z</span><br><span class="line">OS/Arch:   linux/amd64</span><br><span class="line">Manifests: 366</span><br><span class="line"></span><br><span class="line">Pull From: quay.io/openshift-release-dev/ocp-release@sha256:64320fbf95d968fc6b9863581a92d373bc75f563a13ae1c727af37450579f61a</span><br><span class="line"></span><br><span class="line">Release Metadata:</span><br><span class="line">  Version:  4.3.5</span><br><span class="line">  Upgrades: 4.2.21, 4.2.22, 4.3.0, 4.3.1, 4.3.2, 4.3.3</span><br><span class="line">  Metadata:</span><br><span class="line">    description:</span><br><span class="line">  Metadata:</span><br><span class="line">    url: https://access.redhat.com/errata/RHBA-2020:0676</span><br><span class="line"></span><br><span class="line">Component Versions:</span><br><span class="line">  Kubernetes 1.16.2</span><br><span class="line">...</span><br></pre></td></tr></table></figure><h3 id="下载安装镜像">下载安装镜像</h3><p>实验环境中的 quay 在内网 https://quay.svc.vopsdev.com 提供服务. 事先创建好 namespace/organization: openshift-release-dev 用来存放安装镜像仓库</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> OCP_RELEASE=4.3.5</span><br><span class="line"><span class="built_in">export</span> ARCH=x86_64</span><br><span class="line"><span class="built_in">export</span> LOCAL_REGISTRY=<span class="string">'quay.svc.vopsdev.com'</span> </span><br><span class="line"><span class="built_in">export</span> LOCAL_REPOSITORY=<span class="string">'openshift-release-dev/ocp-v4.0-art-dev'</span></span><br><span class="line"><span class="built_in">export</span> UPSTREAM_REPO=<span class="string">'openshift-release-dev'</span></span><br><span class="line"><span class="built_in">export</span> RELEASE_NAME=<span class="string">"ocp-release"</span></span><br><span class="line"><span class="comment"># 将你从红帽获取的 pull secret 以及你的私有镜像库的 secret 加入到 pull-secret.json 中</span></span><br><span class="line"><span class="built_in">export</span> LOCAL_SECRET_JSON=<span class="string">"pull-secret.json"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果你需要科学上网</span></span><br><span class="line"><span class="built_in">export</span> HTTP_PROXY=...</span><br><span class="line"><span class="built_in">export</span> HTTPS_PROXY=...</span><br><span class="line"><span class="built_in">export</span> NO_PROXY=<span class="string">"quay.svc.vopsdev.com"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 这样 release 镜像以及安装需要的镜像会都同步到 quay.svc.vopsdev.com/openshift-release-dev/ocp-v4.0-art-dev 下了</span></span><br><span class="line">oc adm release mirror -a <span class="variable">$&#123;LOCAL_SECRET_JSON&#125;</span> --from=quay.io/<span class="variable">$&#123;UPSTREAM_REPO&#125;</span>/<span class="variable">$&#123;RELEASE_NAME&#125;</span>:<span class="variable">$&#123;OCP_RELEASE&#125;</span>-<span class="variable">$&#123;ARCH&#125;</span> --to-release-image=<span class="variable">$&#123;LOCAL_REGISTRY&#125;</span>/<span class="variable">$&#123;LOCAL_REPOSITORY&#125;</span>:<span class="variable">$&#123;OCP_RELEASE&#125;</span> --to=<span class="variable">$&#123;LOCAL_REGISTRY&#125;</span>/<span class="variable">$&#123;LOCAL_REPOSITORY&#125;</span></span><br></pre></td></tr></table></figure><p><code>oc adm release mirror</code> 命令完成后会输出下面类似的信息, 保存下来, 将来会用在 install-config.yaml 文件中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">imageContentSources:</span><br><span class="line">- mirrors:</span><br><span class="line">  - quay.svc.vopsdev.com&#x2F;openshift-release-dev&#x2F;ocp-v4.0-art-dev</span><br><span class="line">  source: quay.io&#x2F;openshift-release-dev&#x2F;ocp-release</span><br><span class="line">- mirrors:</span><br><span class="line">  - quay.svc.vopsdev.com&#x2F;openshift-release-dev&#x2F;ocp-v4.0-art-dev</span><br><span class="line">  source: quay.io&#x2F;openshift-release-dev&#x2F;ocp-v4.0-art-dev</span><br></pre></td></tr></table></figure><h3 id="提取安装程序">提取安装程序</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">oc adm release extract -a <span class="variable">$&#123;LOCAL_SECRET_JSON&#125;</span> --<span class="built_in">command</span>=openshift-install <span class="string">"<span class="variable">$&#123;LOCAL_REGISTRY&#125;</span>/<span class="variable">$&#123;LOCAL_REPOSITORY&#125;</span>:<span class="variable">$&#123;OCP_RELEASE&#125;</span>"</span></span><br></pre></td></tr></table></figure><p>会产生 openshift-install 二进制程序 (不要直接从 <a href="https://mirror.openshift.com/pub/openshift-v4/clients/ocp/4.3.5" target="_blank" rel="external nofollow noopener noreferrer">https://mirror.openshift.com/pub/openshift-v4/clients/ocp/4.3.5</a> 下载, 后面会有 sha256 匹配不上的问题)</p><h2 id="准备-image-stream-样例镜像">准备 Image Stream 样例镜像</h2><p>准备一个镜像列表, 然后使用 <code>oc image mirror</code>将镜像同步到私有仓库中</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cat sample-images.txt | <span class="keyword">while</span> <span class="built_in">read</span> line; <span class="keyword">do</span></span><br><span class="line">  target=$(<span class="built_in">echo</span> <span class="variable">$line</span> | sed <span class="string">'s/registry.redhat.io/quay.svc.vopsdev.com/'</span>)</span><br><span class="line">  oc image mirror -a <span class="variable">$&#123;LOCAL_SECRET_JSON&#125;</span> <span class="variable">$line</span> <span class="variable">$target</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><p>完整的镜像列表如何获取? 如果之前装过 OCP 4.3.5, 把 openshift-cluster-samples-operator 项目下 cluster-samples-operator pod 的 /opt/openshift 目录同步出来, 简单 grep 一下就都有了.</p><p>完整列表参考<a href="https://gist.github.com/yuanlinios/7eea8207083e649cbe07e108a22df00b" target="_blank" rel="external nofollow noopener noreferrer">这里</a></p><h2 id="准备-operatorhub-离线资源">准备 OperatorHub 离线资源</h2><p>首先构建 RedHat Operators 的 catalog image, 保存为 quay.svc.vopsdev.com/devinfra/redhat-operators:v1.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">oc adm catalog build --appregistry-endpoint https://quay.io/cnr --appregistry-org redhat-operators --to=quay.svc.vopsdev.com/devinfra/redhat-operators:v1</span><br></pre></td></tr></table></figure><p>这个 catalog image 相当于 RedHat Operators 的一个目录, 通过 catalog image 可以找到 RedHat Operators 的所有镜像. 而且 catalog image 使用 sha256 digest 来引用镜像, 能够确保应用有稳定可重复的部署.</p><p>然后使用 catalog image 同步 RedHat Operators 的所有镜像到私有仓库. 按照官方文档的做法是</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">oc adm catalog mirror quay.svc.vopsdev.com&#x2F;devinfra&#x2F;redhat-operators:v1 quay.svc.vopsdev.com</span><br></pre></td></tr></table></figure><p>这个命令结束后会产生 redhat-operators-manifests 目录, 下面有两个文件: mapping.txt 和 imageContentSourcePolicy.yaml.</p><p>然而这么做目前还有问题 <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1800674" target="_blank" rel="external nofollow noopener noreferrer">1800674</a>: 同步出来的镜像 manifest digest 不对, 导致后面离线安装 operator 时会报镜像无法获取的错误. 暂时可以使用上面 bugzilla 链接里给出的临时解决方案: <code>skopeo copy --all</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">cat redhat-operators-manifests/mapping.txt | <span class="keyword">while</span> <span class="built_in">read</span> line; <span class="keyword">do</span></span><br><span class="line">  origin=$(<span class="built_in">echo</span> <span class="variable">$line</span> | cut -d= -f1)</span><br><span class="line">  target=$(<span class="built_in">echo</span> <span class="variable">$line</span> | cut -d= -f2)</span><br><span class="line">  <span class="keyword">if</span> [[ <span class="string">"<span class="variable">$origin</span>"</span> =~ <span class="string">"sha256"</span> ]]; <span class="keyword">then</span></span><br><span class="line">    tag=$(<span class="built_in">echo</span> <span class="variable">$origin</span> | cut -d: -f2 | cut -c -8)</span><br><span class="line">    skopeo copy --all docker://<span class="variable">$origin</span> docker://<span class="variable">$target</span>:<span class="variable">$tag</span></span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">    skopeo copy --all docker://<span class="variable">$origin</span> docker://<span class="variable">$target</span></span><br><span class="line">  <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本系列文章记录了 Openshift Container Platform (OCP) 4.3.5 离线部署的过程. 离线资源包括安装镜像, 所有样例 Image Stream, OperatorHub 下的 RedHat Operators. 虽然这是一个实验系列, 我会尽可能的按照生产实践来进行.
    
    </summary>
    
    
      <category term="RedHat" scheme="https://notes.yuanlinios.me/categories/RedHat/"/>
    
      <category term="Openshift" scheme="https://notes.yuanlinios.me/categories/RedHat/Openshift/"/>
    
    
      <category term="RedHat" scheme="https://notes.yuanlinios.me/tags/RedHat/"/>
    
      <category term="Openshift" scheme="https://notes.yuanlinios.me/tags/Openshift/"/>
    
  </entry>
  
  <entry>
    <title>实施 AWS Landing Zone 3: 部署 Initiation Stack</title>
    <link href="https://notes.yuanlinios.me/2020-03-12/%E5%AE%9E%E6%96%BD-AWS-Landing-Zone-3-%E9%83%A8%E7%BD%B2-Initiation-Stack/"/>
    <id>https://notes.yuanlinios.me/2020-03-12/%E5%AE%9E%E6%96%BD-AWS-Landing-Zone-3-%E9%83%A8%E7%BD%B2-Initiation-Stack/</id>
    <published>2020-03-12T16:42:14.000Z</published>
    <updated>2020-03-16T11:17:03.890Z</updated>
    
    <content type="html"><![CDATA[<p>本文将介绍 Landing Zone Initiation Stack 的部署, 参数解析以及一些容易出错的地方</p><a id="more"></a><h2 id="创建-initiation-stack">创建 Initiation Stack</h2><p>登入组织账号 (即这里的 yl-master), 进入 Cloudformation 服务. 创建一个新的 stack: <img src="create-initiation-stack.png"></p><p>template s3 url 使用 https://s3.amazonaws.com/solutions-reference/aws-landing-zone/v2.3.1/aws-landing-zone-initiation.template</p><p>Landing Zone Initiation stack 是一个比较庞大的 cloudformation template, 大约近 6000 行配置, 其核心部分是 5 个 Lambda 和 7 个 Step Functions. 但这还不是 Landing Zone 的全貌, 因为它引用了很多外部资源, 例如保存在 S3 里的 Lambda 函数实现. 在 Initiation stack template 中大部分内容 (约 2/3) 是 7 个 StateMachines (Step Functions) 的定义.</p><div class="note info">            <p>AWS 会对 Landing Zone 进行更新升级, 最新版本的信息可以从 https://aws.amazon.com/solutions/aws-landing-zone/ 获取</p>          </div><h3 id="参数详解">参数详解</h3><h4 id="landing-zone-core-account-configuration">Landing Zone Core Account Configuration</h4><p>核心账号配置部分</p><ul><li>Shared Service Account Email Address: 填写现存共享服务账号的邮件地址. 如果你的现存环境中还没有共享服务账号也没关系, 如果有需要, Landing Zone 可以帮你创建</li><li>Log Archive Account Email Address: 填写现存的日志账号邮件地址. 同上</li><li>Security Account Email Address: 填写现存的安全审计账号的邮件地址. 同上</li><li>Nest OU Name Delimiter: Colon (:) 使用冒号作为 OU 路径的界定符. 保持默认即可</li><li>Core OU Name: vopsdev-landing-zone:core 根据上一篇文章中的 OU 规划来填写, 这里是核心账号所在的 OU</li><li>Non Core OU Names: vopsdev-landing-zone:production,vopsdev-landing-zone:department,vopsdev-landing-zone:staging,vopsdev-landing-zone:sandbox 根据上一篇文章中的 OU 规划来写, 这里是非核心账号所在的 OU. 使用逗号作为分隔符</li><li>Security Alert Email Address: 接受安全告警邮件的地址. 具体是来自 GuardDuty Finding, Cloudwatch Alarm, Config Rules Compliance Status Change 的消息</li><li>Lock StackSetsExecution Role: Yes. 是否锁定角色 AWSCloudFormationStackSetExecutionRole. Landing Zone 通过 Stackset 来部署资源基线到各个受控账号, 因此会在各个账号下创建角色 AWSCloudFormationStackSetExecutionRole 来执行 stack instance. 该角色具有账户的管理员权限. 锁定该角色的含义是只允许特定的实体 (principal) 来承担 (assume role).</li><li>Subscribe All Change Events Email To Topic: No. 是否订阅所有的配置变更事件. 按需设置</li><li>All Change Events Email: 接受配置变更事件通知的邮件地址</li></ul><h4 id="landing-zone-pipeline-configuration">Landing Zone Pipeline Configuration</h4><p>流水线配置部分</p><ul><li>Pipeline Approval Stage: Yes. 给 Landing Zone 配置流水线添加手动批准的步骤</li><li>Pipeline Approval Email Address: 批准 Landing Zone 配置流水线的邮件地址</li><li>Auto Build Landing Zone: No. 是否在 initiation stack 创建完成后立刻启动配置流水线. 我们需要对配置进行定制, 因此这里一定需要设置为 No</li></ul><h4 id="shared-services-vpc-configuration">Shared Services VPC Configuration</h4><p>共享服务的 VPC 配置部分</p><ul><li>Shared Services VPC Options: Shared-Services-Network-3-AZs. 选择共享服务账号的 VPC 类型. 仅仅用来生成初始配置. 并不会在 Initiation 阶段创建实际的资源. 后面可以按自己的需要修改初始配置</li><li>Shared Services VPC CIDR: 100.65.0.0/16 共享服务 VPC 的网段. 同上</li></ul><h4 id="vpc-flow-logs-retention-policy">VPC Flow Logs Retention Policy</h4><p>VPC Flow Logs 的留存策略</p><ul><li>VPC Flow Logs Retention In Days: 90</li></ul><h4 id="aws-security-and-configuration-services">AWS Security and Configuration Services</h4><p>安全和配置管理部分</p><ul><li>Enable AWS Security and Configuraiton Monitoring in: All regions 在哪些区域启用安全和配置管理服务 (GuardDuty, Config 之类). 同样也仅仅用于生成初始化配置. 后期可以按需要修改</li></ul><h4 id="aws-config-rules">AWS Config Rules</h4><p>启用哪些 AWS Config Rules. 按默认都启用即可</p><h4 id="add-on-publisher-configuraiton">Add-On Publisher Configuraiton</h4><p>Add-On 产品更新配置部分</p><ul><li>AWS Manages Service Catalog Add-On Portfolio? Manual Updates 手动管理 Add-On Portfolio 的版本更新</li><li>Add-On Update Notification Email: Add-On 更新通知的邮件地址. 按需设置</li></ul><h2 id="关于通知邮件地址">关于通知邮件地址</h2><h3 id="结论">结论</h3><p>不同的 AWS 账号需要不同的邮件地址, 这基本不会出错. 但是在部署 Landing Zone Initiation stack 时需要注意 Security Account Email Address 的邮件地址不能用来接收 Security Alert. 即 Security Alert Email Address 和 Security Account Email Address 不能是同一个.</p><h3 id="原因">原因</h3><p>Initiaton stack 中有这样的资源定义</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">LandingZoneConfigDeployer:</span></span><br><span class="line">  <span class="attr">Type:</span> <span class="string">Custom::ConfigDeployer</span></span><br><span class="line">  <span class="attr">Properties:</span></span><br><span class="line">    <span class="attr">metrics_flag:</span> <span class="type">!FindInMap</span> <span class="string">[Solution,</span> <span class="string">Metrics,</span> <span class="string">SendAnonymousData]</span></span><br><span class="line">    <span class="attr">email_list:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="type">!Ref</span> <span class="string">SharedServicesAccountEmail</span></span><br><span class="line">      <span class="bullet">-</span> <span class="type">!Ref</span> <span class="string">LoggingAccountEmail</span></span><br><span class="line">      <span class="bullet">-</span> <span class="type">!Ref</span> <span class="string">SecurityAccountEmail</span></span><br><span class="line">      <span class="bullet">-</span> <span class="type">!Ref</span> <span class="string">SecurityAlertEmail</span></span><br></pre></td></tr></table></figure><p>而 Lambda 函数 LandingZoneDeploymentLambda 的 config_deployer.py 中会检查 email_list 中是否有重复地址 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">unique_email_validator</span><span class="params">(email_list)</span>:</span></span><br><span class="line">    result = set([x <span class="keyword">for</span> x <span class="keyword">in</span> email_list <span class="keyword">if</span> email_list.count(x) &gt; <span class="number">1</span>])</span><br><span class="line">    duplicate_list = list(result)</span><br><span class="line">    logger.info(<span class="string">"Duplicate Emails: &#123;&#125;"</span>.format(duplicate_list))</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> duplicate_list:</span><br><span class="line">        logger.info(<span class="string">"Duplicate emails not found"</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> Exception(<span class="string">"Found duplicate email(s) &#123;&#125; in the parameters."</span>.format(duplicate_list))</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">config_deployer</span><span class="params">(event)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        s3 = S3(logger)</span><br><span class="line">    ...</span><br><span class="line">    <span class="comment"># Check if the emails are Unique</span></span><br><span class="line">    unique_email_validator(event.get(<span class="string">'email_list'</span>))</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将介绍 Landing Zone Initiation Stack 的部署, 参数解析以及一些容易出错的地方&lt;/p&gt;
    
    </summary>
    
    
      <category term="AWS" scheme="https://notes.yuanlinios.me/categories/AWS/"/>
    
      <category term="Landing Zone" scheme="https://notes.yuanlinios.me/categories/AWS/Landing-Zone/"/>
    
    
      <category term="AWS" scheme="https://notes.yuanlinios.me/tags/AWS/"/>
    
      <category term="Landing Zone" scheme="https://notes.yuanlinios.me/tags/Landing-Zone/"/>
    
  </entry>
  
  <entry>
    <title>实施 AWS Landing Zone 2: 实施前场景与目标规划</title>
    <link href="https://notes.yuanlinios.me/2020-03-12/%E5%AE%9E%E6%96%BD-AWS-Landing-Zone-2-%E5%AE%9E%E6%96%BD%E5%89%8D%E5%9C%BA%E6%99%AF%E4%B8%8E%E7%9B%AE%E6%A0%87%E8%A7%84%E5%88%92/"/>
    <id>https://notes.yuanlinios.me/2020-03-12/%E5%AE%9E%E6%96%BD-AWS-Landing-Zone-2-%E5%AE%9E%E6%96%BD%E5%89%8D%E5%9C%BA%E6%99%AF%E4%B8%8E%E7%9B%AE%E6%A0%87%E8%A7%84%E5%88%92/</id>
    <published>2020-03-12T15:33:52.000Z</published>
    <updated>2020-03-17T02:03:56.703Z</updated>
    
    <content type="html"><![CDATA[<p>该系列文章将通过一个虚拟场景介绍如何在现存的多账户环境下实施 Landing Zone (brownfield deployment). 本文介绍实施前企业多账户环境的现状, 迁移的目标规划和准备工作</p><a id="more"></a><h2 id="实施前状况">实施前状况</h2><p>VOPSDEV.COM 目前在 AWS Organization 下按照部门职能和项目环境类型划分了多个组织单元 (Organization Unit, OU) 并采用了多个账号以实现职责/资源分离:</p><p><img src="original-organization-structure.png"></p><p>业务的生产环境和测试环境账号分别在 production OU 和 staging OU 下, 开发人员的沙盒环境账号创建在 sandbox OU 下, 各个职能部门的账号创建在 department OU 下. Organization master, 安全, 审计日志, 公共服务相关账号在 core OU 下. 具体分布如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">Root&#x2F;</span><br><span class="line">  vopsdev&#x2F;</span><br><span class="line">    production&#x2F;</span><br><span class="line">      yl-production</span><br><span class="line">    staging&#x2F;</span><br><span class="line">      yl-stagin</span><br><span class="line">    department&#x2F;</span><br><span class="line">      yl-marketing</span><br><span class="line">      yl-hr</span><br><span class="line">      yl-network</span><br><span class="line">      yl-collaboration</span><br><span class="line">      yl-itdev</span><br><span class="line">      yl-itops</span><br><span class="line">    sandbox&#x2F;</span><br><span class="line">      yl-sandbox-1</span><br><span class="line">      yl-sandbox-2</span><br><span class="line">    core&#x2F;</span><br><span class="line">      yl-log-archive</span><br><span class="line">      yl-security</span><br><span class="line">      yl-shared-services</span><br><span class="line">      yl-master</span><br></pre></td></tr></table></figure><p>各个账号可以自己独立的创建 VPC. 但是如果 VPC 需要接入公司 VPN, 则需要经过安全部门审批. 经批准后, 账号的 VPC 可以通过 transit gateway attachment 连接到网络部门账号 yl-network 下的 transit gateway 来访问内网资源. DNS, Active Directory 之类的公共服务已经在共享服务账号 yl-shared-services 下创建完成. SSO 在组织账号 yl-master 下配置完成, 使用 AD-Connector 连接到共享服务账号下的 Active Directory 作为身份认证源.</p><p>这套环境目前面临的问题是: 由于缺少统一的配置管理方案, 各个账户初始化完成, 交付给用户使用后, 难以继续维持配置基线. 新的配置基线发布后又很难应用到现存的账户. 为此考虑在尽量重用现有服务的基础上实施 Landing Zone.</p><h2 id="实施规划">实施规划</h2><p>在组织根下创建一个新的 OU 分支, 后面会将 Landing Zone 应用到这个 OU 分支上 <img src="intermediate-organization-structure.png"></p><p>然后逐步将现有账号迁移到新的 OU 分支下, 由 Landing Zone Codepipeline 创建并维持基线资源. 而所有新的账号将通过 Landing Zone 的 Accounting Vending Machine 自动创建到新的 OU 分支下.</p><p>所有账户迁移完成后即可删除旧的 OU 分支, 最终的账号分布如下 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">Root&#x2F;</span><br><span class="line">  vopsdev-landing-zone&#x2F;</span><br><span class="line">    production&#x2F;</span><br><span class="line">      yl-production</span><br><span class="line">    staging&#x2F;</span><br><span class="line">      yl-stagin</span><br><span class="line">    department&#x2F;</span><br><span class="line">      yl-marketing</span><br><span class="line">      yl-hr</span><br><span class="line">      yl-network</span><br><span class="line">      yl-collaboration</span><br><span class="line">      yl-itdev</span><br><span class="line">      yl-itops</span><br><span class="line">    sandbox&#x2F;</span><br><span class="line">      yl-sandbox-1</span><br><span class="line">      yl-sandbox-2</span><br><span class="line">    core&#x2F;</span><br><span class="line">      yl-log-archive</span><br><span class="line">      yl-security</span><br><span class="line">      yl-shared-services</span><br><span class="line">      yl-master</span><br></pre></td></tr></table></figure></p><h2 id="资源限制">资源限制</h2><p>对于一个新创建的账号, 其 AWS Organization 下默认只能加入很少几个账号 (2个?) 无法满足 Landing Zone 的要求. 如果你想从头开始实施, 请确保提升组织账号的 AWS Organization Account Limit (需要通过给 AWS support 提交 ticket)</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;该系列文章将通过一个虚拟场景介绍如何在现存的多账户环境下实施 Landing Zone (brownfield deployment). 本文介绍实施前企业多账户环境的现状, 迁移的目标规划和准备工作&lt;/p&gt;
    
    </summary>
    
    
      <category term="AWS" scheme="https://notes.yuanlinios.me/categories/AWS/"/>
    
      <category term="Landing Zone" scheme="https://notes.yuanlinios.me/categories/AWS/Landing-Zone/"/>
    
    
      <category term="AWS" scheme="https://notes.yuanlinios.me/tags/AWS/"/>
    
      <category term="Landing Zone" scheme="https://notes.yuanlinios.me/tags/Landing-Zone/"/>
    
  </entry>
  
  <entry>
    <title>实施 AWS Landing Zone 1: 背景及架构介绍</title>
    <link href="https://notes.yuanlinios.me/2020-03-08/%E5%AE%9E%E6%96%BD-AWS-Landing-Zone-1-%E8%83%8C%E6%99%AF%E5%8F%8A%E6%9E%B6%E6%9E%84%E4%BB%8B%E7%BB%8D/"/>
    <id>https://notes.yuanlinios.me/2020-03-08/%E5%AE%9E%E6%96%BD-AWS-Landing-Zone-1-%E8%83%8C%E6%99%AF%E5%8F%8A%E6%9E%B6%E6%9E%84%E4%BB%8B%E7%BB%8D/</id>
    <published>2020-03-08T14:37:50.000Z</published>
    <updated>2020-03-09T06:26:41.099Z</updated>
    
    <content type="html"><![CDATA[<p>本文介绍了 AWS 多账号管理的需求, Landing Zone 方案的背景及基本结构, Landing Zone 方案和 Control Tower 的对比与选择</p><a id="more"></a><h2 id="多账户的需求和面临的挑战">多账户的需求和面临的挑战</h2><p>随着越来越多的工作负载迁移到 AWS 公有云, 过去的单个账户逐渐无法满足复杂环境的需要: 难以清晰界定的职责边界, 资源无法有效隔离, 对不同部门团队的计费和审计等. 此时会自然而然的引入多账户策略: 不同部门/团队, 不同环境使用不同的 AWS 账号. 优点很明显</p><ul><li>降低触及单个账户资源上限的风险</li><li>不同团队的职责分离</li><li>开发/测试/生产环境的资源隔离</li><li>减少人为故障的影响范围</li></ul><p>AWS Organization 提供了多账户环境的基础. 然而创建账户, 加入组织并完成初始化基线配置是一项繁琐的工作. 如何确保各个账户的合规性和配置的一致性? 如果后期配置基线发生变更, 又如何将基线变更追溯的应用到所有的现存账户上? Landing Zone 方案旨在解决这些问题.</p><h2 id="landing-zone-简介">Landing Zone 简介</h2><p>Landing Zone 是 AWS 在 2018 年推出的一套解决方案 (注意: 并不是一个 AWS 原生服务), 用来帮助用户根据 AWS 最佳实践, 快速建立安全的多账户环境</p><p><img src="aws-landing-zone-architecture.png"></p><p>上图为 Landing Zone 的基本结构:</p><ul><li>核心账户: master, shared services, log archive, security<ul><li>Master: 也称为 Organization account. 是创建 AWS Organization, Landing Zone 基础组件 (Service Catalog, CodePipeline, State Machines) 的地方</li><li>Shared Services: 公共服务所在的账号</li><li>Log Archive: 存放所有账户 Cloudtrail 和 Config log的 S3 bucket 所在的账户</li><li>Security: Guarduty master 所在账户, 全局 admin/readonly 角色所在账户</li></ul></li><li>Accounting Vending Machine (AVM): 是 Landing Zone 在 Service Catalog 里创建的一个产品, 通过运行 AVM 来创建新账户并应用配置基线</li><li>Landing Zone Codepipline: 配置变更流水线. 触发该流水线来更改核心账户资源, Service Control Policies 和基线资源等. 该流水线最终会调用 AVM 将基线资源变更应用到所有账户上</li></ul><p>各个组件/服务更深入的介绍将在后续的系列文章中给出.</p><h2 id="control-tower-还是-landing-zone">Control Tower 还是 Landing Zone?</h2><p>Landing Zone方案涉及多种AWS服务, 其部署较为复杂, 因此在 2019 年 AWS 推出了 Control Tower 服务, 旨在帮助用户通过最简单的方式设置<strong>全新</strong>的多账户 AWS 环境. 其背后依然是 Landing Zone, 但是隐藏了实现细节和基础服务.</p><p>用户在实际生产环境下该如何选择?</p><ul><li>Control Tower 只适用于全新部署 (greenfield). 如果你已经建立了多账户的环境 (Organization, SSO), 又不想迁移账户, 此时应该选择 Landing Zone</li><li>Landing Zone 具有更强的定制化 (当然这也正是它更加复杂的原因), 所有的基线资源都通过 Cloudformation template 创建, 用户可以方便的修改预设基线资源或者添加自定义资源</li><li>Landing Zone提供了 CI/CD 流水线, 用户可以将其配置保存在代码仓库中以实现版本控制</li></ul><h2 id="在现存多账户环境下实施-landing-zone">在现存多账户环境下实施 Landing Zone</h2><p>由于实际环境的复杂性, Landing Zone 方案在实施 AWS 最佳实践的同时, 不可避免的会对用户的环境做一些假设, 因此其部署的某些服务或者服务的设置对特定的用户可能并不适用. 此时需要对其预设配置进行调整.</p><p>举个例子, 默认 Landing Zone 会在所有账号和共享服务账号 (shared services account) 之间做 VPC peering 以便所有账户可以访问共享服务账号中的公共服务. 然而 VPC peering 本身有诸多限制, 企业环境下可能更多会选择 transit VPC 架构或者使用 transit gateway. 此时用户需要修改 Landing Zone 配置模板以匹配自身环境. 在系列的后续文章中我将逐一列示.</p><div class="note warning">            <p>AWS 声明 Landing Zone 必须由客户的 AWS account 团队或者经过认证的合作伙伴来部署, 以确保方案的实施成功. 如果你想自己实施, 请自行评估风险</p>          </div>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文介绍了 AWS 多账号管理的需求, Landing Zone 方案的背景及基本结构, Landing Zone 方案和 Control Tower 的对比与选择&lt;/p&gt;
    
    </summary>
    
    
      <category term="AWS" scheme="https://notes.yuanlinios.me/categories/AWS/"/>
    
      <category term="Landing Zone" scheme="https://notes.yuanlinios.me/categories/AWS/Landing-Zone/"/>
    
    
      <category term="AWS" scheme="https://notes.yuanlinios.me/tags/AWS/"/>
    
      <category term="Landing Zone" scheme="https://notes.yuanlinios.me/tags/Landing-Zone/"/>
    
  </entry>
  
</feed>
