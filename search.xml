<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>离线部署 Openshift Container Platform 4.3 - 2: 初始安装</title>
    <url>/2020-03-16/%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B2-Openshift-Container-Platform-4-3-2-%E5%88%9D%E5%A7%8B%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[<p>本文记录了离线环境下以 UPI (User Provisioned Infrastructure) 模式初始安装 OCP 4.3.5 集群的步骤, 包括地址/DNS 名称规划, DHCP, 负载均衡配置, ignition 文件生成, 到最后的集群部署.</p>
<p>IaaS 平台为 VMware vSphere 6.7U2. DDI 方案使用 Infoblox NIOS. 负载均衡方案使用 HAProxy</p>
<a id="more"></a>
<h2 id="dns-及-ip-地址规划">DNS 及 IP 地址规划</h2>
<table>
<colgroup>
<col style="width: 31%">
<col style="width: 5%">
<col style="width: 27%">
<col style="width: 35%">
</colgroup>
<thead>
<tr class="header">
<th>DNS 记录</th>
<th>类型</th>
<th>值</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ocp-bootstrap.int.vopsdev.com</td>
<td>A</td>
<td>192.168.11.119</td>
<td>bootstrap 节点. A 记录对应的 PTR 记录也同时创建出来, 下同</td>
</tr>
<tr class="even">
<td>ocp-node-0.int.vopsdev.com</td>
<td>A</td>
<td>192.168.11.120</td>
<td>master 节点</td>
</tr>
<tr class="odd">
<td>ocp-node-1.int.vopsdev.com</td>
<td>A</td>
<td>192.168.11.121</td>
<td>master 节点</td>
</tr>
<tr class="even">
<td>ocp-node-2.int.vopsdev.com</td>
<td>A</td>
<td>192.168.11.122</td>
<td>master 节点</td>
</tr>
<tr class="odd">
<td>ocp-node-3.int.vopsdev.com</td>
<td>A</td>
<td>192.168.11.123</td>
<td>worker 节点</td>
</tr>
<tr class="even">
<td>ocp-node-4.int.vopsdev.com</td>
<td>A</td>
<td>192.168.11.124</td>
<td>worker 节点</td>
</tr>
<tr class="odd">
<td>ocp-node-5.int.vopsdev.com</td>
<td>A</td>
<td>192.168.11.125</td>
<td>worker 节点</td>
</tr>
<tr class="even">
<td>api.ocp.vopsdev.com</td>
<td>CNAME</td>
<td>lb-if-192-168-11-249.int.vopsdev.com</td>
<td>别名, 指向现有的 LB 记录</td>
</tr>
<tr class="odd">
<td>api-int.ocp.vopsdev.com</td>
<td>CNAME</td>
<td>lb-if-192-168-11-249.int.vopsdev.com</td>
<td>如果有需要, api 和 api-int 可以分别指向内部/外部 LB</td>
</tr>
<tr class="even">
<td>*.apps.ocp.vopsdev.com</td>
<td>CNAME</td>
<td>lb-if-192-168-11-249.int.vopsdev.com</td>
<td></td>
</tr>
<tr class="odd">
<td>etcd-0.ocp.vopsdev.com</td>
<td>CNAME</td>
<td>ocp-node-0.int.vopsdev.com</td>
<td></td>
</tr>
<tr class="even">
<td>etcd-1.ocp.vopsdev.com</td>
<td>CNAME</td>
<td>ocp-node-1.int.vopsdev.com</td>
<td></td>
</tr>
<tr class="odd">
<td>etcd-2.ocp.vopsdev.com</td>
<td>CNAME</td>
<td>ocp-node-2.int.vopsdev.com</td>
<td></td>
</tr>
<tr class="even">
<td>_etcd-server-ssl._tcp.ocp.vopsdev.com</td>
<td>SRV</td>
<td>0 10 2380 etcd-0.ocp.vopsdev.com</td>
<td></td>
</tr>
<tr class="odd">
<td>_etcd-server-ssl._tcp.ocp.vopsdev.com</td>
<td>SRV</td>
<td>0 10 2380 etcd-1.ocp.vopsdev.com</td>
<td></td>
</tr>
<tr class="even">
<td>_etcd-server-ssl._tcp.ocp.vopsdev.com</td>
<td>SRV</td>
<td>0 10 2380 etcd-2.ocp.vopsdev.com</td>
<td></td>
</tr>
</tbody>
</table>
<h2 id="dhcp">DHCP</h2>
<p>所有 OCP 节点都通过 DHCP 自动获取 IP. DHCP 上做预留 (在 Infoblox 里叫 fixed address), 确保每个节点都可以获取预先规划的地址.</p>
<p><img src="infoblox-ocp-dhcp-fixed-addresses.png"></p>
<p>由于节点 MAC 地址要在虚拟机部署之后才能确定, 这里先随便填写. 后面部署节点时会通过 Infoblox API 动态更新 fixed address 记录的 MAC 字段.</p>
<h2 id="负载均衡">负载均衡</h2>
<p>负载均衡器目前使用是 HAProxy, 简单写几个四层 LB 配置 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">frontend vs_ocp_master_6443</span><br><span class="line">    mode tcp</span><br><span class="line">    option tcplog</span><br><span class="line">    <span class="built_in">bind</span> 192.168.11.249:6443</span><br><span class="line">    default_backend pl_ocp_master_6443</span><br><span class="line"></span><br><span class="line">frontend vs_ocp_master_22623</span><br><span class="line">    mode tcp</span><br><span class="line">    option tcplog</span><br><span class="line">    <span class="built_in">bind</span> 192.168.11.249:22623</span><br><span class="line">    default_backend pl_ocp_master_22623</span><br><span class="line"></span><br><span class="line">frontend vs_ocp_router_80</span><br><span class="line">    mode tcp</span><br><span class="line">    option tcplog</span><br><span class="line">    <span class="built_in">bind</span> 192.168.11.249:80</span><br><span class="line">    default_backend pl_ocp_router_80</span><br><span class="line"></span><br><span class="line">frontend vs_ocp_router_443</span><br><span class="line">    mode tcp</span><br><span class="line">    option tcplog</span><br><span class="line">    <span class="built_in">bind</span> 192.168.11.249:443</span><br><span class="line">    default_backend pl_ocp_router_443</span><br><span class="line"></span><br><span class="line">backend pl_ocp_master_6443</span><br><span class="line">    mode tcp</span><br><span class="line">    balance <span class="built_in">source</span></span><br><span class="line">    server ocp-node-0 ocp-node-0.int.vopsdev.com:6443 check</span><br><span class="line">    server ocp-node-1 ocp-node-1.int.vopsdev.com:6443 check</span><br><span class="line">    server ocp-node-2 ocp-node-2.int.vopsdev.com:6443 check</span><br><span class="line">    server ocp-bootstrap ocp-bootstrap.int.vopsdev.com:6443 check</span><br><span class="line"></span><br><span class="line">backend pl_ocp_master_22623</span><br><span class="line">    mode tcp</span><br><span class="line">    balance <span class="built_in">source</span></span><br><span class="line">    server ocp-node-0 ocp-node-0.int.vopsdev.com:22623 check</span><br><span class="line">    server ocp-node-1 ocp-node-1.int.vopsdev.com:22623 check</span><br><span class="line">    server ocp-node-2 ocp-node-2.int.vopsdev.com:22623 check</span><br><span class="line">    server ocp-bootstrap ocp-bootstrap.int.vopsdev.com:22623 check</span><br><span class="line"></span><br><span class="line">backend pl_ocp_router_80</span><br><span class="line">    mode tcp</span><br><span class="line">    balance <span class="built_in">source</span></span><br><span class="line">    server ocp-node-3 ocp-node-3.int.vopsdev.com:80 check</span><br><span class="line">    server ocp-node-4 ocp-node-4.int.vopsdev.com:80 check</span><br><span class="line">    server ocp-node-5 ocp-node-5.int.vopsdev.com:80 check</span><br><span class="line"></span><br><span class="line">backend pl_ocp_router_443</span><br><span class="line">    mode tcp</span><br><span class="line">    balance <span class="built_in">source</span></span><br><span class="line">    server ocp-node-3 ocp-node-3.int.vopsdev.com:443 check</span><br><span class="line">    server ocp-node-4 ocp-node-4.int.vopsdev.com:443 check</span><br><span class="line">    server ocp-node-5 ocp-node-5.int.vopsdev.com:443 check</span><br></pre></td></tr></table></figure></p>
<h2 id="准备-ignition-文件">准备 ignition 文件</h2>
<h3 id="编写-install-config.yaml">编写 install-config.yaml</h3>
<p>根据环境的实际情况编写 install-config.yaml. 下面是我的实验环境所使用的版本 <figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">baseDomain:</span> <span class="string">vopsdev.com</span></span><br><span class="line"><span class="attr">compute:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">hyperthreading:</span> <span class="string">Enabled</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">worker</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">0</span></span><br><span class="line"><span class="attr">controlPlane:</span></span><br><span class="line">  <span class="attr">hyperthreading:</span> <span class="string">Enabled</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">master</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">3</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">ocp</span></span><br><span class="line"><span class="attr">platform:</span></span><br><span class="line">  <span class="attr">vsphere:</span></span><br><span class="line">    <span class="attr">vcenter:</span> <span class="string">vc.int.vopsdev.com</span></span><br><span class="line">    <span class="attr">username:</span> <span class="string">xxx</span></span><br><span class="line">    <span class="attr">password:</span> <span class="string">xxx</span></span><br><span class="line">    <span class="attr">datacenter:</span> <span class="string">HDC</span></span><br><span class="line">    <span class="attr">defaultDatastore:</span> <span class="string">DS-SHARED</span></span><br><span class="line"><span class="attr">fips:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">networking:</span></span><br><span class="line">  <span class="attr">networkType:</span> <span class="string">OpenShiftSDN</span></span><br><span class="line">  <span class="attr">clusterNetworks:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">cidr:</span> <span class="number">10.9</span><span class="number">.0</span><span class="number">.0</span><span class="string">/20</span></span><br><span class="line">    <span class="attr">hostPrefix:</span> <span class="number">24</span></span><br><span class="line">  <span class="attr">serviceNetwork:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="number">172.16</span><span class="number">.0</span><span class="number">.0</span><span class="string">/20</span></span><br><span class="line"><span class="attr">sshKey:</span> <span class="string">'ssh-rsa ...'</span></span><br><span class="line"><span class="attr">additionalTrustBundle:</span> <span class="string">|</span></span><br><span class="line">  <span class="string">-----BEGIN</span> <span class="string">CERTIFICATE-----</span></span><br><span class="line">  <span class="string">...</span></span><br><span class="line">  <span class="string">-----END</span> <span class="string">CERTIFICATE-----</span></span><br><span class="line"><span class="attr">imageContentSources:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">mirrors:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">quay.svc.vopsdev.com/openshift-release-dev/ocp-v4.0-aart-dev</span></span><br><span class="line">  <span class="attr">source:</span> <span class="string">quay.io/openshift-release-dev/ocp-release</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">mirrors:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">quay.svc.vopsdev.com/openshift-release-dev/ocp-v4.0-art-dev</span></span><br><span class="line">  <span class="attr">source:</span> <span class="string">quay.io/openshift-release-dev/ocp-v4.0-art-dev</span></span><br><span class="line"><span class="attr">pullSecret:</span> <span class="string">'&#123;"auths":&#123;...&#125;&#125;'</span></span><br></pre></td></tr></table></figure></p>
<p>一些注释</p>
<ul>
<li>metadata.name + baseDomain 组成集群的基础域名, 这里是 ocp.vopsdev.com</li>
<li>需要在 vSphere Datacenter 下按照 metadata.name 建立一个虚拟机目录, 将来创建 vSphere 动态卷需要, 具体原因看<a href="https://github.com/vmware-archive/kubernetes/issues/499" target="_blank" rel="external nofollow noopener noreferrer">这里</a>. 然而你并不需要将 OCP 节点放到这个虚拟机目录下</li>
<li>platform 下的 vcenter 信息主要用于动态卷的创建, 确保这个用户有足够的权限, 具体看<a href="https://vmware.github.io/vsphere-storage-for-kubernetes/documentation/vcp-roles.html" target="_blank" rel="external nofollow noopener noreferrer">这里</a></li>
<li>我的环境有私有 PKI, 所有的服务器证书由二级 CA 签发, 因此把根 CA 的证书放到 additionalTrustBundle 里</li>
<li>imageContentSources 来自前面<code>oc adm release mirror</code> 的输出结果</li>
<li>我的 quay 私有镜像库不允许匿名拉取, 因此提供了 pullSecret</li>
</ul>
<h3 id="生成-ignition">生成 ignition</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rm -rf assets; mkdir assets</span><br><span class="line">cp install-config.yaml assets/</span><br><span class="line">./openshift-install create manifests --dir=assets</span><br><span class="line">sed -i <span class="string">'s/mastersSchedulable: true/mastersSchedulable: false/'</span> assets/manifests/cluster-scheduler-02-config.yml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置一下时间同步服务. 我这里两个 ntp 服务器是 192.168.11.2 和 192.168.255.249</span></span><br><span class="line">chrony_config=$(<span class="built_in">echo</span> -n <span class="string">"server 192.168.11.2 iburst</span></span><br><span class="line"><span class="string">server 192.168.255.249 iburst</span></span><br><span class="line"><span class="string">driftfile /var/lib/chrony/drift</span></span><br><span class="line"><span class="string">makestep 1.0 3</span></span><br><span class="line"><span class="string">rtcsync</span></span><br><span class="line"><span class="string">logdir /var/log/chrony"</span> | base64 -w0)</span><br><span class="line"></span><br><span class="line">cat &gt; assets/openshift/99_master-chrony.yaml &lt;&lt;EOF</span><br><span class="line">apiVersion: machineconfiguration.openshift.io/v1</span><br><span class="line">kind: MachineConfig</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    machineconfiguration.openshift.io/role: master</span><br><span class="line">  name: 50-vopsdev-master-chrony</span><br><span class="line">spec:</span><br><span class="line">  config:</span><br><span class="line">    ignition:</span><br><span class="line">      version: 2.2.0</span><br><span class="line">    storage:</span><br><span class="line">      files:</span><br><span class="line">      - contents:</span><br><span class="line">          <span class="built_in">source</span>: data:text/plain;charset=utf-8;base64,<span class="variable">$chrony_config</span></span><br><span class="line">        filesystem: root</span><br><span class="line">        mode: 0644</span><br><span class="line">        path: /etc/chrony.conf</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &gt; assets/openshift/99_worker-chrony.yaml &lt;&lt;EOF</span><br><span class="line">apiVersion: machineconfiguration.openshift.io/v1</span><br><span class="line">kind: MachineConfig</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    machineconfiguration.openshift.io/role: worker</span><br><span class="line">  name: 50-vopsdev-worker-chrony</span><br><span class="line">spec:</span><br><span class="line">  config:</span><br><span class="line">    ignition:</span><br><span class="line">      version: 2.2.0</span><br><span class="line">    storage:</span><br><span class="line">      files:</span><br><span class="line">      - contents:</span><br><span class="line">          <span class="built_in">source</span>: data:text/plain;charset=utf-8;base64,<span class="variable">$chrony_config</span></span><br><span class="line">        filesystem: root</span><br><span class="line">        mode: 0644</span><br><span class="line">        path: /etc/chrony.conf</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成 ignition 配置</span></span><br><span class="line">./openshift-install create ignition-configs --dir=assets</span><br><span class="line"></span><br><span class="line"><span class="comment"># 跳板机上运行了 httpd, 提供静态文件服务</span></span><br><span class="line"><span class="comment"># 后面节点可以通过 http://static.svc.vopsdev.com/ocp/bootstrap.ign 下载</span></span><br><span class="line">sudo cp -f assets/bootstrap.ign /var/www/static/ocp/</span><br><span class="line"></span><br><span class="line">cat &gt; ./assets/append-bootstrap.ign &lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"ignition"</span>: &#123;</span><br><span class="line">    <span class="string">"config"</span>: &#123;</span><br><span class="line">      <span class="string">"append"</span>: [</span><br><span class="line">        &#123;</span><br><span class="line">          <span class="string">"source"</span>: <span class="string">"http://static.svc.vopsdev.com/ocp/bootstrap.ign"</span>, </span><br><span class="line">          <span class="string">"verification"</span>: &#123;&#125;</span><br><span class="line">        &#125;</span><br><span class="line">      ]</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"timeouts"</span>: &#123;&#125;,</span><br><span class="line">    <span class="string">"version"</span>: <span class="string">"2.1.0"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"networkd"</span>: &#123;&#125;,</span><br><span class="line">  <span class="string">"passwd"</span>: &#123;&#125;,</span><br><span class="line">  <span class="string">"storage"</span>: &#123;&#125;,</span><br><span class="line">  <span class="string">"systemd"</span>: &#123;&#125;</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># base64 编码的 ignition</span></span><br><span class="line">base64 -w0 assets/master.ign &gt; assets/master.64</span><br><span class="line">base64 -w0 assets/worker.ign &gt; assets/worker.64</span><br><span class="line">base64 -w0 assets/append-bootstrap.ign &gt; assets/append-bootstrap.64</span><br></pre></td></tr></table></figure>
<h2 id="部署节点">部署节点</h2>
<p>对于 vSphere 环境下的部署, ignition 数据需要通过 guestinfo.ignition.config.data 虚拟机属性注入, 手工操作显然不可接受. 我这里采用的方法是 ovftool + govc. 用 ansible 或者 terraform 之类也可以做, 但是在这儿显得杀鸡用牛刀了</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">BOOTSTRAP_B64_CONF=<span class="string">"<span class="variable">$(cat assets/append-bootstrap.64)</span>"</span></span><br><span class="line">MASTER_B64_CONF=<span class="string">"<span class="variable">$(cat assets/master.64)</span>"</span></span><br><span class="line">WORKER_B64_CONF=<span class="string">"<span class="variable">$(cat assets/worker.64)</span>"</span></span><br><span class="line">RHCOS_OVA=<span class="string">"/mnt/driver/RedHat/openshift-4.3/rhcos-4.3.0-x86_64-vmware.ova"</span></span><br><span class="line">USER=<span class="string">'administrator@vsphere.local'</span></span><br><span class="line">PASS=<span class="string">'xxx'</span></span><br><span class="line">VC=<span class="string">"vc.int.vopsdev.com"</span></span><br><span class="line">VMFOLDER=<span class="string">"Lab/OCP"</span></span><br><span class="line">NETWORK=<span class="string">"SG-SVC-11"</span></span><br><span class="line">DATASTORE=<span class="string">"DS-SHARED"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># infoblox api access</span></span><br><span class="line">IBUSER=<span class="string">"xxx"</span></span><br><span class="line">IBPASS=<span class="string">"xxx"</span></span><br><span class="line">IBAPI_BASE=<span class="string">"https://infoblox.int.vopsdev.com/wapi/v2.1"</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> GOVC_URL=<span class="string">"<span class="variable">$USER</span>:<span class="variable">$PASS</span>@<span class="variable">$VC</span>"</span></span><br><span class="line"><span class="built_in">export</span> GOVC_INSECURE=<span class="literal">true</span></span><br><span class="line"><span class="built_in">export</span> GOVC_DATACENTER=HDC</span><br><span class="line"></span><br><span class="line">OCP_MASTERS=()</span><br><span class="line">OCP_WORKERS=()</span><br><span class="line"></span><br><span class="line"><span class="comment"># bootstrap node: 4 vCPU + 16G RAM</span></span><br><span class="line">VM=<span class="string">"ocp-bootstrap"</span></span><br><span class="line">ovftool --vmFolder=<span class="string">"<span class="variable">$VMFOLDER</span>"</span> --datastore=<span class="string">"<span class="variable">$DATASTORE</span>"</span> --diskMode=<span class="string">"thin"</span> --net:<span class="string">"VM Network"</span>=<span class="string">"<span class="variable">$NETWORK</span>"</span> --name=<span class="string">"<span class="variable">$VM</span>"</span>  --numberOfCpus:<span class="string">'*'</span>=4 --memorySize:<span class="string">'*'</span>=16384 --allowExtraConfig --extraConfig:disk.EnableUUID=<span class="literal">true</span> --extraConfig:guestinfo.ignition.config.data.encoding=<span class="string">"base64"</span> --extraConfig:guestinfo.ignition.config.data=<span class="string">"<span class="variable">$BOOTSTRAP_B64_CONF</span>"</span> <span class="variable">$RHCOS_OVA</span> <span class="string">"vi://<span class="variable">$USER</span>:<span class="variable">$PASS</span>@<span class="variable">$VC</span>/HDC/host/compute-cluster"</span></span><br><span class="line">govc vm.disk.change -vm <span class="variable">$VM</span> -disk.label <span class="string">"Hard disk 1"</span> -size 120G</span><br><span class="line"></span><br><span class="line"><span class="comment"># master nodes: 4 vCPU + 16G RAM</span></span><br><span class="line"><span class="keyword">for</span> idx <span class="keyword">in</span> 0 1 2; <span class="keyword">do</span></span><br><span class="line">  VM=<span class="string">"ocp-node-<span class="variable">$idx</span>"</span></span><br><span class="line">  ovftool --vmFolder=<span class="string">"<span class="variable">$VMFOLDER</span>"</span> --datastore=<span class="string">"<span class="variable">$DATASTORE</span>"</span> --diskMode=<span class="string">"thin"</span> --net:<span class="string">"VM Network"</span>=<span class="string">"<span class="variable">$NETWORK</span>"</span> --name=<span class="string">"<span class="variable">$VM</span>"</span>  --numberOfCpus:<span class="string">'*'</span>=4 --memorySize:<span class="string">'*'</span>=16384 --allowExtraConfig --extraConfig:disk.EnableUUID=<span class="literal">true</span> --prop:guestinfo.ignition.config.data.encoding=<span class="string">"base64"</span> --prop:guestinfo.ignition.config.data=<span class="string">"<span class="variable">$MASTER_B64_CONF</span>"</span> <span class="variable">$RHCOS_OVA</span> <span class="string">"vi://<span class="variable">$USER</span>:<span class="variable">$PASS</span>@<span class="variable">$VC</span>/HDC/host/compute-cluster"</span></span><br><span class="line">  govc vm.disk.change -vm <span class="variable">$VM</span> -disk.label <span class="string">"Hard disk 1"</span> -size 120G</span><br><span class="line">  OCP_MASTERS=(<span class="string">"<span class="variable">$&#123;OCP_MASTERS[@]&#125;</span>"</span> <span class="variable">$VM</span>)</span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># worker nodes: 4 vCPU + 32G RAM</span></span><br><span class="line"><span class="keyword">for</span> idx <span class="keyword">in</span> 3 4 5; <span class="keyword">do</span></span><br><span class="line">  VM=<span class="string">"ocp-node-<span class="variable">$idx</span>"</span></span><br><span class="line">  ovftool --vmFolder=<span class="string">"<span class="variable">$VMFOLDER</span>"</span> --datastore=<span class="string">"<span class="variable">$DATASTORE</span>"</span> --diskMode=<span class="string">"thin"</span> --net:<span class="string">"VM Network"</span>=<span class="string">"<span class="variable">$NETWORK</span>"</span> --name=<span class="string">"<span class="variable">$VM</span>"</span>  --numberOfCpus:<span class="string">'*'</span>=4 --memorySize:<span class="string">'*'</span>=32768 --allowExtraConfig --extraConfig:disk.EnableUUID=<span class="literal">true</span> --prop:guestinfo.ignition.config.data.encoding=<span class="string">"base64"</span> --prop:guestinfo.ignition.config.data=<span class="string">"<span class="variable">$WORKER_B64_CONF</span>"</span> <span class="variable">$RHCOS_OVA</span> <span class="string">"vi://<span class="variable">$USER</span>:<span class="variable">$PASS</span>@<span class="variable">$VC</span>/HDC/host/compute-cluster"</span></span><br><span class="line">  govc vm.disk.change -vm <span class="variable">$VM</span> -disk.label <span class="string">"Hard disk 1"</span> -size 120G</span><br><span class="line">  OCP_WORKERS=(<span class="string">"<span class="variable">$&#123;OCP_WORKERS[@]&#125;</span>"</span> <span class="variable">$VM</span>)</span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据新的 mac 地址, 更新 dhcp fixed address 记录</span></span><br><span class="line">ALL_NODES=(ocp-bootstrap <span class="string">"<span class="variable">$&#123;OCP_MASTERS[@]&#125;</span>"</span> <span class="string">"<span class="variable">$&#123;OCP_WORKERS[@]&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">for</span> node <span class="keyword">in</span> <span class="string">"<span class="variable">$&#123;ALL_NODES[@]&#125;</span>"</span>; <span class="keyword">do</span></span><br><span class="line">  mac=$(govc device.info -vm <span class="variable">$node</span> -json ethernet-0 | jq -r .Devices[].MacAddress)</span><br><span class="line">  ip=$(host <span class="variable">$node</span> | cut -d<span class="string">' '</span> -f4)</span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"<span class="variable">$node</span>: <span class="variable">$ip</span>, <span class="variable">$mac</span>"</span></span><br><span class="line">  api_path=$(curl -s -k -u <span class="variable">$IBUSER</span>:<span class="variable">$IBPASS</span> -H <span class="string">"Content-Type: application/json"</span> -X GET <span class="string">"<span class="variable">$IBAPI_BASE</span>/fixedaddress?ipv4addr=<span class="variable">$ip</span>"</span> | jq -r <span class="string">'.[] | ._ref'</span>)  </span><br><span class="line">  curl -s -k -u <span class="variable">$IBUSER</span>:<span class="variable">$IBPASS</span> -H <span class="string">"Content-Type: application/json"</span> -X PUT  <span class="string">"<span class="variable">$IBAPI_BASE</span>/<span class="variable">$&#123;api_path&#125;</span>"</span> -d <span class="string">"&#123;\"mac\":\"<span class="variable">$mac</span>\"&#125;"</span> &gt; /dev/null</span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 必要时 reload infoblox DHCP 服务</span></span><br><span class="line">grid_path=$(curl -s -k -u <span class="variable">$IBUSER</span>:<span class="variable">$IBPASS</span> -H <span class="string">"Content-Type: application/json"</span> <span class="string">"<span class="variable">$IBAPI_BASE</span>/grid"</span> | jq -r <span class="string">'.[] | ._ref'</span>)</span><br><span class="line">curl -s -k -u <span class="variable">$IBUSER</span>:<span class="variable">$IBPASS</span> -H <span class="string">"Content-Type: application/json"</span> -X POST <span class="string">"<span class="variable">$IBAPI_BASE</span>/<span class="variable">$&#123;grid_path&#125;</span>?_function=restartservices"</span> -d <span class="string">'&#123;"member_order":"SIMULTANEOUSLY","restart_option":"RESTART_IF_NEEDED","service_option":"DHCP"&#125;'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动 bootstrap</span></span><br><span class="line">govc vm.power -on <span class="string">"ocp-bootstrap"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 观察负载均衡器, 下面两个没问题了再进行下去</span></span><br><span class="line"><span class="comment"># curl -v -k https://api-int.ocp.vopsdev.com:22623/config/master</span></span><br><span class="line"><span class="comment"># curl -v -k https://api-int.ocp.vopsdev.com:22623/config/worker </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动所有 master 节点. 节点会去 bootstrap 上下载配置</span></span><br><span class="line"><span class="keyword">for</span> node <span class="keyword">in</span> <span class="string">"<span class="variable">$&#123;OCP_MASTERS[@]&#125;</span>"</span>; <span class="keyword">do</span></span><br><span class="line">  govc vm.power -on <span class="variable">$node</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 观察负载均衡器, 3 个 master 都起来了再进行下去</span></span><br><span class="line"><span class="comment"># curl -v -k https://api-int.ocp.vopsdev.com:22623/config/master</span></span><br><span class="line"><span class="comment"># curl -v -k https://api-int.ocp.vopsdev.com:22623/config/worker </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动所有 worker 节点. 节点会去 master 上下载配置</span></span><br><span class="line"><span class="keyword">for</span> node <span class="keyword">in</span> <span class="string">"<span class="variable">$&#123;OCP_WORKERS[@]&#125;</span>"</span>; <span class="keyword">do</span></span><br><span class="line">  govc vm.power -on <span class="variable">$node</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 到这儿这步一般已经结束了</span></span><br><span class="line">./openshift-install --dir=assets <span class="built_in">wait</span>-for bootstrap-complete --<span class="built_in">log</span>-level=debug</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> KUBECONFIG=<span class="string">'./assets/auth/kubeconfig'</span></span><br><span class="line">oc whoami</span><br><span class="line">oc version</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果有 pending 的证书请求就批准</span></span><br><span class="line">oc get csr</span><br><span class="line">oc get csr -o json | jq -r <span class="string">'.items[] | select(.status == &#123;&#125; ) | .metadata.name'</span> | xargs oc adm certificate approve</span><br><span class="line"></span><br><span class="line"><span class="comment"># 等待 worker 节点起来, 确保所有节点都是 Ready</span></span><br><span class="line">oc get nodes</span><br><span class="line"></span><br><span class="line"><span class="comment"># 等所有 cluster operator 状态 AVAILABLE: TRUE</span></span><br><span class="line">oc get clusteroperators</span><br><span class="line"></span><br><span class="line"><span class="comment"># 等待安装结束, 获取集群管理员密码</span></span><br><span class="line">./openshift-install --dir=assets <span class="built_in">wait</span>-for install-complete</span><br></pre></td></tr></table></figure>
<p>使用集群管理员 kubeadmin 登录 web console <img src="ocp-console-initial.png"></p>
<p>至此集群初步搭建完成, 后面将针对各个基础服务进行完善.</p>
]]></content>
      <categories>
        <category>RedHat</category>
        <category>Openshift</category>
      </categories>
      <tags>
        <tag>RedHat</tag>
        <tag>Openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>实施 AWS Landing Zone 4: 定制 Landing Zone CodePipeline</title>
    <url>/2020-03-16/%E5%AE%9E%E6%96%BD-AWS-Landing-Zone-4-%E5%AE%9A%E5%88%B6-Landing-Zone-CodePipeline/</url>
    <content><![CDATA[<p>Landing Zone CodePipeline 默认使用一个启用了版本的 S3 bucket 作为代码来源: 当有新的配置文件 aws-landing-zone-configuration.zip 上传时, 触发 CodePipeline. 然而使用 versioned bucket 作为版本控制显然算不上最佳实践. 这里将修改 Landing Zone Codepipeline 使用 CodeCommit repo 作为代码来源. 这样就可以通过 git 来管理组织下所有 AWS 账号的配置基线.</p>
<a id="more"></a>
<p>进一步, 企业一般都有自己的代码管理平台 (bitbucket/gitlab/github), 并不见得愿意单单为了管理 Landing Zone 的配置而引入额外的 CodeCommit 代码仓库的维护成本. 实际上可以利用现有代码平台来保存 Landing Zone 配置, 其基本原理是做仓库镜像: 利用 CI 工具将特定分支的更新自动推送到 CodeCommit, 进而触发 CodePipeline.</p>
<h2 id="配置-codecommit">配置 CodeCommit</h2>
<h3 id="创建-codecommit-代码仓库">创建 CodeCommit 代码仓库</h3>
<p>到 Developer Tools, CodeCommit 下创建一个名为 landing-zone-configuration 的代码仓库 <img src="create-lz-codecommit-repo.png"></p>
<h3 id="创建-iam-policy-和服务账号">创建 IAM Policy 和服务账号</h3>
<p>到 IAM 下创建自定义 Policy: LandingZoneCodeCommitRepoAccess <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;Version&quot;: &quot;2012-10-17&quot;,</span><br><span class="line">    &quot;Statement&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;Effect&quot;: &quot;Allow&quot;,</span><br><span class="line">            &quot;Action&quot;: [</span><br><span class="line">                &quot;codecommit:BatchGet*&quot;,</span><br><span class="line">                &quot;codecommit:Create*&quot;,</span><br><span class="line">                &quot;codecommit:DeleteBranch&quot;,</span><br><span class="line">                &quot;codecommit:Get*&quot;,</span><br><span class="line">                &quot;codecommit:List*&quot;,</span><br><span class="line">                &quot;codecommit:Describe*&quot;,</span><br><span class="line">                &quot;codecommit:Put*&quot;,</span><br><span class="line">                &quot;codecommit:Post*&quot;,</span><br><span class="line">                &quot;codecommit:Merge*&quot;,</span><br><span class="line">                &quot;codecommit:Test*&quot;,</span><br><span class="line">                &quot;codecommit:Update*&quot;,</span><br><span class="line">                &quot;codecommit:GitPull&quot;,</span><br><span class="line">                &quot;codecommit:GitPush&quot;</span><br><span class="line">            ],</span><br><span class="line">            &quot;Resource&quot;: [</span><br><span class="line">                &quot;arn:aws:codecommit:&lt;region&gt;:&lt;accountid&gt;:landing-zone-configuration&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure> 接着创建一个 IAM user 作为服务账号: landingzone-repo-mirror, 附加 LandingZoneCodeCommitRepoAccess 策略. 配置这个用户的 Security credentials: HTTPS Git credentials for AWS CodeCommit <img src="https-codecommit-credentials.png"></p>
<p>我们并不会直接使用这个账户来修改 Landing Zone 配置. 这个账号仅仅用于同步私有代码仓库到 CodeCommit.</p>
<h2 id="使用-codecommit-作为-landing-zone-pipeline-构建源">使用 CodeCommit 作为 Landing Zone Pipeline 构建源</h2>
<p>进入 CodePipeline, 编辑 AWS-Landing-Zone-CodePipeline. Edit Source, Edit stage <img src="codepipeline-edit-source.png"></p>
<p>编辑原有的 Source</p>
<ul>
<li>Action provider: AWS CodeCommit</li>
<li>Repository name: landing-zone-configuration</li>
<li>Branch name: master</li>
<li>Change detection options: Amazon Cloudwatch Events</li>
<li>Output artifacts: SourceApp</li>
</ul>
<p><img src="pipeline-codecommit-source.png"></p>
<p>保存退出. 此时会提示将自动添加 AWS Cloudwatch Events rule. 这里我们需要 Events rule 来跟踪 CodeCommit 的变化进而触发 CodePipeline, 因此保持 "No resource updates needed for this source action change" 为默认的不勾选状态, 让 AWS 自动创建 Cloudwatch Events rule.</p>
<p><img src="pipeline-event-rule.png"></p>
<p>如果感兴趣可以到 Cloudwatch Events 下查看自动产生的规则.</p>
<h2 id="使用私有代码仓库保存-landing-zone-配置">使用私有代码仓库保存 Landing Zone 配置</h2>
<p>下面是使用 gitlab CI 的例子: Landing Zone 配置保存在企业内部的 gitlab 代码仓库中, 当 master 分支发生变更时触发 gitlab CI, 将代码库镜像到 AWS CodeCommit, 进而触发 Landing Zone CodePipeline.</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">image:</span> <span class="string">rtcamp/gitlab-aws-codecommit-mirror</span></span><br><span class="line"><span class="attr">stages:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">deploy</span></span><br><span class="line"></span><br><span class="line"><span class="attr">deploy to production:</span></span><br><span class="line">  <span class="attr">tags:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">aws</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">codecommit</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">landingzone</span></span><br><span class="line">  <span class="attr">stage:</span> <span class="string">deploy</span></span><br><span class="line">  <span class="attr">only:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">master</span></span><br><span class="line">  <span class="attr">script:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">git</span> <span class="string">clone</span> <span class="string">--mirror</span> <span class="string">https://$TOKEN_NAME:$TOKEN_VALUE@$SOURCE_REPO</span> <span class="string">rtSync</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">cd</span> <span class="string">rtSync</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">git</span> <span class="string">branch</span> <span class="string">-a</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">git</span> <span class="string">push</span> <span class="string">--mirror</span> <span class="string">https://$USERNAME:$SECRET@$DEST_REPO</span></span><br></pre></td></tr></table></figure>
<p>这里用到的几个 CI 变量:</p>
<ul>
<li>USERNAME: 就是前面配置的 IAM 服务账号 landing-zone-repo-mirror 下 HTTPS Git credentials for AWS CodeCommit 的用户名</li>
<li>DEST_REPO: AWS CodeCommit 代码库位置, 例如 git-codecommit.us-west-2.amazonaws.com/v1/repos/landing-zone-configuration</li>
<li>SECRET: landing-zone-repo-mirror 下 HTTPS Git credentials for AWS CodeCommit 的密码. 注意特殊字符转义</li>
<li>SOURCE_REPO: 内部代码库位置, 例如 gitlab.svc.vopsdev.com/devinfra/landing-zone-configuration.git</li>
<li>TOKEN_NAME, TOKEN_VALUE: 这里使用 deploy token 来获取私有代码库</li>
</ul>
]]></content>
      <categories>
        <category>AWS</category>
        <category>Landing Zone</category>
      </categories>
      <tags>
        <tag>AWS</tag>
        <tag>Landing Zone</tag>
      </tags>
  </entry>
  <entry>
    <title>离线部署 Openshift Container Platform 4.3 - 1: 准备离线资源</title>
    <url>/2020-03-15/%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B2-Openshift-Container-Platform-4-3-1-%E5%87%86%E5%A4%87%E7%A6%BB%E7%BA%BF%E8%B5%84%E6%BA%90/</url>
    <content><![CDATA[<p>本系列文章记录了 Openshift Container Platform (OCP) 4.3.5 离线部署的过程. 离线资源包括安装镜像, 所有样例 Image Stream, OperatorHub 下的 RedHat Operators. 虽然这是一个实验系列, 我会尽可能的按照生产实践来进行. <a id="more"></a></p>
<h2 id="实验环境">实验环境</h2>
<h3 id="跳板机">跳板机</h3>
<p>可以同时访问内外网, 同时具备科学上网的能力, 用来执行安装任务和离线材料准备</p>
<h3 id="私有镜像库">私有镜像库</h3>
<p>离线镜像会保存到私有镜像库中, 以供 OCP 安装和运行时使用, <strong>要求支持 version 2 schema 2 (manifest list)</strong>. 我这里选择的是 <strong>Quay 3</strong></p>
<ul>
<li>Nexus 目前还不支持 manifest list: <a href="https://issues.sonatype.org/browse/NEXUS-18546" target="_blank" rel="external nofollow noopener noreferrer">NEXUS-18546</a></li>
<li>Harbor 目前还不支持 manifest list: <a href="https://github.com/goharbor/harbor/issues/6522" target="_blank" rel="external nofollow noopener noreferrer">6522</a></li>
</ul>
<h2 id="关于镜像的获取">关于镜像的获取</h2>
<p>发现很多人误以为必须联系红帽销售, 签单之后才可以试用 OCP4, 实际上并不是这样. 注册一个<a href="https://developers.redhat.com" target="_blank" rel="external nofollow noopener noreferrer">开发者账号</a> 后就可以获得 quay.io, registry.redhat.io 的 pull secret 来进行测试实验了.</p>
<h2 id="准备离线安装介质">准备离线安装介质</h2>
<h3 id="获取目前的版本信息">获取目前的版本信息</h3>
<p>目前最新的 OCP 版本是 4.3.5. 从这里下载 oc 客户端</p>
<ul>
<li><a href="https://mirror.openshift.com/pub/openshift-v4/clients/ocp/4.3.5/openshift-client-linux-4.3.5.tar.gz" target="_blank" rel="external nofollow noopener noreferrer">https://mirror.openshift.com/pub/openshift-v4/clients/ocp/4.3.5/openshift-client-linux-4.3.5.tar.gz</a></li>
</ul>
<p>解压出来的二进制文件放到跳板机 PATH 下. 先看一下当前 4.3.5 的版本信息</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">oc adm release info quay.io/openshift-release-dev/ocp-release:4.3.5-x86_64</span><br><span class="line"></span><br><span class="line">Name:      4.3.5</span><br><span class="line">Digest:    sha256:64320fbf95d968fc6b9863581a92d373bc75f563a13ae1c727af37450579f61a</span><br><span class="line">Created:   2020-03-06T12:05:47Z</span><br><span class="line">OS/Arch:   linux/amd64</span><br><span class="line">Manifests: 366</span><br><span class="line"></span><br><span class="line">Pull From: quay.io/openshift-release-dev/ocp-release@sha256:64320fbf95d968fc6b9863581a92d373bc75f563a13ae1c727af37450579f61a</span><br><span class="line"></span><br><span class="line">Release Metadata:</span><br><span class="line">  Version:  4.3.5</span><br><span class="line">  Upgrades: 4.2.21, 4.2.22, 4.3.0, 4.3.1, 4.3.2, 4.3.3</span><br><span class="line">  Metadata:</span><br><span class="line">    description:</span><br><span class="line">  Metadata:</span><br><span class="line">    url: https://access.redhat.com/errata/RHBA-2020:0676</span><br><span class="line"></span><br><span class="line">Component Versions:</span><br><span class="line">  Kubernetes 1.16.2</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<h3 id="下载安装镜像">下载安装镜像</h3>
<p>实验环境中的 quay 在内网 https://quay.svc.vopsdev.com 提供服务. 事先创建好 namespace/organization: openshift-release-dev 用来存放安装镜像仓库</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> OCP_RELEASE=4.3.5</span><br><span class="line"><span class="built_in">export</span> ARCH=x86_64</span><br><span class="line"><span class="built_in">export</span> LOCAL_REGISTRY=<span class="string">'quay.svc.vopsdev.com'</span> </span><br><span class="line"><span class="built_in">export</span> LOCAL_REPOSITORY=<span class="string">'openshift-release-dev/ocp-v4.0-art-dev'</span></span><br><span class="line"><span class="built_in">export</span> UPSTREAM_REPO=<span class="string">'openshift-release-dev'</span></span><br><span class="line"><span class="built_in">export</span> RELEASE_NAME=<span class="string">"ocp-release"</span></span><br><span class="line"><span class="comment"># 将你从红帽获取的 pull secret 以及你的私有镜像库的 secret 加入到 pull-secret.json 中</span></span><br><span class="line"><span class="built_in">export</span> LOCAL_SECRET_JSON=<span class="string">"pull-secret.json"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果你需要科学上网</span></span><br><span class="line"><span class="built_in">export</span> HTTP_PROXY=...</span><br><span class="line"><span class="built_in">export</span> HTTPS_PROXY=...</span><br><span class="line"><span class="built_in">export</span> NO_PROXY=<span class="string">"quay.svc.vopsdev.com"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 这样 release 镜像以及安装需要的镜像会都同步到 quay.svc.vopsdev.com/openshift-release-dev/ocp-v4.0-art-dev 下了</span></span><br><span class="line">oc adm release mirror -a <span class="variable">$&#123;LOCAL_SECRET_JSON&#125;</span> --from=quay.io/<span class="variable">$&#123;UPSTREAM_REPO&#125;</span>/<span class="variable">$&#123;RELEASE_NAME&#125;</span>:<span class="variable">$&#123;OCP_RELEASE&#125;</span>-<span class="variable">$&#123;ARCH&#125;</span> --to-release-image=<span class="variable">$&#123;LOCAL_REGISTRY&#125;</span>/<span class="variable">$&#123;LOCAL_REPOSITORY&#125;</span>:<span class="variable">$&#123;OCP_RELEASE&#125;</span> --to=<span class="variable">$&#123;LOCAL_REGISTRY&#125;</span>/<span class="variable">$&#123;LOCAL_REPOSITORY&#125;</span></span><br></pre></td></tr></table></figure>
<p><code>oc adm release mirror</code> 命令完成后会输出下面类似的信息, 保存下来, 将来会用在 install-config.yaml 文件中</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">imageContentSources:</span><br><span class="line">- mirrors:</span><br><span class="line">  - quay.svc.vopsdev.com&#x2F;openshift-release-dev&#x2F;ocp-v4.0-art-dev</span><br><span class="line">  source: quay.io&#x2F;openshift-release-dev&#x2F;ocp-release</span><br><span class="line">- mirrors:</span><br><span class="line">  - quay.svc.vopsdev.com&#x2F;openshift-release-dev&#x2F;ocp-v4.0-art-dev</span><br><span class="line">  source: quay.io&#x2F;openshift-release-dev&#x2F;ocp-v4.0-art-dev</span><br></pre></td></tr></table></figure>
<h3 id="提取安装程序">提取安装程序</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">oc adm release extract -a <span class="variable">$&#123;LOCAL_SECRET_JSON&#125;</span> --<span class="built_in">command</span>=openshift-install <span class="string">"<span class="variable">$&#123;LOCAL_REGISTRY&#125;</span>/<span class="variable">$&#123;LOCAL_REPOSITORY&#125;</span>:<span class="variable">$&#123;OCP_RELEASE&#125;</span>"</span></span><br></pre></td></tr></table></figure>
<p>会产生 openshift-install 二进制程序 (不要直接从 <a href="https://mirror.openshift.com/pub/openshift-v4/clients/ocp/4.3.5" target="_blank" rel="external nofollow noopener noreferrer">https://mirror.openshift.com/pub/openshift-v4/clients/ocp/4.3.5</a> 下载, 后面会有 sha256 匹配不上的问题)</p>
<h2 id="准备-image-stream-样例镜像">准备 Image Stream 样例镜像</h2>
<p>准备一个镜像列表, 然后使用 <code>oc image mirror</code>将镜像同步到私有仓库中</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cat sample-images.txt | <span class="keyword">while</span> <span class="built_in">read</span> line; <span class="keyword">do</span></span><br><span class="line">  target=$(<span class="built_in">echo</span> <span class="variable">$line</span> | sed <span class="string">'s/registry.redhat.io/quay.svc.vopsdev.com/'</span>)</span><br><span class="line">  oc image mirror -a <span class="variable">$&#123;LOCAL_SECRET_JSON&#125;</span> <span class="variable">$line</span> <span class="variable">$target</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure>
<p>完整的镜像列表如何获取? 如果之前装过 OCP 4.3.5, 把 openshift-cluster-samples-operator 项目下 cluster-samples-operator pod 的 /opt/openshift 目录同步出来, 简单 grep 一下就都有了.</p>
<p>完整列表参考<a href="https://gist.github.com/yuanlinios/7eea8207083e649cbe07e108a22df00b" target="_blank" rel="external nofollow noopener noreferrer">这里</a></p>
<h2 id="准备-operatorhub-离线资源">准备 OperatorHub 离线资源</h2>
<p>首先构建 RedHat Operators 的 catalog image, 保存为 quay.svc.vopsdev.com/devinfra/redhat-operators:v1.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">oc adm catalog build --appregistry-endpoint https://quay.io/cnr --appregistry-org redhat-operators --to=quay.svc.vopsdev.com/devinfra/redhat-operators:v1</span><br></pre></td></tr></table></figure>
<p>这个 catalog image 相当于 RedHat Operators 的一个目录, 通过 catalog image 可以找到 RedHat Operators 的所有镜像. 而且 catalog image 使用 sha256 digest 来引用镜像, 能够确保应用有稳定可重复的部署.</p>
<p>然后使用 catalog image 同步 RedHat Operators 的所有镜像到私有仓库. 按照官方文档的做法是</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">oc adm catalog mirror quay.svc.vopsdev.com&#x2F;devinfra&#x2F;redhat-operators:v1 quay.svc.vopsdev.com</span><br></pre></td></tr></table></figure>
<p>这个命令结束后会产生 redhat-operators-manifests 目录, 下面有两个文件: mapping.txt 和 imageContentSourcePolicy.yaml.</p>
<p>然而这么做目前还有问题 <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1800674" target="_blank" rel="external nofollow noopener noreferrer">1800674</a>: 同步出来的镜像 manifest digest 不对, 导致后面离线安装 operator 时会报镜像无法获取的错误. 暂时可以使用上面 bugzilla 链接里给出的临时解决方案: <code>skopeo copy --all</code></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cat redhat-operators-manifests/mapping.txt | <span class="keyword">while</span> <span class="built_in">read</span> line; <span class="keyword">do</span></span><br><span class="line">  origin=$(<span class="built_in">echo</span> <span class="variable">$line</span> | cut -d= -f1)</span><br><span class="line">  target=$(<span class="built_in">echo</span> <span class="variable">$line</span> | cut -d= -f2)</span><br><span class="line">  <span class="keyword">if</span> [[ <span class="string">"<span class="variable">$origin</span>"</span> =~ <span class="string">"sha256"</span> ]]; <span class="keyword">then</span></span><br><span class="line">    tag=$(<span class="built_in">echo</span> <span class="variable">$origin</span> | cut -d: -f2 | cut -c -8)</span><br><span class="line">    skopeo copy --all docker://<span class="variable">$origin</span> docker://<span class="variable">$target</span>:<span class="variable">$tag</span></span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">    skopeo copy --all docker://<span class="variable">$origin</span> docker://<span class="variable">$target</span></span><br><span class="line">  <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>RedHat</category>
        <category>Openshift</category>
      </categories>
      <tags>
        <tag>RedHat</tag>
        <tag>Openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>实施 AWS Landing Zone 3: 部署 Initiation Stack</title>
    <url>/2020-03-12/%E5%AE%9E%E6%96%BD-AWS-Landing-Zone-3-%E9%83%A8%E7%BD%B2-Initiation-Stack/</url>
    <content><![CDATA[<p>本文将介绍 Landing Zone Initiation Stack 的部署, 参数解析以及一些容易出错的地方</p>
<a id="more"></a>
<h2 id="创建-initiation-stack">创建 Initiation Stack</h2>
<p>登入组织账号 (即这里的 yl-master), 进入 Cloudformation 服务. 创建一个新的 stack: <img src="create-initiation-stack.png"></p>
<p>template s3 url 使用 https://s3.amazonaws.com/solutions-reference/aws-landing-zone/v2.3.1/aws-landing-zone-initiation.template</p>
<p>Landing Zone Initiation stack 是一个比较庞大的 cloudformation template, 大约近 6000 行配置, 其核心部分是 5 个 Lambda 和 7 个 Step Functions. 但这还不是 Landing Zone 的全貌, 因为它引用了很多外部资源, 例如保存在 S3 里的 Lambda 函数实现. 在 Initiation stack template 中大部分内容 (约 2/3) 是 7 个 StateMachines (Step Functions) 的定义.</p>
<div class="note info">
            <p>AWS 会对 Landing Zone 进行更新升级, 最新版本的信息可以从 https://aws.amazon.com/solutions/aws-landing-zone/ 获取</p>
          </div>
<h3 id="参数详解">参数详解</h3>
<h4 id="landing-zone-core-account-configuration">Landing Zone Core Account Configuration</h4>
<p>核心账号配置部分</p>
<ul>
<li>Shared Service Account Email Address: 填写现存共享服务账号的邮件地址. 如果你的现存环境中还没有共享服务账号也没关系, 如果有需要, Landing Zone 可以帮你创建</li>
<li>Log Archive Account Email Address: 填写现存的日志账号邮件地址. 同上</li>
<li>Security Account Email Address: 填写现存的安全审计账号的邮件地址. 同上</li>
<li>Nest OU Name Delimiter: Colon (:) 使用冒号作为 OU 路径的界定符. 保持默认即可</li>
<li>Core OU Name: vopsdev-landing-zone:core 根据上一篇文章中的 OU 规划来填写, 这里是核心账号所在的 OU</li>
<li>Non Core OU Names: vopsdev-landing-zone:production,vopsdev-landing-zone:department,vopsdev-landing-zone:staging,vopsdev-landing-zone:sandbox 根据上一篇文章中的 OU 规划来写, 这里是非核心账号所在的 OU. 使用逗号作为分隔符</li>
<li>Security Alert Email Address: 接受安全告警邮件的地址. 具体是来自 GuardDuty Finding, Cloudwatch Alarm, Config Rules Compliance Status Change 的消息</li>
<li>Lock StackSetsExecution Role: Yes. 是否锁定角色 AWSCloudFormationStackSetExecutionRole. Landing Zone 通过 Stackset 来部署资源基线到各个受控账号, 因此会在各个账号下创建角色 AWSCloudFormationStackSetExecutionRole 来执行 stack instance. 该角色具有账户的管理员权限. 锁定该角色的含义是只允许特定的实体 (principal) 来承担 (assume role).</li>
<li>Subscribe All Change Events Email To Topic: No. 是否订阅所有的配置变更事件. 按需设置</li>
<li>All Change Events Email: 接受配置变更事件通知的邮件地址</li>
</ul>
<h4 id="landing-zone-pipeline-configuration">Landing Zone Pipeline Configuration</h4>
<p>流水线配置部分</p>
<ul>
<li>Pipeline Approval Stage: Yes. 给 Landing Zone 配置流水线添加手动批准的步骤</li>
<li>Pipeline Approval Email Address: 批准 Landing Zone 配置流水线的邮件地址</li>
<li>Auto Build Landing Zone: No. 是否在 initiation stack 创建完成后立刻启动配置流水线. 我们需要对配置进行定制, 因此这里一定需要设置为 No</li>
</ul>
<h4 id="shared-services-vpc-configuration">Shared Services VPC Configuration</h4>
<p>共享服务的 VPC 配置部分</p>
<ul>
<li>Shared Services VPC Options: Shared-Services-Network-3-AZs. 选择共享服务账号的 VPC 类型. 仅仅用来生成初始配置. 并不会在 Initiation 阶段创建实际的资源. 后面可以按自己的需要修改初始配置</li>
<li>Shared Services VPC CIDR: 100.65.0.0/16 共享服务 VPC 的网段. 同上</li>
</ul>
<h4 id="vpc-flow-logs-retention-policy">VPC Flow Logs Retention Policy</h4>
<p>VPC Flow Logs 的留存策略</p>
<ul>
<li>VPC Flow Logs Retention In Days: 90</li>
</ul>
<h4 id="aws-security-and-configuration-services">AWS Security and Configuration Services</h4>
<p>安全和配置管理部分</p>
<ul>
<li>Enable AWS Security and Configuraiton Monitoring in: All regions 在哪些区域启用安全和配置管理服务 (GuardDuty, Config 之类). 同样也仅仅用于生成初始化配置. 后期可以按需要修改</li>
</ul>
<h4 id="aws-config-rules">AWS Config Rules</h4>
<p>启用哪些 AWS Config Rules. 按默认都启用即可</p>
<h4 id="add-on-publisher-configuraiton">Add-On Publisher Configuraiton</h4>
<p>Add-On 产品更新配置部分</p>
<ul>
<li>AWS Manages Service Catalog Add-On Portfolio? Manual Updates 手动管理 Add-On Portfolio 的版本更新</li>
<li>Add-On Update Notification Email: Add-On 更新通知的邮件地址. 按需设置</li>
</ul>
<h2 id="关于通知邮件地址">关于通知邮件地址</h2>
<h3 id="结论">结论</h3>
<p>不同的 AWS 账号需要不同的邮件地址, 这基本不会出错. 但是在部署 Landing Zone Initiation stack 时需要注意 Security Account Email Address 的邮件地址不能用来接收 Security Alert. 即 Security Alert Email Address 和 Security Account Email Address 不能是同一个.</p>
<h3 id="原因">原因</h3>
<p>Initiaton stack 中有这样的资源定义</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">LandingZoneConfigDeployer:</span></span><br><span class="line">  <span class="attr">Type:</span> <span class="string">Custom::ConfigDeployer</span></span><br><span class="line">  <span class="attr">Properties:</span></span><br><span class="line">    <span class="attr">metrics_flag:</span> <span class="type">!FindInMap</span> <span class="string">[Solution,</span> <span class="string">Metrics,</span> <span class="string">SendAnonymousData]</span></span><br><span class="line">    <span class="attr">email_list:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="type">!Ref</span> <span class="string">SharedServicesAccountEmail</span></span><br><span class="line">      <span class="bullet">-</span> <span class="type">!Ref</span> <span class="string">LoggingAccountEmail</span></span><br><span class="line">      <span class="bullet">-</span> <span class="type">!Ref</span> <span class="string">SecurityAccountEmail</span></span><br><span class="line">      <span class="bullet">-</span> <span class="type">!Ref</span> <span class="string">SecurityAlertEmail</span></span><br></pre></td></tr></table></figure>
<p>而 Lambda 函数 LandingZoneDeploymentLambda 的 config_deployer.py 中会检查 email_list 中是否有重复地址 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">unique_email_validator</span><span class="params">(email_list)</span>:</span></span><br><span class="line">    result = set([x <span class="keyword">for</span> x <span class="keyword">in</span> email_list <span class="keyword">if</span> email_list.count(x) &gt; <span class="number">1</span>])</span><br><span class="line">    duplicate_list = list(result)</span><br><span class="line">    logger.info(<span class="string">"Duplicate Emails: &#123;&#125;"</span>.format(duplicate_list))</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> duplicate_list:</span><br><span class="line">        logger.info(<span class="string">"Duplicate emails not found"</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> Exception(<span class="string">"Found duplicate email(s) &#123;&#125; in the parameters."</span>.format(duplicate_list))</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">config_deployer</span><span class="params">(event)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        s3 = S3(logger)</span><br><span class="line">    ...</span><br><span class="line">    <span class="comment"># Check if the emails are Unique</span></span><br><span class="line">    unique_email_validator(event.get(<span class="string">'email_list'</span>))</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>AWS</category>
        <category>Landing Zone</category>
      </categories>
      <tags>
        <tag>AWS</tag>
        <tag>Landing Zone</tag>
      </tags>
  </entry>
  <entry>
    <title>实施 AWS Landing Zone 2: 实施前场景与目标规划</title>
    <url>/2020-03-12/%E5%AE%9E%E6%96%BD-AWS-Landing-Zone-2-%E5%AE%9E%E6%96%BD%E5%89%8D%E5%9C%BA%E6%99%AF%E4%B8%8E%E7%9B%AE%E6%A0%87%E8%A7%84%E5%88%92/</url>
    <content><![CDATA[<p>该系列文章将通过一个虚拟场景介绍如何在现存的多账户环境下实施 Landing Zone (brownfield deployment). 本文介绍实施前企业多账户环境的现状, 迁移的目标规划和准备工作</p>
<a id="more"></a>
<h2 id="实施前状况">实施前状况</h2>
<p>VOPSDEV.COM 目前在 AWS Organization 下按照部门职能和项目环境类型划分了多个组织单元 (Organization Unit, OU) 并采用了多个账号以实现职责/资源分离:</p>
<p><img src="original-organization-structure.png"></p>
<p>业务的生产环境和测试环境账号分别在 production OU 和 staging OU 下, 开发人员的沙盒环境账号创建在 sandbox OU 下, 各个职能部门的账号创建在 department OU 下. Organization master, 安全, 审计日志, 公共服务相关账号在 core OU 下. 具体分布如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Root&#x2F;</span><br><span class="line">  vopsdev&#x2F;</span><br><span class="line">    production&#x2F;</span><br><span class="line">      yl-production</span><br><span class="line">    staging&#x2F;</span><br><span class="line">      yl-stagin</span><br><span class="line">    department&#x2F;</span><br><span class="line">      yl-marketing</span><br><span class="line">      yl-hr</span><br><span class="line">      yl-network</span><br><span class="line">      yl-collaboration</span><br><span class="line">      yl-itdev</span><br><span class="line">      yl-itops</span><br><span class="line">    sandbox&#x2F;</span><br><span class="line">      yl-sandbox-1</span><br><span class="line">      yl-sandbox-2</span><br><span class="line">    core&#x2F;</span><br><span class="line">      yl-log-archive</span><br><span class="line">      yl-security</span><br><span class="line">      yl-shared-services</span><br><span class="line">      yl-master</span><br></pre></td></tr></table></figure>
<p>各个账号可以自己独立的创建 VPC. 但是如果 VPC 需要接入公司 VPN, 则需要经过安全部门审批. 经批准后, 账号的 VPC 可以通过 transit gateway attachment 连接到网络部门账号 yl-network 下的 transit gateway 来访问内网资源. DNS, Active Directory 之类的公共服务已经在共享服务账号 yl-shared-services 下创建完成. SSO 在组织账号 yl-master 下配置完成, 使用 AD-Connector 连接到共享服务账号下的 Active Directory 作为身份认证源.</p>
<p>这套环境目前面临的问题是: 由于缺少统一的配置管理方案, 各个账户初始化完成, 交付给用户使用后, 难以继续维持配置基线. 新的配置基线发布后又很难应用到现存的账户. 为此考虑在尽量重用现有服务的基础上实施 Landing Zone.</p>
<h2 id="实施规划">实施规划</h2>
<p>在组织根下创建一个新的 OU 分支, 后面会将 Landing Zone 应用到这个 OU 分支上 <img src="intermediate-organization-structure.png"></p>
<p>然后逐步将现有账号迁移到新的 OU 分支下, 由 Landing Zone Codepipeline 创建并维持基线资源. 而所有新的账号将通过 Landing Zone 的 Accounting Vending Machine 自动创建到新的 OU 分支下.</p>
<p>所有账户迁移完成后即可删除旧的 OU 分支, 最终的账号分布如下 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Root&#x2F;</span><br><span class="line">  vopsdev-landing-zone&#x2F;</span><br><span class="line">    production&#x2F;</span><br><span class="line">      yl-production</span><br><span class="line">    staging&#x2F;</span><br><span class="line">      yl-stagin</span><br><span class="line">    department&#x2F;</span><br><span class="line">      yl-marketing</span><br><span class="line">      yl-hr</span><br><span class="line">      yl-network</span><br><span class="line">      yl-collaboration</span><br><span class="line">      yl-itdev</span><br><span class="line">      yl-itops</span><br><span class="line">    sandbox&#x2F;</span><br><span class="line">      yl-sandbox-1</span><br><span class="line">      yl-sandbox-2</span><br><span class="line">    core&#x2F;</span><br><span class="line">      yl-log-archive</span><br><span class="line">      yl-security</span><br><span class="line">      yl-shared-services</span><br><span class="line">      yl-master</span><br></pre></td></tr></table></figure></p>
<h2 id="资源限制">资源限制</h2>
<p>对于一个新创建的账号, 其 AWS Organization 下默认只能加入很少几个账号 (2个?) 无法满足 Landing Zone 的要求. 如果你想从头开始实施, 请确保提升组织账号的 AWS Organization Account Limit (需要通过给 AWS support 提交 ticket)</p>
]]></content>
      <categories>
        <category>AWS</category>
        <category>Landing Zone</category>
      </categories>
      <tags>
        <tag>AWS</tag>
        <tag>Landing Zone</tag>
      </tags>
  </entry>
  <entry>
    <title>实施 AWS Landing Zone 1: 背景及架构介绍</title>
    <url>/2020-03-08/%E5%AE%9E%E6%96%BD-AWS-Landing-Zone-1-%E8%83%8C%E6%99%AF%E5%8F%8A%E6%9E%B6%E6%9E%84%E4%BB%8B%E7%BB%8D/</url>
    <content><![CDATA[<p>本文介绍了 AWS 多账号管理的需求, Landing Zone 方案的背景及基本结构, Landing Zone 方案和 Control Tower 的对比与选择</p>
<a id="more"></a>
<h2 id="多账户的需求和面临的挑战">多账户的需求和面临的挑战</h2>
<p>随着越来越多的工作负载迁移到 AWS 公有云, 过去的单个账户逐渐无法满足复杂环境的需要: 难以清晰界定的职责边界, 资源无法有效隔离, 对不同部门团队的计费和审计等. 此时会自然而然的引入多账户策略: 不同部门/团队, 不同环境使用不同的 AWS 账号. 优点很明显</p>
<ul>
<li>降低触及单个账户资源上限的风险</li>
<li>不同团队的职责分离</li>
<li>开发/测试/生产环境的资源隔离</li>
<li>减少人为故障的影响范围</li>
</ul>
<p>AWS Organization 提供了多账户环境的基础. 然而创建账户, 加入组织并完成初始化基线配置是一项繁琐的工作. 如何确保各个账户的合规性和配置的一致性? 如果后期配置基线发生变更, 又如何将基线变更追溯的应用到所有的现存账户上? Landing Zone 方案旨在解决这些问题.</p>
<h2 id="landing-zone-简介">Landing Zone 简介</h2>
<p>Landing Zone 是 AWS 在 2018 年推出的一套解决方案 (注意: 并不是一个 AWS 原生服务), 用来帮助用户根据 AWS 最佳实践, 快速建立安全的多账户环境</p>
<p><img src="aws-landing-zone-architecture.png"></p>
<p>上图为 Landing Zone 的基本结构:</p>
<ul>
<li>核心账户: master, shared services, log archive, security
<ul>
<li>Master: 也称为 Organization account. 是创建 AWS Organization, Landing Zone 基础组件 (Service Catalog, CodePipeline, State Machines) 的地方</li>
<li>Shared Services: 公共服务所在的账号</li>
<li>Log Archive: 存放所有账户 Cloudtrail 和 Config log的 S3 bucket 所在的账户</li>
<li>Security: Guarduty master 所在账户, 全局 admin/readonly 角色所在账户</li>
</ul></li>
<li>Accounting Vending Machine (AVM): 是 Landing Zone 在 Service Catalog 里创建的一个产品, 通过运行 AVM 来创建新账户并应用配置基线</li>
<li>Landing Zone Codepipline: 配置变更流水线. 触发该流水线来更改核心账户资源, Service Control Policies 和基线资源等. 该流水线最终会调用 AVM 将基线资源变更应用到所有账户上</li>
</ul>
<p>各个组件/服务更深入的介绍将在后续的系列文章中给出.</p>
<h2 id="control-tower-还是-landing-zone">Control Tower 还是 Landing Zone?</h2>
<p>Landing Zone方案涉及多种AWS服务, 其部署较为复杂, 因此在 2019 年 AWS 推出了 Control Tower 服务, 旨在帮助用户通过最简单的方式设置<strong>全新</strong>的多账户 AWS 环境. 其背后依然是 Landing Zone, 但是隐藏了实现细节和基础服务.</p>
<p>用户在实际生产环境下该如何选择?</p>
<ul>
<li>Control Tower 只适用于全新部署 (greenfield). 如果你已经建立了多账户的环境 (Organization, SSO), 又不想迁移账户, 此时应该选择 Landing Zone</li>
<li>Landing Zone 具有更强的定制化 (当然这也正是它更加复杂的原因), 所有的基线资源都通过 Cloudformation template 创建, 用户可以方便的修改预设基线资源或者添加自定义资源</li>
<li>Landing Zone提供了 CI/CD 流水线, 用户可以将其配置保存在代码仓库中以实现版本控制</li>
</ul>
<h2 id="在现存多账户环境下实施-landing-zone">在现存多账户环境下实施 Landing Zone</h2>
<p>由于实际环境的复杂性, Landing Zone 方案在实施 AWS 最佳实践的同时, 不可避免的会对用户的环境做一些假设, 因此其部署的某些服务或者服务的设置对特定的用户可能并不适用. 此时需要对其预设配置进行调整.</p>
<p>举个例子, 默认 Landing Zone 会在所有账号和共享服务账号 (shared services account) 之间做 VPC peering 以便所有账户可以访问共享服务账号中的公共服务. 然而 VPC peering 本身有诸多限制, 企业环境下可能更多会选择 transit VPC 架构或者使用 transit gateway. 此时用户需要修改 Landing Zone 配置模板以匹配自身环境. 在系列的后续文章中我将逐一列示.</p>
<div class="note warning">
            <p>AWS 声明 Landing Zone 必须由客户的 AWS account 团队或者经过认证的合作伙伴来部署, 以确保方案的实施成功. 如果你想自己实施, 请自行评估风险</p>
          </div>
]]></content>
      <categories>
        <category>AWS</category>
        <category>Landing Zone</category>
      </categories>
      <tags>
        <tag>AWS</tag>
        <tag>Landing Zone</tag>
      </tags>
  </entry>
</search>
