<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>NSX-T 3: 命令行方式部署 NSX-T Manager 集群</title>
    <url>/1636359959/</url>
    <content><![CDATA[<p>使用命令行方式在 vSphere 平台部署 NSX-T 3.1 三节点集群</p>
<span id="more"></span>
<h2 id="为什么使用命令行方式部署-nsx-t-集群">为什么使用命令行方式部署 NSX-T 集群</h2>
<p>这是一个实验环境, 有反复创建/销毁的需求, 图形界面向导的方式略显繁琐. 为此考虑使用命令行的方式来部署:</p>
<ul>
<li>NSX-T Manager 将会被部署到单独的 Management vCenter 下, 而我并不想将 Management vCenter 添加到 NSX-T 的 Compute Managers 里</li>
<li>一旦第一次做成功以后, 后面重新部署只需要复制粘贴即可, 比 UI 方式快捷</li>
</ul>
<h2 id="准备配置模板">准备配置模板</h2>
<p>首先提取 ova 的 spec.json <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">OVA=<span class="string">&quot;/mnt/driver/VMware/NSX-T_3.1/nsx-unified-appliance-3.1.2.0.0.17884005-le.ova&quot;</span></span><br><span class="line">govc import.spec <span class="variable">$OVA</span> &gt; spec.json.tpl</span><br></pre></td></tr></table></figure></p>
<p>根据环境的定制需求, 将 spec.json.tpl 中可变的字段使用变量替换: <figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;Deployment&quot;</span>: <span class="string">&quot;small&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;DiskProvisioning&quot;</span>: <span class="string">&quot;thin&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;IPAllocationPolicy&quot;</span>: <span class="string">&quot;fixedPolicy&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;IPProtocol&quot;</span>: <span class="string">&quot;IPv4&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;PropertyMapping&quot;</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">&quot;Key&quot;</span>: <span class="string">&quot;nsx_passwd_0&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;Value&quot;</span>: <span class="string">&quot;$NSX_PASSWORD&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">&quot;Key&quot;</span>: <span class="string">&quot;nsx_cli_passwd_0&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;Value&quot;</span>: <span class="string">&quot;$NSX_PASSWORD&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">&quot;Key&quot;</span>: <span class="string">&quot;nsx_cli_audit_passwd_0&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;Value&quot;</span>: <span class="string">&quot;$NSX_PASSWORD&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">&quot;Key&quot;</span>: <span class="string">&quot;nsx_cli_username&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;Value&quot;</span>: <span class="string">&quot;admin&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">&quot;Key&quot;</span>: <span class="string">&quot;nsx_cli_audit_username&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;Value&quot;</span>: <span class="string">&quot;audit&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">&quot;Key&quot;</span>: <span class="string">&quot;extraPara&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;Value&quot;</span>: <span class="string">&quot;&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">&quot;Key&quot;</span>: <span class="string">&quot;nsx_hostname&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;Value&quot;</span>: <span class="string">&quot;$NSX_HOSTNAME&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">&quot;Key&quot;</span>: <span class="string">&quot;nsx_role&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;Value&quot;</span>: <span class="string">&quot;NSX Manager&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">&quot;Key&quot;</span>: <span class="string">&quot;nsx_ip_0&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;Value&quot;</span>: <span class="string">&quot;$NSX_IP&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">&quot;Key&quot;</span>: <span class="string">&quot;nsx_netmask_0&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;Value&quot;</span>: <span class="string">&quot;$NETMASK&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">&quot;Key&quot;</span>: <span class="string">&quot;nsx_gateway_0&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;Value&quot;</span>: <span class="string">&quot;$GATEWAY&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">&quot;Key&quot;</span>: <span class="string">&quot;nsx_dns1_0&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;Value&quot;</span>: <span class="string">&quot;$DNS&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">&quot;Key&quot;</span>: <span class="string">&quot;nsx_domain_0&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;Value&quot;</span>: <span class="string">&quot;$DOMAIN&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">&quot;Key&quot;</span>: <span class="string">&quot;nsx_ntp_0&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;Value&quot;</span>: <span class="string">&quot;$NTP&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">&quot;Key&quot;</span>: <span class="string">&quot;nsx_isSSHEnabled&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;Value&quot;</span>: <span class="string">&quot;True&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">&quot;Key&quot;</span>: <span class="string">&quot;nsx_allowSSHRootLogin&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;Value&quot;</span>: <span class="string">&quot;True&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">&quot;Key&quot;</span>: <span class="string">&quot;nsx_swIntegrityCheck&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;Value&quot;</span>: <span class="string">&quot;False&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">&quot;Key&quot;</span>: <span class="string">&quot;mpIp&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;Value&quot;</span>: <span class="string">&quot;&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">&quot;Key&quot;</span>: <span class="string">&quot;mpToken&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;Value&quot;</span>: <span class="string">&quot;&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">&quot;Key&quot;</span>: <span class="string">&quot;mpThumbprint&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;Value&quot;</span>: <span class="string">&quot;&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">&quot;Key&quot;</span>: <span class="string">&quot;mpNodeId&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;Value&quot;</span>: <span class="string">&quot;&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">&quot;Key&quot;</span>: <span class="string">&quot;mpClusterId&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;Value&quot;</span>: <span class="string">&quot;&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">  ],</span><br><span class="line">  <span class="attr">&quot;NetworkMapping&quot;</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">&quot;Name&quot;</span>: <span class="string">&quot;Network 1&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;Network&quot;</span>: <span class="string">&quot;$NETWORK&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">  ],</span><br><span class="line">  <span class="attr">&quot;MarkAsTemplate&quot;</span>: <span class="literal">false</span>,</span><br><span class="line">  <span class="attr">&quot;PowerOn&quot;</span>: <span class="literal">true</span>,</span><br><span class="line">  <span class="attr">&quot;InjectOvfEnv&quot;</span>: <span class="literal">false</span>,</span><br><span class="line">  <span class="attr">&quot;WaitForIP&quot;</span>: <span class="literal">false</span>,</span><br><span class="line">  <span class="attr">&quot;Name&quot;</span>: <span class="literal">null</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="使用-govc-部署-nsx-t-节点">使用 govc 部署 NSX-T 节点</h2>
<p>将需要替换的变量 export 出来, 替换 spec.json.tpl 中的内容, 提供给 govc 来导入 ova <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">OVA=<span class="string">&quot;/mnt/driver/VMware/NSX-T_3.1/nsx-unified-appliance-3.1.2.0.0.17884005-le.ova&quot;</span></span><br><span class="line">DC=<span class="string">&quot;HDC&quot;</span></span><br><span class="line">DS=<span class="string">&quot;NAS-Shared&quot;</span></span><br><span class="line">POOL=<span class="string">&quot;/HDC/host/Mars/Resources/Test&quot;</span></span><br><span class="line">VM_FOLDER=<span class="string">&quot;/HDC/vm/Lab/NSX-T&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> NETWORK=<span class="string">&quot;SG-SVC-11&quot;</span></span><br><span class="line"><span class="built_in">export</span> NETMASK=<span class="string">&quot;255.255.255.0&quot;</span></span><br><span class="line"><span class="built_in">export</span> GATEWAY=<span class="string">&quot;192.168.11.254&quot;</span></span><br><span class="line"><span class="built_in">export</span> DOMAIN=<span class="string">&quot;int.vopsdev.com&quot;</span></span><br><span class="line"><span class="built_in">export</span> DNS=<span class="string">&quot;192.168.11.10&quot;</span></span><br><span class="line"><span class="built_in">export</span> NTP=<span class="string">&quot;1.ntp.svc.vopsdev.com,2.ntp.svc.vopsdev.com&quot;</span></span><br><span class="line"><span class="built_in">export</span> NSX_PASSWORD=<span class="string">&#x27;CHANGEME&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># the 1st one</span></span><br><span class="line"><span class="built_in">export</span> NSX_HOSTNAME=<span class="string">&quot;nsx-0&quot;</span></span><br><span class="line"><span class="built_in">export</span> NSX_IP=<span class="string">&quot;192.168.11.30&quot;</span></span><br><span class="line">envsubst &lt; spec.json.tpl &gt; /tmp/spec.json</span><br><span class="line">govc import.ova --options=/tmp/spec.json --name=<span class="variable">$NSX_HOSTNAME</span> -ds=<span class="variable">$DS</span> -dc=<span class="variable">$DC</span> -pool=<span class="variable">$POOL</span> -folder=<span class="variable">$VM_FOLDER</span> <span class="variable">$OVA</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># the 2nd one</span></span><br><span class="line"><span class="built_in">export</span> NSX_HOSTNAME=<span class="string">&quot;nsx-1&quot;</span></span><br><span class="line"><span class="built_in">export</span> NSX_IP=<span class="string">&quot;192.168.11.31&quot;</span></span><br><span class="line">envsubst &lt; spec.json.tpl &gt; /tmp/spec.json</span><br><span class="line">govc import.ova --options=/tmp/spec.json --name=<span class="variable">$NSX_HOSTNAME</span> -ds=<span class="variable">$DS</span> -dc=<span class="variable">$DC</span> -pool=<span class="variable">$POOL</span> -folder=<span class="variable">$VM_FOLDER</span> <span class="variable">$OVA</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># the 3rd one</span></span><br><span class="line"><span class="built_in">export</span> NSX_HOSTNAME=<span class="string">&quot;nsx-2&quot;</span></span><br><span class="line"><span class="built_in">export</span> NSX_IP=<span class="string">&quot;192.168.11.32&quot;</span></span><br><span class="line">envsubst &lt; spec.json.tpl &gt; /tmp/spec.json</span><br><span class="line">govc import.ova --options=/tmp/spec.json --name=<span class="variable">$NSX_HOSTNAME</span> -ds=<span class="variable">$DS</span> -dc=<span class="variable">$DC</span> -pool=<span class="variable">$POOL</span> -folder=<span class="variable">$VM_FOLDER</span> <span class="variable">$OVA</span></span><br></pre></td></tr></table></figure> 根据环境将变量设置为需要的值, 然后复制粘贴到命令行即可.</p>
<h2 id="节点加入集群">节点加入集群</h2>
<p>SSH 登入 nsx-0, 设置 cluster vip, 获取 cluster id 和 thumbprint <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">set cluster vip 192.168.11.33</span><br><span class="line">get cluster config</span><br><span class="line">get certificate api thumbprint</span><br></pre></td></tr></table></figure></p>
<div class="note info"><p>如果获取 thumbprint 报错 % An error occurred while reading the API server certificate, 需要等一会儿再试</p>
</div>
<p>SSH 登入其他节点, 使用第一个节点上获取的 cluster id 和 thumbprint 加入集群 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">join 192.168.11.30 cluster-id 75fe7cbe-010b-4b11-879c-485380d4b2fd username admin password &#x27;CHANGEME&#x27; thumbprint 924746f887524722e1caf02de69dec73c25284e7770b63cdbc3e619da99dd44e</span><br></pre></td></tr></table></figure></p>
<p>一个节点加入后, 使用 <code>get cluster status</code> 检查集群状态. 直到 Overal Status 变成 STABLE 以后再加下一个.</p>
<p>最后测试通过 VIP 访问 NSX-T Manager UI: <img src="nsx-t-ui.png" /></p>
]]></content>
      <categories>
        <category>VMware</category>
        <category>NSX-T</category>
      </categories>
      <tags>
        <tag>VMware</tag>
        <tag>NSX-T</tag>
        <tag>govc</tag>
      </tags>
  </entry>
  <entry>
    <title>NSX-T 3: 替换 NSX-T Manager 证书</title>
    <url>/3718857964/</url>
    <content><![CDATA[<p>使用 PKI 签发的服务器证书替换 NSX-T Manager 的 cluster 与 node 自签名证书, 消除用户访问时的浏览器警告 <span id="more"></span></p>
<h2 id="通过-pki-为-nsx-manager-签发证书">通过 PKI 为 NSX Manager 签发证书</h2>
<p>方便起见, 这里使用同一个通配符证书来替换 cluster 证书以及所有三个 node 的证书.</p>
<p>证书的 openssl 配置文件: etc/nsx.conf:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[ req ]</span><br><span class="line">default_bits            = 2048                  # RSA key size</span><br><span class="line">encrypt_key             = no                    # Protect private key</span><br><span class="line">default_md              = sha256                # MD to use</span><br><span class="line">utf8                    = yes                   # Input is UTF-8</span><br><span class="line">string_mask             = utf8only              # Emit UTF-8 strings</span><br><span class="line">prompt                  = no                    # Prompt for DN</span><br><span class="line">distinguished_name      = server_dn             # DN template</span><br><span class="line">req_extensions          = server_reqext         # Desired extensions</span><br><span class="line"></span><br><span class="line">[ server_dn ]</span><br><span class="line">0.domainComponent       = com</span><br><span class="line">1.domainComponent       = vopsdev</span><br><span class="line">organizationName        = &quot;VOPSDEV&quot;</span><br><span class="line">commonName              = nsx</span><br><span class="line"></span><br><span class="line">[ server_reqext ]</span><br><span class="line">keyUsage                = critical,digitalSignature,keyEncipherment,dataEncipherment,nonRepudiation</span><br><span class="line">extendedKeyUsage        = serverAuth,clientAuth</span><br><span class="line">subjectKeyIdentifier    = hash</span><br><span class="line">subjectAltName          = @san</span><br><span class="line"></span><br><span class="line">[ san ]</span><br><span class="line">DNS.1   = vopsdev.com</span><br><span class="line">DNS.2   = *.vopsdev.com</span><br><span class="line">DNS.3   = *.int.vopsdev.com</span><br><span class="line">DNS.4   = *.svc.vopsdev.com</span><br><span class="line">DNS.5   = localhost</span><br><span class="line">IP.1    = 192.168.11.30</span><br><span class="line">IP.2    = 192.168.11.31</span><br><span class="line">IP.3    = 192.168.11.32</span><br><span class="line">IP.4    = 192.168.11.33</span><br><span class="line">IP.5    = 127.0.0.1</span><br></pre></td></tr></table></figure>
<p>根据需要调整配置文件的内容.</p>
<p>我的 PKI 包含一个 openssl root CA 和一个 hashicorp vault issuing CA. 使用 openssl 创建私钥和证书请求文件 csr, 然后向 vault issuing CA 请求签发服务器证书</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">openssl genpkey -out key/nsx.key -outform PEM -algorithm RSA -pkeyopt rsa_keygen_bits:2048 -pkeyopt rsa_keygen_pubexp:65537</span><br><span class="line">openssl req -new -config etc/nsx.conf -out csr/nsx.csr -key key/nsx.key</span><br><span class="line"></span><br><span class="line">vault write vopsdev-intermediate-ca-g1/sign-verbatim/general-server csr=@csr/nsx.csr ttl=<span class="string">&quot;87600h&quot;</span></span><br></pre></td></tr></table></figure>
<p>将完整证书链保存为 nsx.full-chain.crt</p>
<h2 id="替换-nsx-manager-证书">替换 NSX Manager 证书</h2>
<p>导入 PKI 的 root CA 证书: System, Settings, Certificates, Import CA Certificate</p>
<p><img src="import-root-ca.png" /></p>
<p>导入 PKI 签发的 nsx 服务器证书与私钥: System, Settings, Certificates, Import Certificate</p>
<p><img src="import-nsx-cert.png" /></p>
<p>导入完成后记录这个证书的 ID</p>
<p>调用 API 设置 cluster 证书和 node 证书</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ID=f7ba2345-f521-4572-860b-9ab4fc5a8c07</span><br><span class="line"></span><br><span class="line"><span class="comment"># cluster</span></span><br><span class="line">curl -k -X POST -u <span class="string">&#x27;admin:CHANGEME&#x27;</span> <span class="string">&quot;https://nsx.svc.vopsdev.com/api/v1/cluster/api-certificate?action=set_cluster_certificate&amp;certificate_id=<span class="variable">$ID</span>&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># nsx-0</span></span><br><span class="line">curl -k -X POST -u <span class="string">&#x27;admin:CHANGEME&#x27;</span> <span class="string">&quot;https://nsx-0.int.vopsdev.com/api/v1/node/services/http?action=apply_certificate&amp;certificate_id=<span class="variable">$ID</span>&quot;</span></span><br><span class="line"><span class="comment"># nsx-1</span></span><br><span class="line">curl -k -X POST -u <span class="string">&#x27;admin:CHANGEME&#x27;</span> <span class="string">&quot;https://nsx-1.int.vopsdev.com/api/v1/node/services/http?action=apply_certificate&amp;certificate_id=<span class="variable">$ID</span>&quot;</span></span><br><span class="line"><span class="comment"># nsx-2</span></span><br><span class="line">curl -k -X POST -u <span class="string">&#x27;admin:CHANGEME&#x27;</span> <span class="string">&quot;https://nsx-2.int.vopsdev.com/api/v1/node/services/http?action=apply_certificate&amp;certificate_id=<span class="variable">$ID</span>&quot;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<div class="note info"><p>这里 NSX-T 会验证 crl, 如果你的 CDP 无法正常访问 (例如很多 ADCS CA 仅仅配置了将 crl 发布到 ldap), 你需要禁用 crl 检查</p>
</div>
<p>最后刷新浏览器测试</p>
<p><img src="check-cert.png" /></p>
]]></content>
      <categories>
        <category>VMware</category>
        <category>NSX-T</category>
      </categories>
      <tags>
        <tag>VMware</tag>
        <tag>NSX-T</tag>
        <tag>TLS</tag>
        <tag>PKI</tag>
      </tags>
  </entry>
  <entry>
    <title>使用 Hashicorp Vault 保存 Ansible 的敏感信息</title>
    <url>/4090725925/</url>
    <content><![CDATA[<p>对于敏感信息 (密码/私钥之类), Ansible 提供了 ansible-vault 工具将其加密保存. 运行 playbook 时, 通过输入密码或者读取密码文件来解密, 算是一个简单有效的方案. Hashicorp Vault 则是一个有名的私密信息管理工具. 如果公司已经开始使用 vault, 当然希望能够集中管理各种私密信息, 而不是各个工具各自为战. 这里简单介绍如何使用 vault 的 KV 密文引擎保存敏感信息, 供 ansible-playbook 使用. <span id="more"></span></p>
<h2 id="准备">准备</h2>
<p>通过 ansible 提供的 lookup 类型插件 hashi_vault 可以读取保存在 vault 的 KV 引擎中的数据 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ansible-doc -t lookup hashi_vault</span><br></pre></td></tr></table></figure> 通过 <code>pip install hvac</code> 来安装 hashi_vault 插件.</p>
<p>本文不会介绍如何启用/配置 vault 密文引擎, 如何配置认证引擎给用户分发 token, 如何为用户设置策略进行权限管理等内容. 假定 vault 在 secret 路径下启用了 KV ver 2 的密文引擎, 用户可以正确获取 token 来访问需要的数据.</p>
<p>使用 KV ver2 引擎的好处是支持多版本, 方便追溯.</p>
<h2 id="示例-rotate-password">示例: rotate password</h2>
<p>产生随机新密码保存到 vault 的 secret/dc1/local-password/root 路径下 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vault kv put secret/dc1/local-password/root value=$(&lt;/dev/urandom tr -dc &#x27;A-Za-z0-9+$%#&lt;=&gt;&#x27; | head -c12)</span><br></pre></td></tr></table></figure> 这样你的命令行历史记录里面不会留下新密码的内容. 可以通过下面命令查看新密码 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vault kv get secret/dc1/local-password/root</span><br><span class="line"></span><br><span class="line">====== Metadata ======</span><br><span class="line">Key              Value</span><br><span class="line">---              -----</span><br><span class="line">created_time     2020-06-07T19:30:14.490822153Z</span><br><span class="line">deletion_time    n/a</span><br><span class="line">destroyed        false</span><br><span class="line">version          1</span><br><span class="line"></span><br><span class="line">==== Data ====</span><br><span class="line">Key      Value</span><br><span class="line">---      -----</span><br><span class="line">value    bqVYx5#VU$0f</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<p>ansible 角色定义的 rotate_local_password/tasks/main.yml: <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line"># tasks file for rotate_local_password</span><br><span class="line">- name: change target account password</span><br><span class="line">  user:</span><br><span class="line">    name: &quot;&#123;&#123; account &#125;&#125;&quot;</span><br><span class="line">    password: &quot;&#123;&#123; new_password | password_hash(&#x27;sha512&#x27;, 65534 | random(seed=inventory_hostname) | string) &#125;&#125;&quot;</span><br></pre></td></tr></table></figure> 为了实现幂等, password_hash 的第二个参数使用了基于 inventory_hostname 产生的 salt 值. 即对于同一个 host, 不管执行多少次, 产生的密码哈希值都一样. 如果不提供第二个参数, 则会使用随机产生的 salt, 导致每次运行都会提示发生了变化 (changed).</p>
<p>Playbook:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">- name: rotate root password in all hosts</span><br><span class="line">  hosts:</span><br><span class="line">  - all</span><br><span class="line">  roles:</span><br><span class="line">  - role: rotate_local_password</span><br><span class="line">    vars:</span><br><span class="line">      vault_param: &quot;secret=secret/data/dc1/local-password/root:data url=https://vault.svc.vopsdev.com:8200 ca_cert=/etc/pki/ca-trust/source/anchors/private-trusted-bundle.pem&quot;</span><br><span class="line">      account: root</span><br><span class="line">      new_password: &quot;&#123;&#123; lookup(&#x27;hashi_vault&#x27;, vault_param)[&#x27;value&#x27;] &#125;&#125;&quot;</span><br></pre></td></tr></table></figure>
<p>ansible 需要获取用户的 vault token 来访问 vault 数据. 将登录 vault 的用户名/密码或者 vault token 直接写入 lookup 参数显然不合适. 有几种可行的方法:</p>
<ul>
<li>用户先执行 <code>vault login ...</code> 通过认证引擎登录 vault, 这样就会在家目录下产生 .vault-token 文件, 用于和 vault 交互</li>
<li>用户将 vault token 通过环境变量 VAULT_TOKEN 导出, 使用此环境变量和 vault 交互</li>
</ul>
<h2 id="示例-rotate-certificate">示例: rotate certificate</h2>
<p>将更新的 https 证书和私钥保存到 vault 中, 然后通过 ansible 分发到服务器上. <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vault kv put secret/dc1/rproxy/certificate value=@fullchain.pem</span><br><span class="line">vault kv put secret/dc1/rproxy/privatekey value=@privkey.pem</span><br></pre></td></tr></table></figure></p>
<p>查看内容 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vault kv get -field=data -format=json secret/dc1/rproxy/certificate | jq -r .value</span><br><span class="line">vault kv get -field=data -format=json secret/dc1/rproxy/privatekey | jq -r .value</span><br><span class="line"># 输出略</span><br></pre></td></tr></table></figure></p>
<p>ansible 角色定义的 deploy_certificate/tasks/main.yml: <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">- name: copy certificate content</span><br><span class="line">  copy:</span><br><span class="line">    content: &quot;&#123;&#123; certificate_content &#125;&#125;&quot;</span><br><span class="line">    dest: /etc/nginx/ssl/server.crt</span><br><span class="line">    owner: root</span><br><span class="line">    group: root</span><br><span class="line">    mode: &#x27;0644&#x27;</span><br><span class="line">  notify: reload nginx</span><br><span class="line"></span><br><span class="line">- name: copy key content</span><br><span class="line">  copy:</span><br><span class="line">    content: &quot;&#123;&#123; key_content &#125;&#125;&quot;</span><br><span class="line">    dest: /etc/nginx/ssl/server.key</span><br><span class="line">    owner: root</span><br><span class="line">    group: root</span><br><span class="line">    mode: &#x27;0644&#x27;</span><br><span class="line">  notify: reload nginx</span><br></pre></td></tr></table></figure></p>
<p>Playbook: <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">- name: deploy certificate and private key to all reverse proxies</span><br><span class="line">  hosts:</span><br><span class="line">  - rproxy</span><br><span class="line">  serial: 1</span><br><span class="line">  vars:</span><br><span class="line">    vault_param: &quot;url=https://vault.svc.vopsdev.com:8200 ca_cert=/etc/pki/ca-trust/source/anchors/private-trusted-bundle.pem&quot;</span><br><span class="line">    cert_param: &quot;secret=secret/data/dc1/rproxy/certificate:data &#123;&#123; vault_param &#125;&#125;&quot;</span><br><span class="line">    key_param: &quot;secret=secret/data/dc1/rproxy/privatekey:data &#123;&#123; vault_param &#125;&#125;&quot;</span><br><span class="line"></span><br><span class="line">  roles:</span><br><span class="line">  - role: deploy_certificate</span><br><span class="line">    vars:</span><br><span class="line">      certificate_content: &quot;&#123;&#123; lookup(&#x27;hashi_vault&#x27;, cert_param)[&#x27;value&#x27;] &#125;&#125;&quot;</span><br><span class="line">      key_content: &quot;&#123;&#123; lookup(&#x27;hashi_vault&#x27;, key_param)[&#x27;value&#x27;] &#125;&#125;&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure> 这里使用 <code>serial: 1</code> 来控制并行度, 确保一台主机上证书更新完成, 服务重新加载配置后再执行下一台 host.</p>
]]></content>
      <categories>
        <category>Automation</category>
        <category>Ansible</category>
      </categories>
      <tags>
        <tag>ansible</tag>
        <tag>vault</tag>
        <tag>Hashicorp</tag>
      </tags>
  </entry>
  <entry>
    <title>使用 tlog 在 CentOS 7 上实施 ssh 会话记录</title>
    <url>/913795803/</url>
    <content><![CDATA[<p>Native Session Recording 是 RHEL 8 / CentOS 8 引入的新功能, 可以方便的录制用户会话用以审计. 按照<a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/recording_sessions/index">官方文档</a>的步骤, 在 RHEL 8 或者 CentOS 8 上配置非常容易. 由于目前我的大部分服务器还是跑在 CentOS 7 上, 很自然的想能否将这套方案在 CentOS 7 上部署出来. 原本以为很容易, 实际做下来才发现坑不少. 这里做一个简要的记录 <span id="more"></span></p>
<h2 id="tlog-会话记录的组件和原理">tlog 会话记录的组件和原理</h2>
<p>之前也有不少开源工具可以提供 ssh 会话录制的功能, 比如 script + scriptreplay. 不过也就是玩具的水平, 要真正生产可用还需要大量的 DIY 脚本来辅助. 而 tlog 算是一个相对比较成熟的开源方案了: 使用简便, 搭配 cockpit 可以方便的管理会话记录, 针对重放也提供了丰富的控制.</p>
<h3 id="基本原理">基本原理</h3>
<p>核心组件是 tlog + sssd</p>
<ul>
<li>用户通过 pam 机制登入</li>
<li>sssd-session-recording 会将 nss response 中的用户 shell 替换为 tlog-rec-session</li>
<li>启动 tlog-rec-session</li>
<li>tlog-rec-session 通过配置文件获取用户的原始 shell</li>
<li>tlog-rec-session 启动用户的原始 shell</li>
<li>tlog-rec-session 位于 user terminal 和 user shell 中间, 记录所有通过的数据, 发送给 journal</li>
</ul>
<h3 id="web-ui">Web UI</h3>
<p>cockpit-session-recording 提供一个 web 界面来管理记录的会话, 并提供一个简易的回放播放器.</p>
<p>针对记录会话功能, Web UI 并不是必须的组件. 然而直接通过命令行 <code>journalctl -o verbose | grep -i '"rec"'</code> 来过滤感兴趣会话, 体验并不是很友好</p>
<h2 id="编译-rpm-包">编译 rpm 包</h2>
<p>CentOS 7 的源默认不要带 tlog, 可以使用 tar ball 源码编译为 rpm. Cockpit 软件包有一部分, 但是不包含 cockpit-session-recording 组件, 因此也需要单独编译 rpm</p>
<p>当前 tlog 最新版本为 v8-2, cockpit-session-recording 最新版本为 v4</p>
<h3 id="tlog">tlog</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">wget -c https://github.com/Scribery/tlog/releases/download/v8-2/tlog-8.tar.gz</span><br><span class="line">yum install -y gcc systemd-devel json-c-devel libcurl-devel rpm-build autoconf automake libtool</span><br><span class="line">rpmbuild -tb tlog-8.tar.gz</span><br></pre></td></tr></table></figure>
<h3 id="cockpit-session-recording">cockpit-session-recording</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">wget -c https://github.com/Scribery/cockpit-session-recording/releases/download/4/cockpit-session-recording-4.tar.gz</span><br><span class="line">yum install -y libappstream-glib-devel</span><br><span class="line">rpmbuild -tb cockpit-session-recording-4.tar.gz</span><br></pre></td></tr></table></figure>
<h2 id="方案部署">方案部署</h2>
<h3 id="配置持久化-journal">配置持久化 journal</h3>
<p>默认 tlog 将记录的会话保存在 journal 中 (也支持发送给 Elasticsearch). 如果不存在 /var/log/journal 目录, 则 journal 将保存在 /run/log/journal 下, 为非持久存储, 重启丢失. 因此首先需要将 journal 保存到持久存储:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mkdir -p /var/<span class="built_in">log</span>/journal /etc/systemd/journald.conf.d</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;<span class="string">EOF &gt; /etc/systemd/journald.conf.d/99-persistent.conf</span></span><br><span class="line"><span class="string">[Journal]</span></span><br><span class="line"><span class="string">Storage=persistent</span></span><br><span class="line"><span class="string">Compress=yes</span></span><br><span class="line"><span class="string">SyncIntervalSec=5m</span></span><br><span class="line"><span class="string">RateLimitInterval=30s</span></span><br><span class="line"><span class="string">RateLimitBurst=1000</span></span><br><span class="line"><span class="string">SystemMaxUse=10G</span></span><br><span class="line"><span class="string">SystemMaxFileSize=200M</span></span><br><span class="line"><span class="string">MaxRetentionSec=7week</span></span><br><span class="line"><span class="string">ForwardToSyslog=no</span></span><br><span class="line"><span class="string">EOF</span></span><br><span class="line"></span><br><span class="line">systemctl restart systemd-journald.service</span><br></pre></td></tr></table></figure>
<h3 id="安装-tlog-和-cockpit-session-recording">安装 tlog 和 cockpit-session-recording</h3>
<p>直接安装前面编译出来的 rpm 即可 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum install -y cockpit</span><br><span class="line">yum localinstall -y tlog-8-2.el7.x86_64.rpm cockpit-session-recording-4-1.el7.noarch.rpm</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> --now cockpit.socket</span><br></pre></td></tr></table></figure></p>
<h3 id="配置-sssd">配置 sssd</h3>
<p>CentOS 7 使用 1.16.x 的 sssd, 而 RHEL 8/CentOS 8 使用 2.2.x 版本. 部分行为有所差别.</p>
<p>2.2.x 版本的 sssd 使了 <code>--enable-files-domain</code> 编译参数, 允许在没有 /etc/sssd/sssd.conf 时启动, 而 1.16.x 使用了 <code>--disable-files-domain</code>, 要求 /etc/sssd/sssd.conf 必须存在.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum install sssd-common sssd-client -y</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;<span class="string">EOF &gt; /etc/sssd/sssd.conf</span></span><br><span class="line"><span class="string">[sssd]</span></span><br><span class="line"><span class="string">services = nss, pam</span></span><br><span class="line"><span class="string">domains = local</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">[nss]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">[pam]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">[domain/local]</span></span><br><span class="line"><span class="string">id_provider = files</span></span><br><span class="line"><span class="string">EOF</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># pay attention to the permission</span></span><br><span class="line">chmod 600 /etc/sssd/sssd.conf</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;<span class="string">EOF &gt; /etc/sssd/conf.d/sssd-session-recording.conf</span></span><br><span class="line"><span class="string">[session_recording]</span></span><br><span class="line"><span class="string">scope=all</span></span><br><span class="line"><span class="string">users=</span></span><br><span class="line"><span class="string">groups=</span></span><br><span class="line"><span class="string">EOF</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># pay attention to the permission</span></span><br><span class="line">chmod 600 /etc/sssd/conf.d/sssd-session-recording.conf</span><br><span class="line">systemctl restart sssd</span><br></pre></td></tr></table></figure>
<h3 id="修正-nsswitch">修正 nsswitch</h3>
<p>CentOS 8 / RHEL 8 的 /etc/nsswitch.conf 相对 CentOS 7 / RHEL 7 发生了细微的变化: 对 passwd 和 group, sss 排到了 files 前面 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">passwd:     sss files</span><br><span class="line">group:      sss files</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p>
<p>这里也需要把 passwd 和 group 条目中的 sss 排到 files 前面, 否则用户登录时会话记录无法生效</p>
<p>用下面命令检查一个允许登录的本地账号, 确认设置生效. 命令显示的用户 login shell 应该是 /usr/bin/tlog-rec-session, 虽然在 /etc/passwd 中是 /bin/bash</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">getent passwd alice</span><br><span class="line">getent passwd -s sss alice</span><br></pre></td></tr></table></figure>
<h3 id="测试会话记录">测试会话记录</h3>
<p>用本地账号 (alice) 通过 ssh 登录系统确认会话记录已经生效 <img src="alice-ssh-session-recorded.png" /></p>
<p>进行一些操作后, 退出登录. 通过 cockpit 界面查看记录的会话 <img src="alice-session-cockpit.png" /></p>
<p>测试重放 <img src="alice-session-replay-web.png" /></p>
<h3 id="测试本地命令行回放">测试本地命令行回放</h3>
<p>从 cockpit 界面上选择一个保存的会话, 记录其 ID. 或者通过 <code>journalctl -o verbose | grep -i '"rec"'</code>来找.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">REC_ID=2f84bc33bce0486394706de9be91e72e-965-a3cc</span><br><span class="line">tlog-play -r journal -M TLOG_REC=<span class="variable">$REC_ID</span></span><br></pre></td></tr></table></figure>
<h3 id="测试异地命令行回放">测试异地命令行回放</h3>
<p>需要通过 systemd-journal-remote 命令将 journal 导出为文件, 复制到目标机器上. 在 CentOS 7 中 systemd-journal-remote 命令由 systemd-journal-gateway 包提供</p>
<p>从源服务器导出 journal <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum -y install systemd-journal-gateway</span><br><span class="line"></span><br><span class="line">REC_ID=2f84bc33bce0486394706de9be91e72e-965-a3cc</span><br><span class="line">journalctl -o <span class="built_in">export</span> TLOG_REC=<span class="variable">$REC_ID</span> | /usr/lib/systemd/systemd-journal-remote -o /tmp/mysession.journal -</span><br></pre></td></tr></table></figure></p>
<p>将 /tmp/mysession.journal 复制到到目标服务器, 需要放到 /var/log/journal 下. 在目标服务器上实现重放只需要安装 tlog <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum localinstall -y tlog-8-2.el7.x86_64.rpm</span><br><span class="line"></span><br><span class="line">mkdir -p /var/<span class="built_in">log</span>/journal</span><br><span class="line">mv /tmp/mysession.journal /var/<span class="built_in">log</span>/journal/</span><br><span class="line"></span><br><span class="line">REC_ID=2f84bc33bce0486394706de9be91e72e-965-a3cc</span><br><span class="line">tlog-play -r journal --file-path=mysession.journal -M TLOG_REC=<span class="variable">$REC_ID</span></span><br></pre></td></tr></table></figure> <img src="alice-session-remote-replay.png" /></p>
]]></content>
      <categories>
        <category>CentOS</category>
        <category>Administration</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>CentOS</tag>
      </tags>
  </entry>
  <entry>
    <title>单节点 OKD4 SNO - Assisted Installer</title>
    <url>/3946862006/</url>
    <content><![CDATA[<p>Single Node Openshift (SNO), 顾名思义就是只有一个节点的 Openshift. 让用户可以在资源紧缺的边缘设备上运行 Openshift 工作负载. 对于动辄需要几十上百G内存的普通集群, SNO 也非常适合本地开发/测试环境. 这里将记录如何使用 Assisted Installer 方式来部署单节点 OKD (SNO) 4.12 <span id="more"></span></p>
<h2 id="一些基本信息">一些基本信息</h2>
<p>Openshift 从 4.9 开始支持 SNO, 然而 OKD 官方文档上的 SNO 安装部分目前已经被删除 (4.13 是目前最新版本). 因此这里将主要参考 OCP 文档的相关部分以及网络上的一些其他资源. OKD 版本为 4.12.0-0.okd-2023-04-01-051724</p>
<h2 id="准备基础设施">准备基础设施</h2>
<p>本文将在 vSphere 8 环境安装 OKD 4.12 SNO.</p>
<p>准备好必要的 DNS 条目. 这里将使用静态 IP 地址, 无需 DHCP <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hdc-srv-1.int.vopsdev.com           A       192.168.11.71</span><br><span class="line">api.cls-sno-1.vopsdev.com           CNAME   hdc-srv-1.int.vopsdev.com</span><br><span class="line">api-int.cls-sno-1.vopsdev.com       CNAME   hdc-srv-1.int.vopsdev.com</span><br><span class="line">*.apps.cls-sno-1.vopsdev.com        CNAME   hdc-srv-1.int.vopsdev.com</span><br></pre></td></tr></table></figure></p>
<h2 id="准备-assisted-installer">准备 assisted installer</h2>
<p>我们将在一台控制机上运行 <a href="https://github.com/openshift/assisted-service">assisted service</a>. 请确保将来的 SNO 节点可以访问到控制机来返回/上报节点状态.</p>
<p>将 assisted-service 代码库拉下来后, 修改 <code>deploy/podman/okd-configmap.yml</code> 文件 <figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">config</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">ASSISTED_SERVICE_HOST:</span> <span class="number">192.168</span><span class="number">.255</span><span class="number">.247</span><span class="string">:8090</span></span><br><span class="line">  <span class="string">...</span></span><br><span class="line">  <span class="attr">IMAGE_SERVICE_BASE_URL:</span> <span class="string">http://192.168.255.247:8888</span></span><br><span class="line">  <span class="string">...</span></span><br><span class="line">  <span class="attr">OS_IMAGES:</span> <span class="string">&#x27;[&#123;&quot;openshift_version&quot;:&quot;4.12&quot;,&quot;cpu_architecture&quot;:&quot;x86_64&quot;,&quot;url&quot;:&quot;https://builds.coreos.fedoraproject.org/prod/streams/stable/builds/37.20221127.3.0/x86_64/fedora-coreos-37.20221127.3.0-live.x86_64.iso&quot;,&quot;version&quot;:&quot;37.20221127.3.0&quot;&#125;]&#x27;</span></span><br><span class="line">  <span class="string">...</span></span><br><span class="line">  <span class="attr">RELEASE_IMAGES:</span> <span class="string">&#x27;[&#123;&quot;openshift_version&quot;:&quot;4.12&quot;,&quot;cpu_architecture&quot;:&quot;x86_64&quot;,&quot;cpu_architectures&quot;:[&quot;x86_64&quot;],&quot;url&quot;:&quot;quay.io/openshift/okd:4.12.0-0.okd-2023-04-01-051724&quot;,&quot;version&quot;:&quot;4.12.0-0.okd-2023-04-01-051724&quot;,&quot;default&quot;:true&#125;]&#x27;</span></span><br></pre></td></tr></table></figure></p>
<p><code>ASSISTED_SERVICE_HOST</code> 和 <code>IMAGE_SERVICE_BASE_URL</code> 原来用的 127.0.0.1 的地址, 替换为控制机的地址. <code>OS_IMAGES</code> 替换为目标 OKD 版本对应的 fedora-coreos ISO 地址. <code>RELEASE_IMAGES</code> 替换为目标 OKD 版本</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 获取指定 OKD 版本的对应 fedora-coreos ISO 地址</span></span><br><span class="line">RELEASE_PATH=openshift/okd:4.12.0-0.okd-2023-04-01-051724</span><br><span class="line">oc adm release extract --tools quay.io/<span class="variable">$RELEASE_PATH</span></span><br><span class="line">tar xvf openshift-install-*.tar.gz</span><br><span class="line">ISO_URL=$(./openshift-install coreos print-stream-json | jq -r .architectures.x86_64.artifacts.metal.formats.iso.disk.location)</span><br><span class="line"></span><br><span class="line"><span class="comment"># https://builds.coreos.fedoraproject.org/prod/streams/stable/builds/37.20221127.3.0/x86_64/fedora-coreos-37.20221127.3.0-live.x86_64.iso</span></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$ISO_URL</span></span><br></pre></td></tr></table></figure>
<p>修改 <code>Makefile</code> 文件中 healthcheck 的地址, 原本是 127.0.0.1, 改为控制机的地址 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">deploy-onprem:</span><br><span class="line">        podman play kube --configmap $&#123;PODMAN_CONFIGMAP&#125; deploy/podman/pod.yml</span><br><span class="line">        ./hack/retry.sh 90 2 &quot;curl -f http://192.168.255.247:8090/ready&quot;</span><br><span class="line">        ./hack/retry.sh 60 10 &quot;curl -f http://192.168.255.247:8888/health&quot;</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p>
<h2 id="使用-assisted-installer-部署单节点-okd-4.12">使用 assisted installer 部署单节点 OKD 4.12</h2>
<p>先在 vSphere 上创建一个虚拟机, 最小规格为 8 vCPU/16G RAM/120G Disk, 记录其 MAC 地址, 后面会用到.</p>
<p>使用 <code>make deploy-onprem OKD=true</code> 命令启动 assisted-service. 等待 healthcheck 通过. 通过浏览器打开控制机的8080端口地址 <img src="assisted-create-new-cluster.PNG" /></p>
<p>在创建新集群页面点击 Next 进入 Cluster details 页面. 输入集群信息 cluster name, base domain, 选择 openshift 版本/架构, 勾选 SNO 模式, 选择静态网络配置 Static IP, bridge, and bonds. <img src="assisted-cluster-details.PNG" /></p>
<p>网络设置页面填写 DNS/Subnet/Default gateway <img src="assisted-static-network-config-1.PNG" /></p>
<p>继续填写节点的 MAC 地址和 IP 地址 <img src="assisted-static-network-config-2.PNG" /></p>
<p>Operators 页面不选择安装额外的 operators, 直接 Next 到 Host discovery 页面. 点击 Add host 来生成 discovery ISO. 根据自己的环境输入 SSH public key, trusted CA bundle <img src="assisted-discovery-iso.PNG" /></p>
<p>点击 Download Discovery ISO 下载下来. 到 vSphere 上为虚拟机挂载该 ISO, 设置启动时连接, 然后启动该虚拟机. 等待一段时间后, 该节点会出现在 Host Inventory 中 <img src="assisted-host-discovery.PNG" /></p>
<p>Storage 页面和 Networking 页面按需调整, 否则直接 Next. 到 Review and create 页面, 确认信息无误后 Install cluster 开始安装. <img src="assisted-installation-progress.PNG" /></p>
<p>然后等待安装结束, 大约需要半小时. <img src="assisted-installation-done.PNG" /></p>
<p>至此单节点 OKD 安装完成, 可以根据提示进行访问.</p>
<p>清理 assisted-service: <code>make clear-all</code></p>
<h2 id="参考资料">参考资料</h2>
<ul>
<li><a href="https://docs.openshift.com/container-platform/4.13/installing/installing_sno/install-sno-preparing-to-install-sno.html" class="uri">https://docs.openshift.com/container-platform/4.13/installing/installing_sno/install-sno-preparing-to-install-sno.html</a></li>
<li><a href="https://vrutkovs.eu/posts/okd-disconnected-assisted/" class="uri">https://vrutkovs.eu/posts/okd-disconnected-assisted/</a></li>
</ul>
]]></content>
      <categories>
        <category>RedHat</category>
        <category>Openshift</category>
      </categories>
      <tags>
        <tag>Openshift</tag>
        <tag>OKD4</tag>
        <tag>SNO</tag>
      </tags>
  </entry>
  <entry>
    <title>单节点 OKD4 SNO - 手工方式</title>
    <url>/2503623812/</url>
    <content><![CDATA[<p>使用手工方式部署单节点 OKD SNO 4.12, 记录其中的坑和解决方法. 前文介绍的 assisted installer 方式比较简单, 但是需要用户的交互. 这里虽然是手工方式, 但是很容易转化为自动化流水线. 使用的 OKD 版本依然是 4.12.0-0.okd-2023-04-01-051724 <span id="more"></span></p>
<h2 id="直接使用-ocp-文档会遇到的问题">直接使用 OCP 文档会遇到的问题</h2>
<p>由于 rhcos 和 fedora coreos 的差异, 直接照搬 OCP 文档上创建内嵌 ignotion 文件的 ISO 来进行安装的方法会遇到各种各样的问题.</p>
<h3 id="var-空间不足">/var 空间不足</h3>
<p>如果使用文档中的最小规格 8 vCPU/16G RAM/120G Disk, 初次启动就会遇到下面的错误</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">journalctl -f -u bootkube.service</span><br><span class="line">...</span><br><span class="line">Jul 11 11:57:37 hdc-srv-1.int.vopsdev.com bootkube.sh[14907]: Error: writing blob: adding layer with blob <span class="string">&quot;sha256:a33a998036bf3429b7c0cdd5ae4a78921403f2b4dbedacb0da84559b5cc4156f&quot;</span>: processing tar file(write /var/lib/rpm/Packages: no space left on device): <span class="built_in">exit</span> status 1</span><br></pre></td></tr></table></figure>
<p>查看空间使用量, 可以发现 <code>loop0</code> 设备只有 3.1G, 而 <code>/var</code> 目录挂载自 <code>loop0</code> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">df -h</span><br><span class="line"></span><br><span class="line">Filesystem      Size  Used Avail Use% Mounted on</span><br><span class="line">devtmpfs        4.0M     0  4.0M   0% /dev</span><br><span class="line">tmpfs           7.9G   84K  7.9G   1% /dev/shm</span><br><span class="line">tmpfs           3.2G  3.1G   88M  98% /run</span><br><span class="line">/dev/sr0        746M  746M     0 100% /run/media/iso</span><br><span class="line">/dev/loop1      625M  625M     0 100% /sysroot</span><br><span class="line">/dev/loop0      3.1G  3.0G  129M  96% /run/ephemeral</span><br><span class="line">tmpfs           7.9G     0  7.9G   0% /tmp</span><br><span class="line">tmpfs           1.6G     0  1.6G   0% /run/user/1000</span><br><span class="line">overlay         3.1G  3.0G  129M  96% /var/lib/containers/storage/overlay/e78d92d6f79340cbc2bc7c1bc580d0f0d0c6cbb0101458b6725c450958eeb2e1/merged</span><br></pre></td></tr></table></figure></p>
<p>通过这个 <a href="https://github.com/coreos/fedora-coreos-tracker/issues/1344">fedora coreos issue 1344</a> 可以知道当前版本的 fedora coreos 默认使用内存总量的 20% 用于 <code>/run/ephemeral</code></p>
<h3 id="试图修改-sysroot-只读分区">试图修改 /sysroot 只读分区</h3>
<p>fedora coreos 其实是一个泛用的操作系统, 实际 OKD 安装过程中会使用其对应版本的 fedora-coreos 镜像内容 "替换" 掉原来的系统. <code>release-image-pivot</code> 服务进行 <code>rpm-ostree rebase</code> 时会出现下面的错误</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">journalctl -f -u release-image-pivot.service</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line">Jul 11 12:12:39 hdc-srv-1.int.vopsdev.com bootstrap-pivot.sh[1743]: ++ chmod 0644 /etc/containers/registries.conf</span><br><span class="line">Jul 11 12:12:39 hdc-srv-1.int.vopsdev.com bootstrap-pivot.sh[1743]: ++ rpm-ostree rebase --experimental ostree-unverified-registry:quay.io/openshift/okd-content@sha256:c858d7d2086ca4a7b3b340c2ea32b26b68967fb66b918f8ac027020ca18d5193</span><br><span class="line">Jul 11 12:12:39 hdc-srv-1.int.vopsdev.com bootstrap-pivot.sh[2793]: error: Remounting /sysroot read-write: Permission denied</span><br></pre></td></tr></table></figure>
<p>而 <code>/sysroot</code> 来自 live ISO 的挂载, 不允许写入</p>
<h3 id="journal-格式不匹配">Journal 格式不匹配</h3>
<p>在安装的最后阶段 machine config operator 会因为 <code>journalctl</code> 无法识别日志文件而失败. 具体看这个 <a href="https://github.com/okd-project/okd/issues/1607">OKD issue 1607</a></p>
<h2 id="解决方法和步骤">解决方法和步骤</h2>
<p>主要从 assisted installer 的安装脚本中借用一部分, 追加到 ignition 文件中, 可以解决上面的问题</p>
<h3 id="准备基础设施">准备基础设施</h3>
<p>本文将在 vSphere 8 环境安装 OKD 4.12 SNO.</p>
<p>准备好必要的 DNS 条目. 这里将使用静态 IP 地址, 无需 DHCP <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hdc-srv-1.int.vopsdev.com           A       192.168.11.71</span><br><span class="line">api.cls-sno-1.vopsdev.com           CNAME   hdc-srv-1.int.vopsdev.com</span><br><span class="line">api-int.cls-sno-1.vopsdev.com       CNAME   hdc-srv-1.int.vopsdev.com</span><br><span class="line">*.apps.cls-sno-1.vopsdev.com        CNAME   hdc-srv-1.int.vopsdev.com</span><br></pre></td></tr></table></figure></p>
<h3 id="准备二进制文件">准备二进制文件</h3>
<p>提取 <code>openshift-install</code> 并下载对应版本的 fedora coreos ISO.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">RELEASE_PATH=openshift/okd:4.12.0-0.okd-2023-04-01-051724</span><br><span class="line">oc adm release extract --tools quay.io/<span class="variable">$RELEASE_PATH</span></span><br><span class="line">tar xvf openshift-install-*.tar.gz</span><br><span class="line">ISO_URL=$(./openshift-install coreos print-stream-json | jq -r .architectures.x86_64.artifacts.metal.formats.iso.disk.location)</span><br><span class="line">curl -L <span class="variable">$ISO_URL</span> -o fedora-coreos-live.iso</span><br></pre></td></tr></table></figure>
<p>同时准备后面会用到的 <a href="https://github.com/coreos/butane/releases/download/v0.18.0/butane-x86_64-unknown-linux-gnu">butane</a> 和 <a href="https://mirror.openshift.com/pub/openshift-v4/clients/coreos-installer/latest/coreos-installer">coreos-install</a></p>
<h3 id="生成初始的-ignition-文件">生成初始的 ignition 文件</h3>
<p>准备常规的 <code>install-config.yaml</code> <figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">baseDomain:</span> <span class="string">vopsdev.com</span></span><br><span class="line"><span class="attr">compute:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">worker</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">0</span></span><br><span class="line"><span class="attr">controlPlane:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">master</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">cls-sno-1</span></span><br><span class="line"><span class="attr">networking:</span></span><br><span class="line">  <span class="attr">clusterNetwork:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">cidr:</span> <span class="number">10.128</span><span class="number">.0</span><span class="number">.0</span><span class="string">/14</span></span><br><span class="line">    <span class="attr">hostPrefix:</span> <span class="number">23</span></span><br><span class="line">  <span class="attr">machineNetwork:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">cidr:</span> <span class="number">192.168</span><span class="number">.11</span><span class="number">.0</span><span class="string">/24</span></span><br><span class="line">  <span class="attr">networkType:</span> <span class="string">OVNKubernetes</span></span><br><span class="line">  <span class="attr">serviceNetwork:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="number">172.30</span><span class="number">.0</span><span class="number">.0</span><span class="string">/16</span></span><br><span class="line"><span class="attr">platform:</span></span><br><span class="line">  <span class="attr">none:</span> &#123;&#125;</span><br><span class="line"><span class="attr">publish:</span> <span class="string">External</span></span><br><span class="line"><span class="attr">bootstrapInPlace:</span></span><br><span class="line">  <span class="attr">installationDisk:</span> <span class="string">/dev/sda</span></span><br><span class="line"><span class="attr">additionalTrustBundle:</span> <span class="string">XXX</span></span><br><span class="line"><span class="attr">pullSecret:</span> <span class="string">XXX</span></span><br><span class="line"><span class="attr">sshKey:</span> <span class="string">XXX</span></span><br></pre></td></tr></table></figure> 创建 ignition 文件, 复制一份出来, 后面会用到 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mkdir assets</span><br><span class="line">cp install-config.yaml assets/</span><br><span class="line">./openshift-install --dir=assets create single-node-ignition-config</span><br><span class="line">cp assets/bootstrap-in-place-for-live-iso.ign sno.ign</span><br></pre></td></tr></table></figure></p>
<h3 id="定制-ignition-文件">定制 ignition 文件</h3>
<p>通过定制 ignition 文件来实现:</p>
<ul>
<li>添加 machine config 配置, 对 sno master 进行定制, 这里定制了 <code>/etc/chrony.conf</code> 并创建了一个一次性的服务 <code>systemd-clear-journal.service</code> 来清理 journal 文件, 解决上面 journal 格式不匹配的问题</li>
<li>为 bootstrap 过程添加服务 <code>okd-overlay.service</code> 及其对应的脚本 <code>/usr/local/bin/okd-binaries.sh</code> 解决上面剩余的两个问题</li>
</ul>
<p>我们将使用 <code>butane</code> 工具来生成 machine config 的清单文件. 首先编写 butane 格式的 machine config 配置清单 <code>98-machine-config-maser-sno.bu</code> <figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">variant:</span> <span class="string">openshift</span></span><br><span class="line"><span class="attr">version:</span> <span class="number">4.12</span><span class="number">.0</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">machineconfiguration.openshift.io/role:</span> <span class="string">master</span></span><br><span class="line">  <span class="attr">name:</span> <span class="number">98</span><span class="string">-master-sno</span></span><br><span class="line"><span class="attr">storage:</span></span><br><span class="line">  <span class="attr">files:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">path:</span> <span class="string">/etc/chrony.conf</span></span><br><span class="line">    <span class="attr">mode:</span> <span class="number">0644</span></span><br><span class="line">    <span class="attr">overwrite:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">contents:</span></span><br><span class="line">      <span class="attr">inline:</span> <span class="string">|</span></span><br><span class="line"><span class="string">        server 1.ntp.svc.vopsdev.com iburst</span></span><br><span class="line"><span class="string">        server 2.ntp.svc.vopsdev.com iburst</span></span><br><span class="line"><span class="string">        driftfile /var/lib/chrony/drift</span></span><br><span class="line"><span class="string">        makestep 1.0 3</span></span><br><span class="line"><span class="string">        rtcsync</span></span><br><span class="line"><span class="string">        logdir /var/log/chrony</span></span><br><span class="line"><span class="string"></span><span class="attr">systemd:</span></span><br><span class="line">  <span class="attr">units:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">contents:</span> <span class="string">|</span></span><br><span class="line"><span class="string">      [Unit]</span></span><br><span class="line"><span class="string">      Description=Clear Journal to Remove Corrupt File</span></span><br><span class="line"><span class="string">      DefaultDependencies=no</span></span><br><span class="line"><span class="string">      After=kubelet.service</span></span><br><span class="line"><span class="string"></span></span><br><span class="line">      [<span class="string">Service</span>]</span><br><span class="line">      <span class="string">Type=oneshot</span></span><br><span class="line">      <span class="string">RemainAfterExit=yes</span></span><br><span class="line">      <span class="string">ExecStart=bash</span> <span class="string">-c</span> <span class="string">&quot;/usr/bin/journalctl --rotate &amp;&amp; /usr/bin/journalctl --vacuum-time=1s&quot;</span></span><br><span class="line">      <span class="string">TimeoutSec=0</span></span><br><span class="line"></span><br><span class="line">      [<span class="string">Install</span>]</span><br><span class="line">      <span class="string">WantedBy=multi-user.target</span></span><br><span class="line">    <span class="attr">enabled:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">systemd-clear-journal.service</span></span><br><span class="line"></span><br></pre></td></tr></table></figure> 使用 <code>butane</code> 生成 machine config 的 yaml 清单 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">butane 98-machine-config-maser-sno.bu -o 98-machine-config-maser-sno.yaml</span><br></pre></td></tr></table></figure></p>
<p><code>okd-binaries.sh</code> 脚本的内容如下: <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/env bash</span></span><br><span class="line"><span class="built_in">set</span> -eux</span><br><span class="line"><span class="comment"># Fetch an image with OKD rpms</span></span><br><span class="line">RPMS_IMAGE=<span class="string">&quot;quay.io/openshift/okd-content@sha256:10c02c766abb34085023df151e7855a848fa5e212e21a4e8163e7aae3d1e9651&quot;</span></span><br><span class="line"><span class="keyword">while</span> ! podman pull --quiet <span class="string">&quot;<span class="variable">$&#123;RPMS_IMAGE&#125;</span>&quot;</span></span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;Pull failed. Retrying <span class="variable">$&#123;RPMS_IMAGE&#125;</span>...&quot;</span></span><br><span class="line">    sleep 5</span><br><span class="line"><span class="keyword">done</span></span><br><span class="line">mnt=$(podman image mount <span class="string">&quot;<span class="variable">$&#123;RPMS_IMAGE&#125;</span>&quot;</span>)</span><br><span class="line"><span class="comment"># Install RPMs in overlayed FS</span></span><br><span class="line">mkdir /tmp/rpms</span><br><span class="line">cp -rvf <span class="variable">$&#123;mnt&#125;</span>/rpms/* /tmp/rpms</span><br><span class="line"><span class="comment"># If RPMs image contants manifests these need to be copied as well</span></span><br><span class="line">mkdir -p /opt/openshift/openshift</span><br><span class="line">cp -rvf <span class="variable">$&#123;mnt&#125;</span>/manifests/* /opt/openshift/openshift || <span class="literal">true</span></span><br><span class="line">tmpd=$(mktemp -d)</span><br><span class="line">mkdir <span class="variable">$&#123;tmpd&#125;</span>/&#123;upper,work&#125;</span><br><span class="line">mount -t overlay -o lowerdir=/usr,upperdir=<span class="variable">$&#123;tmpd&#125;</span>/upper,workdir=<span class="variable">$&#123;tmpd&#125;</span>/work overlay /usr</span><br><span class="line">rpm -Uvh /tmp/rpms/*</span><br><span class="line">podman rmi -f <span class="string">&quot;<span class="variable">$&#123;RPMS_IMAGE&#125;</span>&quot;</span></span><br><span class="line"><span class="comment"># Symlink kubelet pull secret</span></span><br><span class="line">mkdir -p /var/lib/kubelet</span><br><span class="line">ln -s /root/.docker/config.json /var/lib/kubelet/config.json</span><br><span class="line"><span class="comment"># Expand /var to 6G if necessary</span></span><br><span class="line"><span class="keyword">if</span> (( $(<span class="built_in">stat</span> -c%s /run/ephemeral.xfsloop) &gt; 6*1024*1024*1024 )); <span class="keyword">then</span></span><br><span class="line">  <span class="built_in">exit</span> 0</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line">/bin/truncate -s 6G /run/ephemeral.xfsloop</span><br><span class="line">losetup -c /dev/loop0</span><br><span class="line">xfs_growfs /var</span><br><span class="line">mount -o remount,size=6G /run</span><br></pre></td></tr></table></figure></p>
<p>这个脚本来自 assisted installer 的 bootstrap 过程. 其中 <code>RPMS_IMAGE</code> 变量的 sha256 值对应 <code>okd-rpms</code> 镜像, 可以通过下面命令得到 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">oc adm release info quay.io/$RELEASE_PATH | grep okd-rpms</span><br></pre></td></tr></table></figure></p>
<p>用于生成整合 ignition 的 butane 格式清单文件 <code>customize-embedded.bu</code> <figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">variant:</span> <span class="string">fcos</span></span><br><span class="line"><span class="attr">version:</span> <span class="number">1.4</span><span class="number">.0</span></span><br><span class="line"><span class="attr">ignition:</span></span><br><span class="line">  <span class="attr">config:</span></span><br><span class="line">    <span class="attr">merge:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">local:</span> <span class="string">sno.ign</span></span><br><span class="line"><span class="attr">storage:</span></span><br><span class="line">  <span class="attr">files:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">path:</span> <span class="string">/opt/openshift/openshift/98_openshift-machineconfig_98-master-config-sno.yaml</span></span><br><span class="line">    <span class="attr">mode:</span> <span class="number">0644</span></span><br><span class="line">    <span class="attr">overwrite:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">contents:</span></span><br><span class="line">      <span class="attr">local:</span> <span class="number">98</span><span class="string">-machine-config-maser-sno.yaml</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">path:</span> <span class="string">/usr/local/bin/okd-binaries.sh</span></span><br><span class="line">    <span class="attr">mode:</span> <span class="number">0755</span></span><br><span class="line">    <span class="attr">overwrite:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">contents:</span></span><br><span class="line">      <span class="attr">local:</span> <span class="string">okd-binaries.sh</span></span><br><span class="line"><span class="attr">systemd:</span></span><br><span class="line">  <span class="attr">units:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">contents:</span> <span class="string">|</span></span><br><span class="line"><span class="string">      [Service]</span></span><br><span class="line"><span class="string">      Type=oneshot</span></span><br><span class="line"><span class="string">      ExecStart=/usr/local/bin/okd-binaries.sh</span></span><br><span class="line"><span class="string"></span></span><br><span class="line">      [<span class="string">Unit</span>]</span><br><span class="line">      <span class="string">Wants=network-online.target</span></span><br><span class="line">      <span class="string">After=network-online.target</span></span><br><span class="line"></span><br><span class="line">      [<span class="string">Install</span>]</span><br><span class="line">      <span class="string">WantedBy=multi-user.target</span></span><br><span class="line">    <span class="attr">enabled:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">okd-overlay.service</span></span><br></pre></td></tr></table></figure></p>
<p>使用下面的命令生成整合的 ignition 文件 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">butane -d . customize-embedded.bu -o boot.ign</span><br></pre></td></tr></table></figure></p>
<h3 id="生成定制镜像">生成定制镜像</h3>
<p>使用 <code>coreos-installer</code> 命令将 ignition 文件内嵌入 ISO, 同时通过内核参数设置 bootstrap 过程的静态网络配置. 这个网络配置会被复制到最终的 machine config 中 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">IP=192.168.11.71</span><br><span class="line">GATEWAY=192.168.11.254</span><br><span class="line">NETMASK=255.255.255.0</span><br><span class="line">HOSTNAME=hdc-srv-1.int.vopsdev.com</span><br><span class="line">DNS=192.168.11.10</span><br><span class="line"></span><br><span class="line">./coreos-installer iso kargs modify -a <span class="string">&quot;rd.neednet=1 ip=<span class="variable">$&#123;IP&#125;</span>::<span class="variable">$&#123;GATEWAY&#125;</span>:<span class="variable">$&#123;NETMASK&#125;</span>:<span class="variable">$&#123;HOSTNAME&#125;</span>::none nameserver=<span class="variable">$&#123;DNS&#125;</span>&quot;</span> fedora-coreos-live.iso</span><br><span class="line">./coreos-installer iso ignition embed -<span class="keyword">fi</span> boot.ign fedora-coreos-live.iso</span><br></pre></td></tr></table></figure></p>
<h3 id="完成安装">完成安装</h3>
<p>将 ISO 挂载到虚拟机并启动, 登入后可以观察其 bootstrap 的过程 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">journalctl -f -u okd-overlay.service -u release-image.service -u bootkube.service -u release-image-pivot.service -u install-to-disk.service</span><br></pre></td></tr></table></figure></p>
<p>中间会经过几次重启, 整个过程大约半小时, 最终可以成功安装 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">./openshift-install --dir=assets wait-for bootstrap-complete</span><br><span class="line">INFO Waiting up to 20m0s (until 10:46PM) for the Kubernetes API at https://api.cls-sno-1.vopsdev.com:6443...</span><br><span class="line">INFO API v1.25.0-2786+eab9cc98fe4c00-dirty up</span><br><span class="line">INFO Waiting up to 30m0s (until 10:56PM) for bootstrapping to complete...</span><br><span class="line">INFO It is now safe to remove the bootstrap resources</span><br><span class="line">INFO Time elapsed: 0s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">./openshift-install --dir=assets wait-for install-complete</span><br><span class="line">INFO Waiting up to 40m0s (until 11:06PM) for the cluster at https://api.cls-sno-1.vopsdev.com:6443 to initialize...</span><br><span class="line">INFO Checking to see if there is a route at openshift-console/console...</span><br><span class="line">INFO Install complete!</span><br><span class="line">INFO To access the cluster as the system:admin user when using &#x27;oc&#x27;, run &#x27;export KUBECONFIG=/home/admin/workspace/okd-sn-4.12/assets/auth/kubeconfig&#x27;</span><br><span class="line">INFO Access the OpenShift web-console here: https://console-openshift-console.apps.cls-sno-1.vopsdev.com</span><br><span class="line">INFO Login to the console with user: &quot;kubeadmin&quot;, and password: &quot;xxxxx-xxxxx-xxxxx-xxxxx&quot;</span><br><span class="line">INFO Time elapsed: 0s</span><br></pre></td></tr></table></figure></p>
<p><img src="manual-sno-done.PNG" /></p>
<h2 id="参考资料">参考资料</h2>
<ul>
<li><a href="https://github.com/coreos/fedora-coreos-tracker/issues/1344" class="uri">https://github.com/coreos/fedora-coreos-tracker/issues/1344</a></li>
<li><a href="https://github.com/okd-project/okd/issues/1607" class="uri">https://github.com/okd-project/okd/issues/1607</a></li>
<li><a href="https://upstreamwithoutapaddle.com/blog%20post/2023/05/21/Pull-Youself-Up-By-Your-Bootstraps.html" class="uri">https://upstreamwithoutapaddle.com/blog%20post/2023/05/21/Pull-Youself-Up-By-Your-Bootstraps.html</a></li>
<li><a href="https://coreos.github.io/butane/" class="uri">https://coreos.github.io/butane/</a></li>
</ul>
]]></content>
      <categories>
        <category>RedHat</category>
        <category>Openshift</category>
      </categories>
      <tags>
        <tag>Openshift</tag>
        <tag>OKD4</tag>
        <tag>SNO</tag>
      </tags>
  </entry>
  <entry>
    <title>在 OKD4 上体验 StackRox</title>
    <url>/4142919773/</url>
    <content><![CDATA[<p>在 OKD 4.8 上部署并体验 StackRox (Red Hat Advanced Cluster Security for Kubernetes) 安全合规平台 <span id="more"></span></p>
<h2 id="简介">简介</h2>
<p>StackRox 是红帽收购的一个 K8S 安全合规平台，提供了丰富的 DevSecOps 功能:</p>
<ul>
<li>镜像/容器漏洞扫描</li>
<li>配置最佳实践建议</li>
<li>风险分析</li>
<li>网络隔离/访问控制分析和管理</li>
<li>合规性检查和报表</li>
<li>威胁主动响应</li>
</ul>
<p>该产品被重新命名为 Red Hat Advanced Cluster Security for Kubernetes. 红帽声称会将该项目开源出来，然而目前看来应该还没有完成. 希望能早日看到这个项目.</p>
<p>StackRox 目前支持的 K8S 平台包括 OCP 3.11, OCP 4.X, EKS, GKE, AKS. 而其支持的节点操作系统包括 Amazon Linux, CentOS, Container-Optimized OS from Google, RHCOS, Debian, RHEL, Ubuntu.</p>
<p>很明显 OKD4 及其底层的 Fedora CoreOS 并不在支持列表中, 这里仅仅是做一下可行性探索.</p>
<h2 id="安装控制平面">安装控制平面</h2>
<p>先简单说一下 StackRox 的架构: 其控制平面组件包含 central + scanner, 需要运行在一个 K8S 管理集群中. 而受控集群上则需要运行集群级别组件 sensor + admission controller，和节点级别组件 collector.</p>
<p>这里使用 Helm Chart 的方式安装控制平面.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">helm repo add rhacs https://mirror.openshift.com/pub/rhacs/charts/</span><br><span class="line">helm repo list</span><br></pre></td></tr></table></figure>
<p>可以发现这里有两个 Charts：rhacs/central-services 和 rhacs/secured-cluster-services 分别对应了控制平面组件和受控集群组件.</p>
<p>先 <code>helm show values rhacs/central-services</code> 看一下默认参数. 根据自己的环境手工编写 <code>values.yaml</code></p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">imagePullSecrets:</span></span><br><span class="line">  <span class="attr">useFromDefaultServiceAccount:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">env:</span></span><br><span class="line">  <span class="attr">openshift:</span> <span class="number">4</span></span><br><span class="line"><span class="attr">central:</span></span><br><span class="line">  <span class="attr">disableTelemetry:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">exposeMonitoring:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">persistence:</span></span><br><span class="line">    <span class="attr">persistentVolumeClaim:</span></span><br><span class="line">      <span class="attr">claimName:</span> <span class="string">stackrox-central-pvc</span></span><br><span class="line">      <span class="attr">createClaim:</span> <span class="literal">true</span></span><br><span class="line">      <span class="attr">size:</span> <span class="number">100</span></span><br><span class="line">  <span class="attr">exposure:</span></span><br><span class="line">    <span class="attr">route:</span></span><br><span class="line">      <span class="attr">enabled:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure>
<p>执行安装</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">helm install -n stackrox --create-namespace stackrox-central-services rhacs/central-services -f values.yaml</span><br></pre></td></tr></table></figure>
<p>输出会给出自动产生的管理员密码，route 信息，并提示保存自动产生的 values:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">oc -n stackrox get secret stackrox-generated-XXXXX -o go-template=<span class="string">&#x27;&#123;&#123; index .data &quot;generated-values.yaml&quot; &#125;&#125;&#x27;</span> | base64 -d &gt; generated-values.yaml</span><br></pre></td></tr></table></figure>
<p>到此为止，控制平面 Web 界面已经可以访问. 但是为了避免自签名证书带来的麻烦，先将其替换为 letsencrypt 证书.</p>
<h2 id="替换控制平面证书">替换控制平面证书</h2>
<p>我的 OKD4 集群都使用 cert-manager 来请求并管理 letsencrypt 证书，并配置了名为 letsencrypt 的 cluster issuer</p>
<p>请求证书, 保存到一个名为 <code>central-default-tls-cert</code> 的 tls 类型 secret, 并重启 central pod 即可:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">oc delete secret central-default-tls-cert || <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">oc apply -f -&lt;&lt;<span class="string">EOF</span></span><br><span class="line"><span class="string">apiVersion: cert-manager.io/v1</span></span><br><span class="line"><span class="string">kind: Certificate</span></span><br><span class="line"><span class="string">metadata:</span></span><br><span class="line"><span class="string">  name: central-default-tls-cert</span></span><br><span class="line"><span class="string">  namespace: stackrox</span></span><br><span class="line"><span class="string">spec:</span></span><br><span class="line"><span class="string">  secretName: central-default-tls-cert</span></span><br><span class="line"><span class="string">  issuerRef:</span></span><br><span class="line"><span class="string">    name: letsencrypt</span></span><br><span class="line"><span class="string">    kind: ClusterIssuer</span></span><br><span class="line"><span class="string">  dnsNames: [&quot;central-stackrox.apps.atlas-iet4.vopsdev.com&quot;]</span></span><br><span class="line"><span class="string">EOF</span></span><br><span class="line"></span><br><span class="line">oc delete $(oc get pod -l app=central -o name)</span><br></pre></td></tr></table></figure>
<p>等 central pod 起来后就可以正常访问 web portal 了</p>
<p><img src="stackrox-web-portal.png" /></p>
<h2 id="加入受控集群">加入受控集群</h2>
<p>登入控制平面 web portal 创建 init bundle 用于受控集群和控制平面之间的认证: Platform Configuration, Integrations, Authentication Tokens, Cluster Init Bundle</p>
<p><img src="cluster-init-bundle.png" /></p>
<p>点击 Generate Bundle, 提供一个命名, 然后下载 Helm Values</p>
<p><img src="download-helm-values.png" /></p>
<p>你的所有受控集群可以使用这同一个 init bundle. 然后就可以使用 helm 来安装受控集群的组件了</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ENDPOINT=central-stackrox.apps.atlas-iet4.vopsdev.com:443</span><br><span class="line"></span><br><span class="line"><span class="comment"># add atlas-set4 cluster</span></span><br><span class="line">helm install -n stackrox --create-namespace \</span><br><span class="line">  stackrox-secured-cluster-services rhacs/secured-cluster-services \</span><br><span class="line">  -f init-bundle.yaml \</span><br><span class="line">  --<span class="built_in">set</span> clusterName=atlas-set4 \</span><br><span class="line">  --<span class="built_in">set</span> centralEndpoint=<span class="variable">$ENDPOINT</span> \</span><br><span class="line">  --<span class="built_in">set</span> collector.collectionMethod=EBPF</span><br><span class="line"></span><br><span class="line"><span class="comment"># add other clusters ...</span></span><br></pre></td></tr></table></figure>
<div class="note info"><p>collector 默认的收集方式是 <code>KERNEL_MODULE</code>. 然而 Fedore CoreOS 不在支持列表中, 因此 stackrox 并不提供对应的内核模块, 此时 collector pod 就会不停的崩溃. 为此可以将 <code>collector.collectionMethod</code> 设置为 <code>EBPF</code> 模式.</p>
</div>
<p>等各个组件都正常起来后, 就可以在 web portal 上看到受管理的集群了: Platform Configuration, Clusters</p>
<p><img src="managed-clusters.png" /></p>
<h2 id="体验安全策略">体验安全策略</h2>
<p>StackRox 的核心在其安全策略，这里创建几个简单的安全策略来测试其静态检查和动态检查的能力.</p>
<h3 id="静态漏洞检查">静态漏洞检查</h3>
<p>创建一个安全策略阻止含有 CVE-2021-27219 安全漏洞的镜像的部署: Platform Configuration, System Policies, +NEW POLICY</p>
<p>Policy Summary 部分如图设置</p>
<p><img src="cve-policy-summary-1.png" /></p>
<p>这里策略针对的生命周期阶段设置为 Deploy</p>
<p><img src="cve-policy-summary-2.png" /></p>
<p>这里对策略的作用域进行限制: 该规则被限定到集群 <code>atlas-set4</code> 的项目 <code>smoke-test</code> 下.</p>
<p>Policy Criteria 部分: 把 Image Contents 下的 CVE 拉到左边的 Policy Section 1 下, 并指定其值为 CVE-2021-27219</p>
<p><img src="cve-policy-criteria.png" /></p>
<p>部署阶段的强制行为: 禁止不合规部署</p>
<p><img src="cve-enforcement.png" /></p>
<p>保存生效以后, 到 <code>atlas-set4</code> 集群的 <code>smoke-test</code> 命名空间下进行测试:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">oc project smoke-test</span><br><span class="line"></span><br><span class="line">oc apply -f -&lt;&lt;<span class="string">EOF</span></span><br><span class="line"><span class="string">apiVersion: apps/v1</span></span><br><span class="line"><span class="string">kind: Deployment</span></span><br><span class="line"><span class="string">metadata:</span></span><br><span class="line"><span class="string">  labels:</span></span><br><span class="line"><span class="string">    app: test-deploy</span></span><br><span class="line"><span class="string">  name: test-deploy</span></span><br><span class="line"><span class="string">  namespace: smoke-test</span></span><br><span class="line"><span class="string">spec:</span></span><br><span class="line"><span class="string">  replicas: 1</span></span><br><span class="line"><span class="string">  selector:</span></span><br><span class="line"><span class="string">    matchLabels:</span></span><br><span class="line"><span class="string">      app: test-deploy</span></span><br><span class="line"><span class="string">  template:</span></span><br><span class="line"><span class="string">    metadata:</span></span><br><span class="line"><span class="string">      labels:</span></span><br><span class="line"><span class="string">        app: test-deploy</span></span><br><span class="line"><span class="string">    spec:</span></span><br><span class="line"><span class="string">      containers:</span></span><br><span class="line"><span class="string">      - image: docker.io/centos:7.9.2009</span></span><br><span class="line"><span class="string">        name: test-deploy</span></span><br><span class="line"><span class="string">        stdin: true</span></span><br><span class="line"><span class="string">        tty: true</span></span><br><span class="line"><span class="string">EOF</span></span><br><span class="line"></span><br><span class="line">oc get all -l app=test-deploy</span><br></pre></td></tr></table></figure>
<p>Dockerhub 上的镜像 centos:7.9.2009 含有 CVE-2021-27219 漏洞. 此时可以发现副本数量被强制设置为 0</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">NAME                          READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.apps/test-deploy   0/0     0            0           47s</span><br><span class="line"></span><br><span class="line">NAME                                     DESIRED   CURRENT   READY   AGE</span><br><span class="line">replicaset.apps/test-deploy-69bb54cbd4   0         0         0       47s</span><br></pre></td></tr></table></figure>
<h3 id="运行时行为检查">运行时行为检查</h3>
<p>禁止容器在运行时执行有风险/可疑的进程. 这里创建一个安全策略, 禁止容器使用 <code>useradd</code>, <code>usermod</code>, 和 <code>adduser</code> 命令</p>
<p>Policy Summary 部分如图设置</p>
<p><img src="runtime-policy-summary-1.png" /></p>
<p>这里策略针对的生命周期阶段设置为 Runtime. Event Sources 设置为 Deployment.</p>
<p><img src="runtime-policy-summary-2.png" /></p>
<p>同样这里将该规则限定到集群 <code>atlas-set4</code> 的项目 <code>smoke-test</code> 下</p>
<p>Policy Criteria 部分: 把 Process Activity 下的 Process Name 拉到左边的 Policy Section 1 下, 并指定其值为 <code>useradd|usermod|adduser</code></p>
<p><img src="runtime-policy-criteria.png" /></p>
<p>运行阶段强制行为: 杀死不合规的 pod</p>
<p><img src="runtime-enforcement.png" /></p>
<p>保存生效以后, 到 <code>atlas-set4</code> 集群的 <code>smoke-test</code> 命名空间下进行测试 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">oc apply -f -&lt;&lt;<span class="string">EOF</span></span><br><span class="line"><span class="string">apiVersion: apps/v1</span></span><br><span class="line"><span class="string">kind: Deployment</span></span><br><span class="line"><span class="string">metadata:</span></span><br><span class="line"><span class="string">  labels:</span></span><br><span class="line"><span class="string">    app: test-runtime</span></span><br><span class="line"><span class="string">  name: test-runtime</span></span><br><span class="line"><span class="string">  namespace: smoke-test</span></span><br><span class="line"><span class="string">spec:</span></span><br><span class="line"><span class="string">  replicas: 1</span></span><br><span class="line"><span class="string">  selector:</span></span><br><span class="line"><span class="string">    matchLabels:</span></span><br><span class="line"><span class="string">      app: test-runtime</span></span><br><span class="line"><span class="string">  template:</span></span><br><span class="line"><span class="string">    metadata:</span></span><br><span class="line"><span class="string">      labels:</span></span><br><span class="line"><span class="string">        app: test-runtime</span></span><br><span class="line"><span class="string">    spec:</span></span><br><span class="line"><span class="string">      containers:</span></span><br><span class="line"><span class="string">      - image: docker.io/centos:7</span></span><br><span class="line"><span class="string">        name: test-runtime</span></span><br><span class="line"><span class="string">        command:</span></span><br><span class="line"><span class="string">        - /bin/bash</span></span><br><span class="line"><span class="string">        - -c</span></span><br><span class="line"><span class="string">        - sleep 60; useradd alice; while true; do sleep 10; done</span></span><br><span class="line"><span class="string">EOF</span></span><br><span class="line"></span><br><span class="line">oc get pod -l app=test-runtime -w</span><br></pre></td></tr></table></figure></p>
<p>这个容器在启动之后的 60s 后会执行 <code>useradd</code> 命令，此时会因为违规而被杀死 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">NAME                            READY   STATUS    RESTARTS   AGE</span><br><span class="line">test-runtime-556bd889d7-4vvsg   1/1     Running   0          7s</span><br><span class="line">test-runtime-556bd889d7-4vvsg   1/1     Terminating   0          63s</span><br><span class="line">test-runtime-556bd889d7-4vvsg   1/1     Terminating   0          63s</span><br><span class="line">test-runtime-556bd889d7-rgjgq   0/1     Pending       0          0s</span><br><span class="line">test-runtime-556bd889d7-rgjgq   0/1     Pending       0          0s</span><br><span class="line">test-runtime-556bd889d7-rgjgq   0/1     Pending       0          0s</span><br><span class="line">test-runtime-556bd889d7-rgjgq   0/1     ContainerCreating   0          0s</span><br><span class="line">test-runtime-556bd889d7-rgjgq   0/1     ContainerCreating   0          2s</span><br><span class="line">test-runtime-556bd889d7-rgjgq   1/1     Running             0          3s</span><br></pre></td></tr></table></figure></p>
<p>在 web portal 的 Violations 处也可以看到这个违规的信息 <img src="violation.png" /></p>
<h2 id="参考">参考</h2>
<ul>
<li><a href="https://docs.openshift.com/acs/3.66/welcome/index.html">RHACS 官方文档</a></li>
</ul>
]]></content>
      <categories>
        <category>RedHat</category>
        <category>Openshift</category>
      </categories>
      <tags>
        <tag>Openshift</tag>
        <tag>OKD4</tag>
        <tag>StackRox</tag>
        <tag>Red Hat Advanced Cluster Security for Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>定制 Openshift 3.11 监控系统 Cluster Monitoring Operator</title>
    <url>/1412485228/</url>
    <content><![CDATA[<p>本文介绍如何对 Openshift (OCP/OKD) 3.11 的监控系统 cluster monitoring operator 进行简单定制, 扩展平台监控的范围. <span id="more"></span></p>
<h2 id="cluster-monitoring-operator-介绍">Cluster Monitoring Operator 介绍</h2>
<p>Openshift 3.11 以及后续的 4.x 平台都使用预定义配置的 cluster monitoring operator 来做平台监控. Cluster monitoring operator 管理 prometheus operator, grafana, kube-state-metrics 和 node-exporter. 而 prometheus operator 再负责管理 prometheus 和 alertmanager. 各组件之间的关系如下图所示:</p>
<p><img src="cluster-monitoring-operator.png" /></p>
<p>通过检查 prometheus 的配置可以发现其 scrape 对象包括</p>
<ul>
<li>prometheus</li>
<li>prometheus operator</li>
<li>cluster monitoring operator</li>
<li>alertmanager</li>
<li>kubernete apiserver</li>
<li>kubelet</li>
<li>kube-controller</li>
<li>kube-state-metrics</li>
<li>node-exporter</li>
<li>etcd (可选择启用)</li>
</ul>
<p>对于一个中小型 Openshift 集群的平台层面 (不包含应用特定 metrics), 用这些对象的 metrics 来做平台监控其实也差不多够用了. 然而 Openshift 监控套件的预定义配置并没有覆盖非系统级的 namespace.</p>
<p>举个例子, 我的应用 pod 跑在 myproj project/namespace 下. 虽然 kube-state-metrics 会收集 myproj 名字空间下资源 (deployment/statefulset/pod) 的状态信息, 但是预定义的 alert rules 只选取 <code>openshift-.\*|kube-.\*|default|logging</code> 这些 namespace 的 metrics 来进行评估. 导致的结果是: 虽然收集了应用 namespace 下的资源状态信息, 却无法实现告警.</p>
<p>遗憾的是 Openshift 并不支持用户扩展其平台监控套装的监控范围. 文档原文如下</p>
<blockquote>
<p>Explicitly unsupported cases include:</p>
<ul>
<li>Creating additional ServiceMonitor objects in the openshift-monitoring namespace, thereby extending the targets the cluster monitoring Prometheus instance scrapes. This can cause collisions and load differences that cannot be accounted for, therefore the Prometheus setup can be unstable.</li>
<li>Creating additional ConfigMap objects, that cause the cluster monitoring Prometheus instance to include additional alerting and recording rules. Note that this behavior is known to cause a breaking behavior if applied, as Prometheus 2.0 will ship with a new rule file syntax.</li>
</ul>
</blockquote>
<p>如果想让监控覆盖到用户的 namespace, RedHat 给出的建议是另外单独部署 prometheus 套件. 对于大型集群, 这可能是一个合理的建议. 但是对于一个小型集群, 如果本身就只有几个用户级别的 namespace, 总共十几或几十个 pod, 你让我再单独来一套 prometheus? 实在是很蛋疼.</p>
<p>如果你使用的是订阅版本的 Openshift Container Platform, 为了避免维保/技术支持的麻烦, 请遵循 RedHat 的建议. 如果你使用的是社区版本的 OKD, 可以继续往下看: 如何改造 cluster monitoring operator 来满足定制化需求.</p>
<h2 id="定制化方案">定制化方案</h2>
<h3 id="需求">需求</h3>
<p>下面这些应该是非常基本的定制化需求</p>
<ul>
<li>调整部分默认告警规则的覆盖范围. 例如 KubePodNotReady, 除了覆盖系统级别的 namespace, 还需要覆盖所有应用 namespace</li>
<li>调整部分默认告警规则的触发状态持续时间, 像 KubePodCrashLooping, KubeNodeNotReady 默认设置状态持续 1h 才会触发告警, 黄花菜都凉了吧</li>
<li>调整默认 target scrape interval. 例如调整 kube-state-metrics 的刮取间隔为 1m</li>
<li>添加自定义告警规则</li>
<li>添加自定义 service monitor 来包含用户应用</li>
</ul>
<p>再次吐槽 RedHat 的产品, 经常弄一些跟玩具一样的东西出来, 性能/稳定性/扩展性/可定制性上有诸多限制. 类似的还有 Openshift 平台的 EFK 日志系统, Openshift 3 的 IPSec 方案 (经过生产验证了吗? 就敢往文档里面写? <a href="https://github.com/libreswan/libreswan/issues/185">#185</a>) 等, 我就不一一点了</p>
<h3 id="添加自定义配置">添加自定义配置</h3>
<p>上面需求中的第四和第五项属于添加自定义配置, 比较容易实现.</p>
<p>Prometheus 规则和监控 target 配置都是通过自定义资源 (Custom Resource) 来管理</p>
<ul>
<li>prometheusrules.monitoring.coreos.com 类型资源 针对 prometheus rules</li>
<li>servicemonitors.monitoring.coreos.com 类型资源针对 target 配置</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">oc get prometheusrules.monitoring.coreos.com</span><br><span class="line">NAME                   AGE</span><br><span class="line">prometheus-k8s-rules   1d</span><br><span class="line"></span><br><span class="line">oc get servicemonitors.monitoring.coreos.com</span><br><span class="line">NAME                          AGE</span><br><span class="line">alertmanager                  1d</span><br><span class="line">cluster-monitoring-operator   1d</span><br><span class="line">kube-apiserver                1d</span><br><span class="line">kube-controllers              1d</span><br><span class="line">kube-state-metrics            1d</span><br><span class="line">kubelet                       1d</span><br><span class="line">node-exporter                 1d</span><br><span class="line">prometheus                    1d</span><br><span class="line">prometheus-operator           1d</span><br></pre></td></tr></table></figure>
<p>如果需要添加自定义规则和自定义监控目标, 创建相应的 CR 即可. 不过要注意, 不要试图修改默认 CR 来实现自定义配置, 具体原因后面会分析.</p>
<h3 id="修改默认配置">修改默认配置</h3>
<p>需求中的 1-3 项属于修改默认配置. 如果尝试修改默认 CR, 你会发现你的更改会被 cluster monitoring operator 自动重置. 一个很自然的想法是能不能重新构建 cluster monitoring operator 镜像, 覆盖掉这些默认 CR 的配置文件来实现需求?</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 获取 cluster monitoring operator 源代码</span></span><br><span class="line">go get -u github.com/openshift/cluster-monitoring-operator/...</span><br><span class="line"><span class="built_in">cd</span> <span class="variable">$GOPATH</span>/src/github.com/openshift/cluster-monitoring-operator/</span><br><span class="line">git checkout release-3.11</span><br></pre></td></tr></table></figure>
<p>简单检查一下 cluster monitoring operator 的源码就可以发现其实并不可行, 原因是这些预定义配置文件会被 go-bindata 用来生成 go 代码, 最终会成为 operator binary 的一部分. 即这些配置文件 "固化" 在二进制文件中了. 我猜想这也是 RedHat 不提供针对 cluster monitoring operator 定制化配置的原因之一.</p>
<p>查看<code>Makefile</code>可以大致了解: <code>$&#123;ASSERTS&#125;</code> 这个目标会执行 <code>./hack/build-jsonnet.sh</code>来产生所有预定义配置, 而 <code>pkg/manifests/bindata.go</code> 这个目标会利用预定义配置生成 go 代码. 因此只需要修改 <code>./hack/build-jsonnet.sh</code> 脚本替换预定义配置即可实现需求.</p>
<p>在<code>hack</code>目录下建立路径来存放需要定制的配置文件. 把预定义文件复制过来, 然后按照需要修改 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mkdir -p hack/customization/assets/&#123;prometheus-k8s,kube-state-metrics&#125;</span><br><span class="line">cp assets/prometheus-k8s/rules.yaml hack/customization/assets/prometheus-k8s/rules.yaml</span><br><span class="line">cp assets/kube-state-metrics/service-monitor.yaml hack/customization/assets/kube-state-metrics/service-monitor.yaml</span><br></pre></td></tr></table></figure></p>
<p>需求三修改 kube-state-metrics target 的刮取时间间隔需要修改<code>service-monitor.yaml</code>的 interval 字段.</p>
<p>需求一和二修改默认告警规则触发的状态持续时间和 namespace 覆盖范围需要修改<code>rules.yaml</code>中 alert rule 的定义. 例如我调整了下面告警的状态持续时间, 并且将原来针对系统级 namepace 的过滤条件删除:</p>
<ul>
<li>kube-state-metrics job 相关
<ul>
<li>KubePodCrashLooping</li>
<li>KubePodNotReady</li>
<li>KubeDeploymentGenerationMismatch</li>
<li>KubeDeploymentReplicasMismatch</li>
<li>KubeStatefulSetReplicasMismatch</li>
<li>KubeStatefulSetGenerationMismatch</li>
<li>KubeDaemonSetRolloutStuck</li>
<li>KubeDaemonSetNotScheduled</li>
<li>KubeDaemonSetMisScheduled</li>
<li>KubeCPUOvercommit</li>
<li>KubeMemOvercommit</li>
<li>KubeQuotaExceeded</li>
<li>KubeNodeNotReady</li>
</ul></li>
<li>kubelet job 相关
<ul>
<li>KubeletDown</li>
</ul></li>
<li>node-export job 相关
<ul>
<li>NodeExportDown</li>
</ul></li>
<li>其他
<ul>
<li>TargetDown</li>
</ul></li>
</ul>
<p>然后修改<code>hack/build-jsonnet.sh</code>文件, 添加脚本代码用自定义配置替换默认配置</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># hack: my customized files</span></span><br><span class="line">cat hack/customization/assets/prometheus-k8s/rules.yaml &gt; assets/prometheus-k8s/rules.yaml</span><br><span class="line">cat hack/customization/assets/kube-state-metrics/service-monitor.yaml &gt; assets/kube-state-metrics/service-monitor.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># These manifests are generated by kube-prmoetheus, but are not necessary in</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<h3 id="构建定制化镜像">构建定制化镜像</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># build operator binary</span></span><br><span class="line">make generate</span><br><span class="line">make crossbuild</span><br><span class="line"></span><br><span class="line"><span class="comment"># build image and push</span></span><br><span class="line">docker build -t quay.svc.vopsdev.com/openshift3/ose-cluster-monitoring-operator:v3.11-rev1 .</span><br><span class="line">docker push quay.svc.vopsdev.com/openshift3/ose-cluster-monitoring-operator:v3.11-rev1</span><br></pre></td></tr></table></figure>
<h2 id="测试">测试</h2>
<p>在 inventory 文件中通过<code>openshift_cluster_monitoring_operator_image</code>参数指定定制镜像 quay.svc.vopsdev.com/openshift3/ose-cluster-monitoring-operator:v3.11-rev1. 重新部署集群监控套装. 部署完成后, 创建 CR 添加自定义告警规则和监控对象 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ansible-playbook -i inventory playbooks/openshift-monitoring/config.yml</span><br><span class="line"></span><br><span class="line">oc apply -f customized-servicemonitor.yaml -n openshift-monitoring</span><br><span class="line">oc apply -f customized-rules.yaml -n openshift-monitoring</span><br></pre></td></tr></table></figure></p>
<p>登录 prometheus 界面确认自定义配置已经生效</p>
<ol type="1">
<li><p>调整 kube-state-metrics 的 metrics 收集周期: <img src="kube-state-metrics-interval.png" /></p></li>
<li><p>扩展默认告警规则到所有 namespace 并调整持续时间: <img src="alert-rule.png" /></p></li>
<li><p>自定义 target 配置: <img src="custom-target.png" /></p></li>
<li><p>自定义告警规则 <img src="custom-alert-rule.png" /></p></li>
</ol>
]]></content>
      <categories>
        <category>RedHat</category>
        <category>Openshift</category>
      </categories>
      <tags>
        <tag>Openshift</tag>
        <tag>RedHat</tag>
      </tags>
  </entry>
  <entry>
    <title>在 OKD4 上部署 Falco 套件</title>
    <url>/1253087136/</url>
    <content><![CDATA[<p>Falco 是一个 CNCF 的安全项目, 可用于 kubernetes 环境下的容器运行时安全监控. Falco 本身的部署，无论是 Linux host 还是 kubernetes 都比较容易. 但是 OKD4 环境的一些特殊性导致 Falco 在其上的部署会出现一系列问题，这里做一个简单的填坑记录.</p>
<span id="more"></span>
<h2 id="falco-在-okd4-上的部署问题">Falco 在 OKD4 上的部署问题</h2>
<p>Falco 监视系统调用的能力来自其 driver 组件 (可以是内核模块或者 eBPF probe). Falco 启动前会检查与当前内核相对应的 driver 是否存在, 如果不存在则先尝试去 <a href="https://download.falco.org/">https://download.falco.org</a> 下载 prebuilt driver. 如果没有提供 prebuilt driver 则尝试自行编译. 具体逻辑可以查看 <code>/usr/bin/falco-driver-loader</code> 脚本.</p>
<p>自行编译需要 host 上安装了对应内核版本的 <code>kernel-devel</code> 包. 对于普通的 K8S 环境, 这并不是什么问题, 在 host 上用包管理器装上即可. 但是 Openshift4 / OKD4 底层的 RHCOS/FCOS 其受控的不可变性并不允许用户随意安装软件包.</p>
<p>如果是 Openshift4 环境, 你还可以使用自定义 <code>MachineConfig</code> 来添加额外软件包. 具体方法参见 <a href="https://github.com/falcosecurity/falco/issues/1505">https://github.com/falcosecurity/falco/issues/1505</a>. 但是这个方法对 OKD4 并不适用: FCOS 官方使用的是 fedora yum 源, 而 fedora yum 源并不保存软件包的历史版本, 通过这种方法装上的内核开发包和实际 FCOS 上运行的内核版本并不一致.</p>
<p>当然你可以尝试自己维护一个 yum 源: 从 koji 构建系统中把和 FCOS 内核版本一致的开发包下载下来做成 yum 源, 然后在 <code>MachineConfig</code> 里使用你自己的 yum 源来安装版本一致的 <code>kernel-devel</code>. 为了避免修改 FCOS 导致的集群升级的不确定性, 我并不倾向于在 FCOS 上安装软件包, 因此没有尝试这种方法.</p>
<p>这里采用的方法是在外部构建系统中编译和 FCOS 内核版本一致的 Falco driver, 保存在私有的 driver repo 中, 配置 Falco 使用这个私有的 driver repo.</p>
<h2 id="falco-在-okd4-上的部署步骤">Falco 在 OKD4 上的部署步骤</h2>
<p>这里编译和使用的是 eBPF probe driver, 总体而言还是比较顺利. 整体步骤为</p>
<ul>
<li>搭建一个 Fedora 34 的虚拟机作为构建环境 (使用和 FCOS 对应的 fedora 版本)</li>
<li>从 <a href="https://kojipkgs.fedoraproject.org">https://kojipkgs.fedoraproject.org</a> 下载内核对应版本的开发包, 并安装到构建环境</li>
<li>编译 Falco 的 eBPF probe driver</li>
<li>将 eBPF probe driver 上传到一个 s3 bucket 供外部下载</li>
<li>编写清单文件</li>
<li>部署到 OKD4</li>
</ul>
<h3 id="编译-ebpf-probe">编译 eBPF probe</h3>
<p>OKD 4.8.0-0.okd-2021-11-14-052418 对应的内核版本是 5.14.14-200. 因此从 koji 下载下面的软件包并安装到构建环境</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kernel-core-5.14.14-200.fc34.x86_64</span><br><span class="line">kernel-modules-5.14.14-200.fc34.x86_64</span><br><span class="line">kernel-5.14.14-200.fc34.x86_64</span><br><span class="line">kernel-devel-5.14.14-200.fc34.x86_64</span><br></pre></td></tr></table></figure>
<p>在构建环境编译 Falco eBPF probe driver:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum -y install gcc gcc-c++ git make cmake autoconf automake pkg-config patch ncurses-devel libtool elfutils-libelf-devel diffutils <span class="built_in">which</span> jq-devel clang llvm c-ares-devel gprbuild grpc-devel grpc-plugins grpc-cpp grpc-devel yaml-cpp-devel</span><br><span class="line"></span><br><span class="line">git <span class="built_in">clone</span> https://github.com/falcosecurity/falco.git</span><br><span class="line"><span class="built_in">cd</span> falco</span><br><span class="line">git checkout 0.30.0</span><br><span class="line">mkdir build</span><br><span class="line"><span class="built_in">cd</span> build</span><br><span class="line">cmake -DBUILD_BPF=True ..</span><br><span class="line">make bpf</span><br></pre></td></tr></table></figure>
<p>最终产生的二进制文件为 <code>driver/bpf/probe.o</code>. 将其重命名为 <code>falco_fedora_5.14.14-200.fc34.x86_64_1.o</code></p>
<p>按照指定的路径格式将编译得到的 driver 文件上传到文件服务器或者 s3 bucket 中, 例如 https://static.example.com/driver/3aa7a83bf7b9e6229a3824e3fd1f4452d1e95cb4/falco_fedora_5.14.14-200.fc34.x86_64_1.o. 注意这里 3aa7a83bf7b9e6229a3824e3fd1f4452d1e95cb4 是 Faclo 0.30.0 的 driver version, 你可以从 release notes 中找到. Falco 下载 prebuilt driver 时会去对应的 driver version 路径下按照预定义的文件名下载.</p>
<h3 id="部署-falco-套件---falco-falcosidekick-falcosidekick-ui">部署 Falco 套件 - falco + falcosidekick + falcosidekick-ui</h3>
<p>你可以使用社区提供的 helm chart 或者自行编写清单文件来部署套件到 OKD4. 只需要注意下面几个地方</p>
<ul>
<li>社区 helm chart 默认只支持 containerd 和 docker 运行时, 而 OKD4 使用的是 cri-o. 为此你需要将 <code>/var/run/crio/crio.sock</code> 挂载进来, 否则有些信息无法解析</li>
<li>为 falco daemonset 添加环境变量
<ul>
<li>FALCO_BPF_PROBE: 取值为空. 表示使用 eBPF probe 而不是 kernel module</li>
<li>DRIVERS_REPO: 取值为你的私有 driver repo 地址, driver version 之前的那段, 例如 <code>https://static.example.com/driver</code></li>
</ul></li>
</ul>
<p>这里编写 openshift template 来部署</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">oc new-project falco</span><br><span class="line">oc process -f default-config-template.yaml | oc apply -f -</span><br><span class="line">oc process -f customized-rules-template.yaml | oc apply -f -</span><br><span class="line">oc process -f falco-suite-template.yaml | oc apply -f -</span><br><span class="line">oc adm policy add-scc-to-user privileged -z falco -n falco</span><br><span class="line">oc adm policy add-scc-to-user anyuid -z falcosidekick -n falco</span><br><span class="line">oc adm policy add-scc-to-user anyuid -z default -n falco</span><br></pre></td></tr></table></figure>
<p>过一段时间后所有 pod 都正常运行了</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">oc get pod</span><br><span class="line">NAME                                READY   STATUS    RESTARTS   AGE</span><br><span class="line">falco-5d5zz                         1/1     Running   0          2m4s</span><br><span class="line">falco-8lc68                         1/1     Running   0          2m4s</span><br><span class="line">falco-8zbbw                         1/1     Running   0          2m4s</span><br><span class="line">falco-hbr7j                         1/1     Running   0          2m4s</span><br><span class="line">falco-n25kg                         1/1     Running   0          2m4s</span><br><span class="line">falco-zsrb9                         1/1     Running   0          2m4s</span><br><span class="line">falcosidekick-747f5d678d-5fx5h      1/1     Running   0          113s</span><br><span class="line">falcosidekick-747f5d678d-hv999      1/1     Running   0          113s</span><br><span class="line">falcosidekick-ui-67fc8bc75b-xrc6w   1/1     Running   0          113s</span><br></pre></td></tr></table></figure>
<p>可以正常访问 web ui</p>
<p><img src="falco-ui.png" /></p>
<h2 id="升级的考量">升级的考量</h2>
<p>Falco driver, 无论是 kernel module 还是 eBPF probe 都和特定内核版本对应. 因此当你升级 falco 或者 OKD4 时需要注意</p>
<h3 id="falco-本身的升级">Falco 本身的升级</h3>
<p>Falco 版本升级后会有新的 driver version. 此时你需要将新编译 driver 放到新的 driver verson 路径下. 这样你更新 falco daemonset 时, falco pod 会去新的 driver version 路径下下载.</p>
<h3 id="okd4-集群的升级">OKD4 集群的升级</h3>
<p>OKD4 集群的升级伴随着内核版本的更新, 此时你需要根据新的内核版本重新编译 falco driver, 上传到你的 driver repo 中. 这样 falco pod 被调度重启时可以下载到对应内核版本的 driver.</p>
]]></content>
      <categories>
        <category>RedHat</category>
        <category>Openshift</category>
      </categories>
      <tags>
        <tag>Openshift</tag>
        <tag>OKD4</tag>
        <tag>Falco</tag>
      </tags>
  </entry>
  <entry>
    <title>实施 AWS Landing Zone 1: 背景及架构介绍</title>
    <url>/2804254/</url>
    <content><![CDATA[<p>本文介绍了 AWS 多账号管理的需求, Landing Zone 方案的背景及基本结构, Landing Zone 方案和 Control Tower 的对比与选择</p>
<span id="more"></span>
<h2 id="多账户的需求和面临的挑战">多账户的需求和面临的挑战</h2>
<p>随着越来越多的工作负载迁移到 AWS 公有云, 过去的单个账户逐渐无法满足复杂环境的需要: 难以清晰界定的职责边界, 资源无法有效隔离, 对不同部门团队的计费和审计等. 此时会自然而然的引入多账户策略: 不同部门/团队, 不同环境使用不同的 AWS 账号. 优点很明显</p>
<ul>
<li>降低触及单个账户资源上限的风险</li>
<li>不同团队的职责分离</li>
<li>开发/测试/生产环境的资源隔离</li>
<li>减少人为故障的影响范围</li>
</ul>
<p>AWS Organization 提供了多账户环境的基础. 然而创建账户, 加入组织并完成初始化基线配置是一项繁琐的工作. 如何确保各个账户的合规性和配置的一致性? 如果后期配置基线发生变更, 又如何将基线变更追溯的应用到所有的现存账户上? Landing Zone 方案旨在解决这些问题.</p>
<h2 id="landing-zone-简介">Landing Zone 简介</h2>
<p>Landing Zone 是 AWS 在 2018 年推出的一套解决方案 (注意: 并不是一个 AWS 原生服务), 用来帮助用户根据 AWS 最佳实践, 快速建立安全的多账户环境</p>
<p><img src="aws-landing-zone-architecture.png" /></p>
<p>上图为 Landing Zone 的基本结构:</p>
<ul>
<li>核心账户: master, shared services, log archive, security
<ul>
<li>Master: 也称为 Organization account. 是创建 AWS Organization, Landing Zone 基础组件 (Service Catalog, CodePipeline, State Machines) 的地方</li>
<li>Shared Services: 公共服务所在的账号</li>
<li>Log Archive: 存放所有账户 Cloudtrail 和 Config log的 S3 bucket 所在的账户</li>
<li>Security: Guarduty master 所在账户, 全局 admin/readonly 角色所在账户</li>
</ul></li>
<li>Accounting Vending Machine (AVM): 是 Landing Zone 在 Service Catalog 里创建的一个产品, 通过运行 AVM 来创建新账户并应用配置基线</li>
<li>Landing Zone Codepipline: 配置变更流水线. 触发该流水线来更改核心账户资源, Service Control Policies 和基线资源等. 该流水线最终会调用 AVM 将基线资源变更应用到所有账户上</li>
</ul>
<p>各个组件/服务更深入的介绍将在后续的系列文章中给出.</p>
<h2 id="control-tower-还是-landing-zone">Control Tower 还是 Landing Zone?</h2>
<p>Landing Zone方案涉及多种AWS服务, 其部署较为复杂, 因此在 2019 年 AWS 推出了 Control Tower 服务, 旨在帮助用户通过最简单的方式设置<strong>全新</strong>的多账户 AWS 环境. 其背后依然是 Landing Zone, 但是隐藏了实现细节和基础服务.</p>
<p>用户在实际生产环境下该如何选择?</p>
<ul>
<li>Control Tower 只适用于全新部署 (greenfield). 如果你已经建立了多账户的环境 (Organization, SSO), 又不想迁移账户, 此时应该选择 Landing Zone</li>
<li>Landing Zone 具有更强的定制化 (当然这也正是它更加复杂的原因), 所有的基线资源都通过 Cloudformation template 创建, 用户可以方便的修改预设基线资源或者添加自定义资源</li>
<li>Landing Zone提供了 CI/CD 流水线, 用户可以将其配置保存在代码仓库中以实现版本控制</li>
</ul>
<h2 id="在现存多账户环境下实施-landing-zone">在现存多账户环境下实施 Landing Zone</h2>
<p>由于实际环境的复杂性, Landing Zone 方案在实施 AWS 最佳实践的同时, 不可避免的会对用户的环境做一些假设, 因此其部署的某些服务或者服务的设置对特定的用户可能并不适用. 此时需要对其预设配置进行调整.</p>
<p>举个例子, 默认 Landing Zone 会在所有账号和共享服务账号 (shared services account) 之间做 VPC peering 以便所有账户可以访问共享服务账号中的公共服务. 然而 VPC peering 本身有诸多限制, 企业环境下可能更多会选择 transit VPC 架构或者使用 transit gateway. 此时用户需要修改 Landing Zone 配置模板以匹配自身环境. 在系列的后续文章中我将逐一列示.</p>
<div class="note warning"><p>AWS 声明 Landing Zone 必须由客户的 AWS account 团队或者经过认证的合作伙伴来部署, 以确保方案的实施成功. 如果你想自己实施, 请自行评估风险</p>
</div>
]]></content>
      <categories>
        <category>AWS</category>
        <category>Landing Zone</category>
      </categories>
      <tags>
        <tag>AWS</tag>
        <tag>Landing Zone</tag>
      </tags>
  </entry>
  <entry>
    <title>实施 AWS Landing Zone 2: 实施前场景与目标规划</title>
    <url>/4237658498/</url>
    <content><![CDATA[<p>该系列文章将通过一个虚拟场景介绍如何在现存的多账户环境下实施 Landing Zone (brownfield deployment). 本文介绍实施前企业多账户环境的现状, 迁移的目标规划和准备工作</p>
<span id="more"></span>
<h2 id="实施前状况">实施前状况</h2>
<p>VOPSDEV.COM 目前在 AWS Organization 下按照部门职能和项目环境类型划分了多个组织单元 (Organization Unit, OU) 并采用了多个账号以实现职责/资源分离:</p>
<p><img src="original-organization-structure.png" /></p>
<p>业务的生产环境和测试环境账号分别在 production OU 和 staging OU 下, 开发人员的沙盒环境账号创建在 sandbox OU 下, 各个职能部门的账号创建在 department OU 下. Organization master, 安全, 审计日志, 公共服务相关账号在 core OU 下. 具体分布如下</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Root/</span><br><span class="line">  vopsdev/</span><br><span class="line">    production/</span><br><span class="line">      yl-production</span><br><span class="line">    staging/</span><br><span class="line">      yl-stagin</span><br><span class="line">    department/</span><br><span class="line">      yl-marketing</span><br><span class="line">      yl-hr</span><br><span class="line">      yl-network</span><br><span class="line">      yl-collaboration</span><br><span class="line">      yl-itdev</span><br><span class="line">      yl-itops</span><br><span class="line">    sandbox/</span><br><span class="line">      yl-sandbox-1</span><br><span class="line">      yl-sandbox-2</span><br><span class="line">    core/</span><br><span class="line">      yl-log-archive</span><br><span class="line">      yl-security</span><br><span class="line">      yl-shared-services</span><br><span class="line">      yl-master</span><br></pre></td></tr></table></figure>
<p>各个账号可以自己独立的创建 VPC. 但是如果 VPC 需要接入公司 VPN, 则需要经过安全部门审批. 经批准后, 账号的 VPC 可以通过 transit gateway attachment 连接到网络部门账号 yl-network 下的 transit gateway 来访问内网资源. DNS, Active Directory 之类的公共服务已经在共享服务账号 yl-shared-services 下创建完成. SSO 在组织账号 yl-master 下配置完成, 使用 AD-Connector 连接到共享服务账号下的 Active Directory 作为身份认证源.</p>
<p>这套环境目前面临的问题是: 由于缺少统一的配置管理方案, 各个账户初始化完成, 交付给用户使用后, 难以继续维持配置基线. 新的配置基线发布后又很难应用到现存的账户. 为此考虑在尽量重用现有服务的基础上实施 Landing Zone.</p>
<h2 id="实施规划">实施规划</h2>
<p>在组织根下创建一个新的 OU 分支, 后面会将 Landing Zone 应用到这个 OU 分支上 <img src="intermediate-organization-structure.png" /></p>
<p>然后逐步将现有账号迁移到新的 OU 分支下, 由 Landing Zone Codepipeline 创建并维持基线资源. 而所有新的账号将通过 Landing Zone 的 Accounting Vending Machine 自动创建到新的 OU 分支下.</p>
<p>所有账户迁移完成后即可删除旧的 OU 分支, 最终的账号分布如下 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Root/</span><br><span class="line">  vopsdev-landing-zone/</span><br><span class="line">    production/</span><br><span class="line">      yl-production</span><br><span class="line">    staging/</span><br><span class="line">      yl-stagin</span><br><span class="line">    department/</span><br><span class="line">      yl-marketing</span><br><span class="line">      yl-hr</span><br><span class="line">      yl-network</span><br><span class="line">      yl-collaboration</span><br><span class="line">      yl-itdev</span><br><span class="line">      yl-itops</span><br><span class="line">    sandbox/</span><br><span class="line">      yl-sandbox-1</span><br><span class="line">      yl-sandbox-2</span><br><span class="line">    core/</span><br><span class="line">      yl-log-archive</span><br><span class="line">      yl-security</span><br><span class="line">      yl-shared-services</span><br><span class="line">      yl-master</span><br></pre></td></tr></table></figure></p>
<h2 id="资源限制">资源限制</h2>
<p>对于一个新创建的账号, 其 AWS Organization 下默认只能加入很少几个账号 (2个?) 无法满足 Landing Zone 的要求. 如果你想从头开始实施, 请确保提升组织账号的 AWS Organization Account Limit (需要通过给 AWS support 提交 ticket)</p>
]]></content>
      <categories>
        <category>AWS</category>
        <category>Landing Zone</category>
      </categories>
      <tags>
        <tag>AWS</tag>
        <tag>Landing Zone</tag>
      </tags>
  </entry>
  <entry>
    <title>实施 AWS Landing Zone 3: 部署 Initiation Stack</title>
    <url>/2267786039/</url>
    <content><![CDATA[<p>本文将介绍 Landing Zone Initiation Stack 的部署, 参数解析以及一些容易出错的地方</p>
<span id="more"></span>
<h2 id="创建-initiation-stack">创建 Initiation Stack</h2>
<p>登入组织账号 (即这里的 yl-master), 进入 Cloudformation 服务. 创建一个新的 stack: <img src="create-initiation-stack.png" /></p>
<p>template s3 url 使用 https://s3.amazonaws.com/solutions-reference/aws-landing-zone/v2.3.1/aws-landing-zone-initiation.template</p>
<p>Landing Zone Initiation stack 是一个比较庞大的 cloudformation template, 大约近 6000 行配置, 其核心部分是 5 个 Lambda 和 7 个 Step Functions. 但这还不是 Landing Zone 的全貌, 因为它引用了很多外部资源, 例如保存在 S3 里的 Lambda 函数实现. 在 Initiation stack template 中大部分内容 (约 2/3) 是 7 个 StateMachines (Step Functions) 的定义.</p>
<div class="note info"><p>AWS 会对 Landing Zone 进行更新升级, 最新版本的信息可以从 https://aws.amazon.com/solutions/aws-landing-zone/ 获取</p>
</div>
<h3 id="参数详解">参数详解</h3>
<h4 id="landing-zone-core-account-configuration">Landing Zone Core Account Configuration</h4>
<p>核心账号配置部分</p>
<ul>
<li>Shared Service Account Email Address: 填写现存共享服务账号的邮件地址. 如果你的现存环境中还没有共享服务账号也没关系, 如果有需要, Landing Zone 可以帮你创建</li>
<li>Log Archive Account Email Address: 填写现存的日志账号邮件地址. 同上</li>
<li>Security Account Email Address: 填写现存的安全审计账号的邮件地址. 同上</li>
<li>Nest OU Name Delimiter: Colon (:) 使用冒号作为 OU 路径的界定符. 保持默认即可</li>
<li>Core OU Name: vopsdev-landing-zone:core 根据上一篇文章中的 OU 规划来填写, 这里是核心账号所在的 OU</li>
<li>Non Core OU Names: vopsdev-landing-zone:production,vopsdev-landing-zone:department,vopsdev-landing-zone:staging,vopsdev-landing-zone:sandbox 根据上一篇文章中的 OU 规划来写, 这里是非核心账号所在的 OU. 使用逗号作为分隔符</li>
<li>Security Alert Email Address: 接受安全告警邮件的地址. 具体是来自 GuardDuty Finding, Cloudwatch Alarm, Config Rules Compliance Status Change 的消息</li>
<li>Lock StackSetsExecution Role: Yes. 是否锁定角色 AWSCloudFormationStackSetExecutionRole. Landing Zone 通过 Stackset 来部署资源基线到各个受控账号, 因此会在各个账号下创建角色 AWSCloudFormationStackSetExecutionRole 来执行 stack instance. 该角色具有账户的管理员权限. 锁定该角色的含义是只允许特定的实体 (principal) 来承担 (assume role).</li>
<li>Subscribe All Change Events Email To Topic: No. 是否订阅所有的配置变更事件. 按需设置</li>
<li>All Change Events Email: 接受配置变更事件通知的邮件地址</li>
</ul>
<h4 id="landing-zone-pipeline-configuration">Landing Zone Pipeline Configuration</h4>
<p>流水线配置部分</p>
<ul>
<li>Pipeline Approval Stage: Yes. 给 Landing Zone 配置流水线添加手动批准的步骤</li>
<li>Pipeline Approval Email Address: 批准 Landing Zone 配置流水线的邮件地址</li>
<li>Auto Build Landing Zone: No. 是否在 initiation stack 创建完成后立刻启动配置流水线. 我们需要对配置进行定制, 因此这里一定需要设置为 No</li>
</ul>
<h4 id="shared-services-vpc-configuration">Shared Services VPC Configuration</h4>
<p>共享服务的 VPC 配置部分</p>
<ul>
<li>Shared Services VPC Options: Shared-Services-Network-3-AZs. 选择共享服务账号的 VPC 类型. 仅仅用来生成初始配置. 并不会在 Initiation 阶段创建实际的资源. 后面可以按自己的需要修改初始配置</li>
<li>Shared Services VPC CIDR: 100.65.0.0/16 共享服务 VPC 的网段. 同上</li>
</ul>
<h4 id="vpc-flow-logs-retention-policy">VPC Flow Logs Retention Policy</h4>
<p>VPC Flow Logs 的留存策略</p>
<ul>
<li>VPC Flow Logs Retention In Days: 90</li>
</ul>
<h4 id="aws-security-and-configuration-services">AWS Security and Configuration Services</h4>
<p>安全和配置管理部分</p>
<ul>
<li>Enable AWS Security and Configuraiton Monitoring in: All regions 在哪些区域启用安全和配置管理服务 (GuardDuty, Config 之类). 同样也仅仅用于生成初始化配置. 后期可以按需要修改</li>
</ul>
<h4 id="aws-config-rules">AWS Config Rules</h4>
<p>启用哪些 AWS Config Rules. 按默认都启用即可</p>
<h4 id="add-on-publisher-configuraiton">Add-On Publisher Configuraiton</h4>
<p>Add-On 产品更新配置部分</p>
<ul>
<li>AWS Manages Service Catalog Add-On Portfolio? Manual Updates 手动管理 Add-On Portfolio 的版本更新</li>
<li>Add-On Update Notification Email: Add-On 更新通知的邮件地址. 按需设置</li>
</ul>
<h2 id="关于通知邮件地址">关于通知邮件地址</h2>
<h3 id="结论">结论</h3>
<p>不同的 AWS 账号需要不同的邮件地址, 这基本不会出错. 但是在部署 Landing Zone Initiation stack 时需要注意 Security Account Email Address 的邮件地址不能用来接收 Security Alert. 即 Security Alert Email Address 和 Security Account Email Address 不能是同一个.</p>
<h3 id="原因">原因</h3>
<p>Initiaton stack 中有这样的资源定义</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">LandingZoneConfigDeployer:</span></span><br><span class="line">  <span class="attr">Type:</span> <span class="string">Custom::ConfigDeployer</span></span><br><span class="line">  <span class="attr">Properties:</span></span><br><span class="line">    <span class="attr">metrics_flag:</span> <span class="type">!FindInMap</span> [<span class="string">Solution</span>, <span class="string">Metrics</span>, <span class="string">SendAnonymousData</span>]</span><br><span class="line">    <span class="attr">email_list:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="type">!Ref</span> <span class="string">SharedServicesAccountEmail</span></span><br><span class="line">      <span class="bullet">-</span> <span class="type">!Ref</span> <span class="string">LoggingAccountEmail</span></span><br><span class="line">      <span class="bullet">-</span> <span class="type">!Ref</span> <span class="string">SecurityAccountEmail</span></span><br><span class="line">      <span class="bullet">-</span> <span class="type">!Ref</span> <span class="string">SecurityAlertEmail</span></span><br></pre></td></tr></table></figure>
<p>而 Lambda 函数 LandingZoneDeploymentLambda 的 config_deployer.py 中会检查 email_list 中是否有重复地址 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">unique_email_validator</span>(<span class="params">email_list</span>):</span></span><br><span class="line">    result = <span class="built_in">set</span>([x <span class="keyword">for</span> x <span class="keyword">in</span> email_list <span class="keyword">if</span> email_list.count(x) &gt; <span class="number">1</span>])</span><br><span class="line">    duplicate_list = <span class="built_in">list</span>(result)</span><br><span class="line">    logger.info(<span class="string">&quot;Duplicate Emails: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(duplicate_list))</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> duplicate_list:</span><br><span class="line">        logger.info(<span class="string">&quot;Duplicate emails not found&quot;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> Exception(<span class="string">&quot;Found duplicate email(s) &#123;&#125; in the parameters.&quot;</span>.<span class="built_in">format</span>(duplicate_list))</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">config_deployer</span>(<span class="params">event</span>):</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        s3 = S3(logger)</span><br><span class="line">    ...</span><br><span class="line">    <span class="comment"># Check if the emails are Unique</span></span><br><span class="line">    unique_email_validator(event.get(<span class="string">&#x27;email_list&#x27;</span>))</span><br><span class="line">        </span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>AWS</category>
        <category>Landing Zone</category>
      </categories>
      <tags>
        <tag>AWS</tag>
        <tag>Landing Zone</tag>
      </tags>
  </entry>
  <entry>
    <title>实施 AWS Landing Zone 4: 定制 Landing Zone CodePipeline</title>
    <url>/4024439897/</url>
    <content><![CDATA[<p>Landing Zone CodePipeline 默认使用一个启用了版本的 S3 bucket 作为代码来源: 当有新的配置文件 aws-landing-zone-configuration.zip 上传时, 触发 CodePipeline. 然而使用 versioned bucket 作为版本控制显然算不上最佳实践. 这里将修改 Landing Zone Codepipeline 使用 CodeCommit repo 作为代码来源. 这样就可以通过 git 来管理组织下所有 AWS 账号的配置基线.</p>
<span id="more"></span>
<p>进一步, 企业一般都有自己的代码管理平台 (bitbucket/gitlab/github), 并不见得愿意单单为了管理 Landing Zone 的配置而引入额外的 CodeCommit 代码仓库的维护成本. 实际上可以利用现有代码平台来保存 Landing Zone 配置, 其基本原理是做仓库镜像: 利用 CI 工具将特定分支的更新自动推送到 CodeCommit, 进而触发 CodePipeline.</p>
<h2 id="配置-codecommit">配置 CodeCommit</h2>
<h3 id="创建-codecommit-代码仓库">创建 CodeCommit 代码仓库</h3>
<p>到 Developer Tools, CodeCommit 下创建一个名为 landing-zone-configuration 的代码仓库 <img src="create-lz-codecommit-repo.png" /></p>
<h3 id="创建-iam-policy-和服务账号">创建 IAM Policy 和服务账号</h3>
<p>到 IAM 下创建自定义 Policy: LandingZoneCodeCommitRepoAccess <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;Version&quot;: &quot;2012-10-17&quot;,</span><br><span class="line">    &quot;Statement&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;Effect&quot;: &quot;Allow&quot;,</span><br><span class="line">            &quot;Action&quot;: [</span><br><span class="line">                &quot;codecommit:BatchGet*&quot;,</span><br><span class="line">                &quot;codecommit:Create*&quot;,</span><br><span class="line">                &quot;codecommit:DeleteBranch&quot;,</span><br><span class="line">                &quot;codecommit:Get*&quot;,</span><br><span class="line">                &quot;codecommit:List*&quot;,</span><br><span class="line">                &quot;codecommit:Describe*&quot;,</span><br><span class="line">                &quot;codecommit:Put*&quot;,</span><br><span class="line">                &quot;codecommit:Post*&quot;,</span><br><span class="line">                &quot;codecommit:Merge*&quot;,</span><br><span class="line">                &quot;codecommit:Test*&quot;,</span><br><span class="line">                &quot;codecommit:Update*&quot;,</span><br><span class="line">                &quot;codecommit:GitPull&quot;,</span><br><span class="line">                &quot;codecommit:GitPush&quot;</span><br><span class="line">            ],</span><br><span class="line">            &quot;Resource&quot;: [</span><br><span class="line">                &quot;arn:aws:codecommit:&lt;region&gt;:&lt;accountid&gt;:landing-zone-configuration&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure> 接着创建一个 IAM user 作为服务账号: landingzone-repo-mirror, 附加 LandingZoneCodeCommitRepoAccess 策略. 配置这个用户的 Security credentials: HTTPS Git credentials for AWS CodeCommit <img src="https-codecommit-credentials.png" /></p>
<p>我们并不会直接使用这个账户来修改 Landing Zone 配置. 这个账号仅仅用于同步私有代码仓库到 CodeCommit.</p>
<h2 id="使用-codecommit-作为-landing-zone-pipeline-构建源">使用 CodeCommit 作为 Landing Zone Pipeline 构建源</h2>
<p>进入 CodePipeline, 编辑 AWS-Landing-Zone-CodePipeline. Edit Source, Edit stage <img src="codepipeline-edit-source.png" /></p>
<p>编辑原有的 Source</p>
<ul>
<li>Action provider: AWS CodeCommit</li>
<li>Repository name: landing-zone-configuration</li>
<li>Branch name: master</li>
<li>Change detection options: Amazon Cloudwatch Events</li>
<li>Output artifacts: SourceApp</li>
</ul>
<p><img src="pipeline-codecommit-source.png" /></p>
<p>保存退出. 此时会提示将自动添加 AWS Cloudwatch Events rule. 这里我们需要 Events rule 来跟踪 CodeCommit 的变化进而触发 CodePipeline, 因此保持 "No resource updates needed for this source action change" 为默认的不勾选状态, 让 AWS 自动创建 Cloudwatch Events rule.</p>
<p><img src="pipeline-event-rule.png" /></p>
<p>如果感兴趣可以到 Cloudwatch Events 下查看自动产生的规则.</p>
<h2 id="使用私有代码仓库保存-landing-zone-配置">使用私有代码仓库保存 Landing Zone 配置</h2>
<p>下面是使用 gitlab CI 的例子: Landing Zone 配置保存在企业内部的 gitlab 代码仓库中, 当 master 分支发生变更时触发 gitlab CI, 将代码库镜像到 AWS CodeCommit, 进而触发 Landing Zone CodePipeline.</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">image:</span> <span class="string">rtcamp/gitlab-aws-codecommit-mirror</span></span><br><span class="line"><span class="attr">stages:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">deploy</span></span><br><span class="line"></span><br><span class="line"><span class="attr">deploy to production:</span></span><br><span class="line">  <span class="attr">tags:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">aws</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">codecommit</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">landingzone</span></span><br><span class="line">  <span class="attr">stage:</span> <span class="string">deploy</span></span><br><span class="line">  <span class="attr">only:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">master</span></span><br><span class="line">  <span class="attr">script:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">git</span> <span class="string">clone</span> <span class="string">--mirror</span> <span class="string">https://$TOKEN_NAME:$TOKEN_VALUE@$SOURCE_REPO</span> <span class="string">rtSync</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">cd</span> <span class="string">rtSync</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">git</span> <span class="string">branch</span> <span class="string">-a</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">git</span> <span class="string">push</span> <span class="string">--mirror</span> <span class="string">https://$USERNAME:$SECRET@$DEST_REPO</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>这里用到的几个 CI 变量:</p>
<ul>
<li>USERNAME: 就是前面配置的 IAM 服务账号 landing-zone-repo-mirror 下 HTTPS Git credentials for AWS CodeCommit 的用户名</li>
<li>DEST_REPO: AWS CodeCommit 代码库位置, 例如 git-codecommit.us-west-2.amazonaws.com/v1/repos/landing-zone-configuration</li>
<li>SECRET: landing-zone-repo-mirror 下 HTTPS Git credentials for AWS CodeCommit 的密码. 注意特殊字符转义</li>
<li>SOURCE_REPO: 内部代码库位置, 例如 gitlab.svc.vopsdev.com/devinfra/landing-zone-configuration.git</li>
<li>TOKEN_NAME, TOKEN_VALUE: 这里使用 deploy token 来获取私有代码库</li>
</ul>
]]></content>
      <categories>
        <category>AWS</category>
        <category>Landing Zone</category>
      </categories>
      <tags>
        <tag>AWS</tag>
        <tag>Landing Zone</tag>
      </tags>
  </entry>
  <entry>
    <title>离线部署 Openshift Container Platform 4.3 - 1: 准备离线资源</title>
    <url>/43606303/</url>
    <content><![CDATA[<p>本系列文章记录了 Openshift Container Platform (OCP) 4.3.5 离线部署的过程. 离线资源包括安装镜像, 所有样例 Image Stream, OperatorHub 下的 RedHat Operators. 虽然这是一个实验系列, 我会尽可能的按照生产实践来进行. <span id="more"></span></p>
<h2 id="实验环境">实验环境</h2>
<h3 id="跳板机">跳板机</h3>
<p>可以同时访问内外网, 同时具备科学上网的能力, 用来执行安装任务和离线材料准备</p>
<h3 id="私有镜像库">私有镜像库</h3>
<p>离线镜像会保存到私有镜像库中, 以供 OCP 安装和运行时使用, <strong>要求支持 version 2 schema 2 (manifest list)</strong>. 我这里选择的是 <strong>Quay 3</strong></p>
<ul>
<li><del>Nexus 目前还不支持 manifest list: <a href="https://issues.sonatype.org/browse/NEXUS-18546">NEXUS-18546</a></del></li>
<li><del>Harbor 目前还不支持 manifest list: <a href="https://github.com/goharbor/harbor/issues/6522">6522</a></del></li>
</ul>
<h2 id="关于镜像的获取">关于镜像的获取</h2>
<p>发现很多人误以为必须联系红帽销售, 签单之后才可以试用 OCP4, 实际上并不是这样. 注册一个<a href="https://developers.redhat.com">开发者账号</a> 后就可以获得 quay.io, registry.redhat.io 的 pull secret 来进行测试实验了.</p>
<h2 id="准备离线安装介质">准备离线安装介质</h2>
<h3 id="获取目前的版本信息">获取目前的版本信息</h3>
<p>目前最新的 OCP 版本是 4.3.5. 从这里下载 oc 客户端</p>
<ul>
<li><a href="https://mirror.openshift.com/pub/openshift-v4/clients/ocp/4.3.5/openshift-client-linux-4.3.5.tar.gz">https://mirror.openshift.com/pub/openshift-v4/clients/ocp/4.3.5/openshift-client-linux-4.3.5.tar.gz</a></li>
</ul>
<p>解压出来的二进制文件放到跳板机 PATH 下. 先看一下当前 4.3.5 的版本信息</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">oc adm release info quay.io/openshift-release-dev/ocp-release:4.3.5-x86_64</span><br><span class="line"></span><br><span class="line">Name:      4.3.5</span><br><span class="line">Digest:    sha256:64320fbf95d968fc6b9863581a92d373bc75f563a13ae1c727af37450579f61a</span><br><span class="line">Created:   2020-03-06T12:05:47Z</span><br><span class="line">OS/Arch:   linux/amd64</span><br><span class="line">Manifests: 366</span><br><span class="line"></span><br><span class="line">Pull From: quay.io/openshift-release-dev/ocp-release@sha256:64320fbf95d968fc6b9863581a92d373bc75f563a13ae1c727af37450579f61a</span><br><span class="line"></span><br><span class="line">Release Metadata:</span><br><span class="line">  Version:  4.3.5</span><br><span class="line">  Upgrades: 4.2.21, 4.2.22, 4.3.0, 4.3.1, 4.3.2, 4.3.3</span><br><span class="line">  Metadata:</span><br><span class="line">    description:</span><br><span class="line">  Metadata:</span><br><span class="line">    url: https://access.redhat.com/errata/RHBA-2020:0676</span><br><span class="line"></span><br><span class="line">Component Versions:</span><br><span class="line">  Kubernetes 1.16.2</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<h3 id="下载安装镜像">下载安装镜像</h3>
<p>实验环境中的 quay 在内网 https://quay.svc.vopsdev.com 提供服务. 事先创建好 namespace/organization: openshift-release-dev 用来存放安装镜像仓库</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> OCP_RELEASE=4.3.5</span><br><span class="line"><span class="built_in">export</span> ARCH=x86_64</span><br><span class="line"><span class="built_in">export</span> LOCAL_REGISTRY=<span class="string">&#x27;quay.svc.vopsdev.com&#x27;</span> </span><br><span class="line"><span class="built_in">export</span> LOCAL_REPOSITORY=<span class="string">&#x27;openshift-release-dev/ocp-v4.0-art-dev&#x27;</span></span><br><span class="line"><span class="built_in">export</span> UPSTREAM_REPO=<span class="string">&#x27;openshift-release-dev&#x27;</span></span><br><span class="line"><span class="built_in">export</span> RELEASE_NAME=<span class="string">&quot;ocp-release&quot;</span></span><br><span class="line"><span class="comment"># 将你从红帽获取的 pull secret 以及你的私有镜像库的 secret 加入到 pull-secret.json 中</span></span><br><span class="line"><span class="built_in">export</span> LOCAL_SECRET_JSON=<span class="string">&quot;pull-secret.json&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果你需要科学上网</span></span><br><span class="line"><span class="built_in">export</span> HTTP_PROXY=...</span><br><span class="line"><span class="built_in">export</span> HTTPS_PROXY=...</span><br><span class="line"><span class="built_in">export</span> NO_PROXY=<span class="string">&quot;quay.svc.vopsdev.com&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 这样 release 镜像以及安装需要的镜像会都同步到 quay.svc.vopsdev.com/openshift-release-dev/ocp-v4.0-art-dev 下了</span></span><br><span class="line">oc adm release mirror -a <span class="variable">$&#123;LOCAL_SECRET_JSON&#125;</span> --from=quay.io/<span class="variable">$&#123;UPSTREAM_REPO&#125;</span>/<span class="variable">$&#123;RELEASE_NAME&#125;</span>:<span class="variable">$&#123;OCP_RELEASE&#125;</span>-<span class="variable">$&#123;ARCH&#125;</span> --to-release-image=<span class="variable">$&#123;LOCAL_REGISTRY&#125;</span>/<span class="variable">$&#123;LOCAL_REPOSITORY&#125;</span>:<span class="variable">$&#123;OCP_RELEASE&#125;</span> --to=<span class="variable">$&#123;LOCAL_REGISTRY&#125;</span>/<span class="variable">$&#123;LOCAL_REPOSITORY&#125;</span></span><br></pre></td></tr></table></figure>
<p><code>oc adm release mirror</code> 命令完成后会输出下面类似的信息, 保存下来, 将来会用在 install-config.yaml 文件中</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">imageContentSources:</span><br><span class="line">- mirrors:</span><br><span class="line">  - quay.svc.vopsdev.com/openshift-release-dev/ocp-v4.0-art-dev</span><br><span class="line">  source: quay.io/openshift-release-dev/ocp-release</span><br><span class="line">- mirrors:</span><br><span class="line">  - quay.svc.vopsdev.com/openshift-release-dev/ocp-v4.0-art-dev</span><br><span class="line">  source: quay.io/openshift-release-dev/ocp-v4.0-art-dev</span><br></pre></td></tr></table></figure>
<h3 id="提取安装程序">提取安装程序</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">oc adm release extract -a <span class="variable">$&#123;LOCAL_SECRET_JSON&#125;</span> --<span class="built_in">command</span>=openshift-install <span class="string">&quot;<span class="variable">$&#123;LOCAL_REGISTRY&#125;</span>/<span class="variable">$&#123;LOCAL_REPOSITORY&#125;</span>:<span class="variable">$&#123;OCP_RELEASE&#125;</span>&quot;</span></span><br></pre></td></tr></table></figure>
<p>会产生 openshift-install 二进制程序 (不要直接从 <a href="https://mirror.openshift.com/pub/openshift-v4/clients/ocp/4.3.5">https://mirror.openshift.com/pub/openshift-v4/clients/ocp/4.3.5</a> 下载, 后面会有 sha256 匹配不上的问题)</p>
<h2 id="准备-image-stream-样例镜像">准备 Image Stream 样例镜像</h2>
<p>准备一个镜像列表, 然后使用 <code>oc image mirror</code>将镜像同步到私有仓库中</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cat sample-images.txt | <span class="keyword">while</span> <span class="built_in">read</span> line; <span class="keyword">do</span></span><br><span class="line">  target=$(<span class="built_in">echo</span> <span class="variable">$line</span> | sed <span class="string">&#x27;s/registry.redhat.io/quay.svc.vopsdev.com/&#x27;</span>)</span><br><span class="line">  oc image mirror -a <span class="variable">$&#123;LOCAL_SECRET_JSON&#125;</span> <span class="variable">$line</span> <span class="variable">$target</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure>
<p>完整的镜像列表如何获取? 如果之前装过 OCP 4.3.5, 把 openshift-cluster-samples-operator 项目下 cluster-samples-operator pod 的 /opt/openshift 目录同步出来, 简单 grep 一下就都有了.</p>
<p>完整列表参考<a href="https://gist.github.com/yuanlinios/7eea8207083e649cbe07e108a22df00b">这里</a></p>
<h2 id="准备-operatorhub-离线资源">准备 OperatorHub 离线资源</h2>
<p>首先构建 RedHat Operators 的 catalog image, 保存为 quay.svc.vopsdev.com/devinfra/redhat-operators:v1.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">oc adm catalog build --appregistry-endpoint https://quay.io/cnr --appregistry-org redhat-operators --to=quay.svc.vopsdev.com/devinfra/redhat-operators:v1</span><br></pre></td></tr></table></figure>
<p>这个 catalog image 相当于 RedHat Operators 的一个目录, 通过 catalog image 可以找到 RedHat Operators 的所有镜像. 而且 catalog image 使用 sha256 digest 来引用镜像, 能够确保应用有稳定可重复的部署.</p>
<p>然后使用 catalog image 同步 RedHat Operators 的所有镜像到私有仓库. 按照官方文档的做法是</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">oc adm catalog mirror quay.svc.vopsdev.com/devinfra/redhat-operators:v1 quay.svc.vopsdev.com</span><br></pre></td></tr></table></figure>
<p>这个命令结束后会产生 redhat-operators-manifests 目录, 下面有两个文件: mapping.txt 和 imageContentSourcePolicy.yaml.</p>
<p>然而这么做目前还有问题 <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1800674">1800674</a>: 同步出来的镜像 manifest digest 不对, 导致后面离线安装 operator 时会报镜像无法获取的错误. 暂时可以使用上面 bugzilla 链接里给出的临时解决方案: <code>skopeo copy --all</code></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cat redhat-operators-manifests/mapping.txt | <span class="keyword">while</span> <span class="built_in">read</span> line; <span class="keyword">do</span></span><br><span class="line">  origin=$(<span class="built_in">echo</span> <span class="variable">$line</span> | cut -d= -f1)</span><br><span class="line">  target=$(<span class="built_in">echo</span> <span class="variable">$line</span> | cut -d= -f2)</span><br><span class="line">  <span class="keyword">if</span> [[ <span class="string">&quot;<span class="variable">$origin</span>&quot;</span> =~ <span class="string">&quot;sha256&quot;</span> ]]; <span class="keyword">then</span></span><br><span class="line">    tag=$(<span class="built_in">echo</span> <span class="variable">$origin</span> | cut -d: -f2 | cut -c -8)</span><br><span class="line">    skopeo copy --all docker://<span class="variable">$origin</span> docker://<span class="variable">$target</span>:<span class="variable">$tag</span></span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">    skopeo copy --all docker://<span class="variable">$origin</span> docker://<span class="variable">$target</span></span><br><span class="line">  <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>RedHat</category>
        <category>Openshift</category>
      </categories>
      <tags>
        <tag>Openshift</tag>
        <tag>RedHat</tag>
      </tags>
  </entry>
  <entry>
    <title>离线部署 Openshift Container Platform 4.3 - 2: 初始安装</title>
    <url>/1074891902/</url>
    <content><![CDATA[<p>本文记录了离线环境下以 UPI (User Provisioned Infrastructure) 模式初始安装 OCP 4.3.5 集群的步骤, 包括地址/DNS 名称规划, DHCP, 负载均衡配置, ignition 文件生成, 到最后的集群部署.</p>
<p>IaaS 平台为 VMware vSphere 6.7U2. DDI 方案使用 Infoblox NIOS. 负载均衡方案使用 HAProxy</p>
<span id="more"></span>
<h2 id="dns-及-ip-地址规划">DNS 及 IP 地址规划</h2>
<table>
<colgroup>
<col style="width: 31%" />
<col style="width: 5%" />
<col style="width: 27%" />
<col style="width: 35%" />
</colgroup>
<thead>
<tr class="header">
<th>DNS 记录</th>
<th>类型</th>
<th>值</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ocp-bootstrap.int.vopsdev.com</td>
<td>A</td>
<td>192.168.11.119</td>
<td>bootstrap 节点. A 记录对应的 PTR 记录也同时创建出来, 下同</td>
</tr>
<tr class="even">
<td>ocp-node-0.int.vopsdev.com</td>
<td>A</td>
<td>192.168.11.120</td>
<td>master 节点</td>
</tr>
<tr class="odd">
<td>ocp-node-1.int.vopsdev.com</td>
<td>A</td>
<td>192.168.11.121</td>
<td>master 节点</td>
</tr>
<tr class="even">
<td>ocp-node-2.int.vopsdev.com</td>
<td>A</td>
<td>192.168.11.122</td>
<td>master 节点</td>
</tr>
<tr class="odd">
<td>ocp-node-3.int.vopsdev.com</td>
<td>A</td>
<td>192.168.11.123</td>
<td>worker 节点</td>
</tr>
<tr class="even">
<td>ocp-node-4.int.vopsdev.com</td>
<td>A</td>
<td>192.168.11.124</td>
<td>worker 节点</td>
</tr>
<tr class="odd">
<td>ocp-node-5.int.vopsdev.com</td>
<td>A</td>
<td>192.168.11.125</td>
<td>worker 节点</td>
</tr>
<tr class="even">
<td>api.ocp.vopsdev.com</td>
<td>CNAME</td>
<td>lb-if-192-168-11-249.int.vopsdev.com</td>
<td>别名, 指向现有的 LB 记录</td>
</tr>
<tr class="odd">
<td>api-int.ocp.vopsdev.com</td>
<td>CNAME</td>
<td>lb-if-192-168-11-249.int.vopsdev.com</td>
<td>如果有需要, api 和 api-int 可以分别指向内部/外部 LB</td>
</tr>
<tr class="even">
<td>*.apps.ocp.vopsdev.com</td>
<td>CNAME</td>
<td>lb-if-192-168-11-249.int.vopsdev.com</td>
<td></td>
</tr>
<tr class="odd">
<td>etcd-0.ocp.vopsdev.com</td>
<td>CNAME</td>
<td>ocp-node-0.int.vopsdev.com</td>
<td></td>
</tr>
<tr class="even">
<td>etcd-1.ocp.vopsdev.com</td>
<td>CNAME</td>
<td>ocp-node-1.int.vopsdev.com</td>
<td></td>
</tr>
<tr class="odd">
<td>etcd-2.ocp.vopsdev.com</td>
<td>CNAME</td>
<td>ocp-node-2.int.vopsdev.com</td>
<td></td>
</tr>
<tr class="even">
<td>_etcd-server-ssl._tcp.ocp.vopsdev.com</td>
<td>SRV</td>
<td>0 10 2380 etcd-0.ocp.vopsdev.com</td>
<td></td>
</tr>
<tr class="odd">
<td>_etcd-server-ssl._tcp.ocp.vopsdev.com</td>
<td>SRV</td>
<td>0 10 2380 etcd-1.ocp.vopsdev.com</td>
<td></td>
</tr>
<tr class="even">
<td>_etcd-server-ssl._tcp.ocp.vopsdev.com</td>
<td>SRV</td>
<td>0 10 2380 etcd-2.ocp.vopsdev.com</td>
<td></td>
</tr>
</tbody>
</table>
<h2 id="dhcp">DHCP</h2>
<p>所有 OCP 节点都通过 DHCP 自动获取 IP. DHCP 上做预留 (在 Infoblox 里叫 fixed address), 确保每个节点都可以获取预先规划的地址.</p>
<p><img src="infoblox-ocp-dhcp-fixed-addresses.png" /></p>
<p>由于节点 MAC 地址要在虚拟机部署之后才能确定, 这里先随便填写. 后面部署节点时会通过 Infoblox API 动态更新 fixed address 记录的 MAC 字段.</p>
<h2 id="负载均衡">负载均衡</h2>
<p>负载均衡器目前使用是 HAProxy, 简单写几个四层 LB 配置 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">frontend vs_ocp_master_6443</span><br><span class="line">    mode tcp</span><br><span class="line">    option tcplog</span><br><span class="line">    <span class="built_in">bind</span> 192.168.11.249:6443</span><br><span class="line">    default_backend pl_ocp_master_6443</span><br><span class="line"></span><br><span class="line">frontend vs_ocp_master_22623</span><br><span class="line">    mode tcp</span><br><span class="line">    option tcplog</span><br><span class="line">    <span class="built_in">bind</span> 192.168.11.249:22623</span><br><span class="line">    default_backend pl_ocp_master_22623</span><br><span class="line"></span><br><span class="line">frontend vs_ocp_router_80</span><br><span class="line">    mode tcp</span><br><span class="line">    option tcplog</span><br><span class="line">    <span class="built_in">bind</span> 192.168.11.249:80</span><br><span class="line">    default_backend pl_ocp_router_80</span><br><span class="line"></span><br><span class="line">frontend vs_ocp_router_443</span><br><span class="line">    mode tcp</span><br><span class="line">    option tcplog</span><br><span class="line">    <span class="built_in">bind</span> 192.168.11.249:443</span><br><span class="line">    default_backend pl_ocp_router_443</span><br><span class="line"></span><br><span class="line">backend pl_ocp_master_6443</span><br><span class="line">    mode tcp</span><br><span class="line">    balance <span class="built_in">source</span></span><br><span class="line">    server ocp-node-0 ocp-node-0.int.vopsdev.com:6443 check</span><br><span class="line">    server ocp-node-1 ocp-node-1.int.vopsdev.com:6443 check</span><br><span class="line">    server ocp-node-2 ocp-node-2.int.vopsdev.com:6443 check</span><br><span class="line">    server ocp-bootstrap ocp-bootstrap.int.vopsdev.com:6443 check</span><br><span class="line"></span><br><span class="line">backend pl_ocp_master_22623</span><br><span class="line">    mode tcp</span><br><span class="line">    balance <span class="built_in">source</span></span><br><span class="line">    server ocp-node-0 ocp-node-0.int.vopsdev.com:22623 check</span><br><span class="line">    server ocp-node-1 ocp-node-1.int.vopsdev.com:22623 check</span><br><span class="line">    server ocp-node-2 ocp-node-2.int.vopsdev.com:22623 check</span><br><span class="line">    server ocp-bootstrap ocp-bootstrap.int.vopsdev.com:22623 check</span><br><span class="line"></span><br><span class="line">backend pl_ocp_router_80</span><br><span class="line">    mode tcp</span><br><span class="line">    balance <span class="built_in">source</span></span><br><span class="line">    server ocp-node-3 ocp-node-3.int.vopsdev.com:80 check</span><br><span class="line">    server ocp-node-4 ocp-node-4.int.vopsdev.com:80 check</span><br><span class="line">    server ocp-node-5 ocp-node-5.int.vopsdev.com:80 check</span><br><span class="line"></span><br><span class="line">backend pl_ocp_router_443</span><br><span class="line">    mode tcp</span><br><span class="line">    balance <span class="built_in">source</span></span><br><span class="line">    server ocp-node-3 ocp-node-3.int.vopsdev.com:443 check</span><br><span class="line">    server ocp-node-4 ocp-node-4.int.vopsdev.com:443 check</span><br><span class="line">    server ocp-node-5 ocp-node-5.int.vopsdev.com:443 check</span><br></pre></td></tr></table></figure></p>
<h2 id="准备-ignition-文件">准备 ignition 文件</h2>
<h3 id="编写-install-config.yaml">编写 install-config.yaml</h3>
<p>根据环境的实际情况编写 install-config.yaml. 下面是我的实验环境所使用的版本 <figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">baseDomain:</span> <span class="string">vopsdev.com</span></span><br><span class="line"><span class="attr">compute:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">hyperthreading:</span> <span class="string">Enabled</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">worker</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">0</span></span><br><span class="line"><span class="attr">controlPlane:</span></span><br><span class="line">  <span class="attr">hyperthreading:</span> <span class="string">Enabled</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">master</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">3</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">ocp</span></span><br><span class="line"><span class="attr">platform:</span></span><br><span class="line">  <span class="attr">vsphere:</span></span><br><span class="line">    <span class="attr">vcenter:</span> <span class="string">vc.int.vopsdev.com</span></span><br><span class="line">    <span class="attr">username:</span> <span class="string">xxx</span></span><br><span class="line">    <span class="attr">password:</span> <span class="string">xxx</span></span><br><span class="line">    <span class="attr">datacenter:</span> <span class="string">HDC</span></span><br><span class="line">    <span class="attr">defaultDatastore:</span> <span class="string">DS-SHARED</span></span><br><span class="line"><span class="attr">fips:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">networking:</span></span><br><span class="line">  <span class="attr">networkType:</span> <span class="string">OpenShiftSDN</span></span><br><span class="line">  <span class="attr">clusterNetworks:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">cidr:</span> <span class="number">10.9</span><span class="number">.0</span><span class="number">.0</span><span class="string">/20</span></span><br><span class="line">    <span class="attr">hostPrefix:</span> <span class="number">24</span></span><br><span class="line">  <span class="attr">serviceNetwork:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="number">172.16</span><span class="number">.0</span><span class="number">.0</span><span class="string">/20</span></span><br><span class="line"><span class="attr">sshKey:</span> <span class="string">&#x27;ssh-rsa ...&#x27;</span></span><br><span class="line"><span class="attr">additionalTrustBundle:</span> <span class="string">|</span></span><br><span class="line"><span class="string">  -----BEGIN CERTIFICATE-----</span></span><br><span class="line"><span class="string">  ...</span></span><br><span class="line"><span class="string">  -----END CERTIFICATE-----</span></span><br><span class="line"><span class="string"></span><span class="attr">imageContentSources:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">mirrors:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">quay.svc.vopsdev.com/openshift-release-dev/ocp-v4.0-art-dev</span></span><br><span class="line">  <span class="attr">source:</span> <span class="string">quay.io/openshift-release-dev/ocp-release</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">mirrors:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">quay.svc.vopsdev.com/openshift-release-dev/ocp-v4.0-art-dev</span></span><br><span class="line">  <span class="attr">source:</span> <span class="string">quay.io/openshift-release-dev/ocp-v4.0-art-dev</span></span><br><span class="line"><span class="attr">pullSecret:</span> <span class="string">&#x27;&#123;&quot;auths&quot;:&#123;...&#125;&#125;&#x27;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<p>一些注释</p>
<ul>
<li>metadata.name + baseDomain 组成集群的基础域名, 这里是 ocp.vopsdev.com</li>
<li>需要在 vSphere Datacenter 下按照 metadata.name 建立一个虚拟机目录, 将来创建 vSphere 动态卷需要, 具体原因看<a href="https://github.com/vmware-archive/kubernetes/issues/499">这里</a>. 然而你并不需要将 OCP 节点放到这个虚拟机目录下</li>
<li>platform 下的 vcenter 信息主要用于动态卷的创建, 确保这个用户有足够的权限, 具体看<a href="https://vmware.github.io/vsphere-storage-for-kubernetes/documentation/vcp-roles.html">这里</a></li>
<li>我的环境有私有 PKI, 所有的服务器证书由二级 CA 签发, 因此把根 CA 的证书放到 additionalTrustBundle 里, 确保所有 OCP 节点信任私有根 CA</li>
<li>imageContentSources 来自前面<code>oc adm release mirror</code> 的输出结果</li>
<li>我的 quay 私有镜像库不允许匿名拉取, 因此提供了 pullSecret</li>
</ul>
<h3 id="生成-ignition">生成 ignition</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rm -rf assets; mkdir assets</span><br><span class="line">cp install-config.yaml assets/</span><br><span class="line">./openshift-install create manifests --dir=assets</span><br><span class="line">sed -i <span class="string">&#x27;s/mastersSchedulable: true/mastersSchedulable: false/&#x27;</span> assets/manifests/cluster-scheduler-02-config.yml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置一下时间同步服务. 我这里两个 ntp 服务器是 192.168.11.2 和 192.168.255.249</span></span><br><span class="line">chrony_config=$(<span class="built_in">echo</span> -n <span class="string">&quot;server 192.168.11.2 iburst</span></span><br><span class="line"><span class="string">server 192.168.255.249 iburst</span></span><br><span class="line"><span class="string">driftfile /var/lib/chrony/drift</span></span><br><span class="line"><span class="string">makestep 1.0 3</span></span><br><span class="line"><span class="string">rtcsync</span></span><br><span class="line"><span class="string">logdir /var/log/chrony&quot;</span> | base64 -w0)</span><br><span class="line"></span><br><span class="line">cat &gt; assets/openshift/99_master-chrony.yaml &lt;&lt;<span class="string">EOF</span></span><br><span class="line"><span class="string">apiVersion: machineconfiguration.openshift.io/v1</span></span><br><span class="line"><span class="string">kind: MachineConfig</span></span><br><span class="line"><span class="string">metadata:</span></span><br><span class="line"><span class="string">  labels:</span></span><br><span class="line"><span class="string">    machineconfiguration.openshift.io/role: master</span></span><br><span class="line"><span class="string">  name: 50-vopsdev-master-chrony</span></span><br><span class="line"><span class="string">spec:</span></span><br><span class="line"><span class="string">  config:</span></span><br><span class="line"><span class="string">    ignition:</span></span><br><span class="line"><span class="string">      version: 2.2.0</span></span><br><span class="line"><span class="string">    storage:</span></span><br><span class="line"><span class="string">      files:</span></span><br><span class="line"><span class="string">      - contents:</span></span><br><span class="line"><span class="string">          source: data:text/plain;charset=utf-8;base64,$chrony_config</span></span><br><span class="line"><span class="string">        filesystem: root</span></span><br><span class="line"><span class="string">        mode: 0644</span></span><br><span class="line"><span class="string">        path: /etc/chrony.conf</span></span><br><span class="line"><span class="string">EOF</span></span><br><span class="line"></span><br><span class="line">cat &gt; assets/openshift/99_worker-chrony.yaml &lt;&lt;<span class="string">EOF</span></span><br><span class="line"><span class="string">apiVersion: machineconfiguration.openshift.io/v1</span></span><br><span class="line"><span class="string">kind: MachineConfig</span></span><br><span class="line"><span class="string">metadata:</span></span><br><span class="line"><span class="string">  labels:</span></span><br><span class="line"><span class="string">    machineconfiguration.openshift.io/role: worker</span></span><br><span class="line"><span class="string">  name: 50-vopsdev-worker-chrony</span></span><br><span class="line"><span class="string">spec:</span></span><br><span class="line"><span class="string">  config:</span></span><br><span class="line"><span class="string">    ignition:</span></span><br><span class="line"><span class="string">      version: 2.2.0</span></span><br><span class="line"><span class="string">    storage:</span></span><br><span class="line"><span class="string">      files:</span></span><br><span class="line"><span class="string">      - contents:</span></span><br><span class="line"><span class="string">          source: data:text/plain;charset=utf-8;base64,$chrony_config</span></span><br><span class="line"><span class="string">        filesystem: root</span></span><br><span class="line"><span class="string">        mode: 0644</span></span><br><span class="line"><span class="string">        path: /etc/chrony.conf</span></span><br><span class="line"><span class="string">EOF</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成 ignition 配置</span></span><br><span class="line">./openshift-install create ignition-configs --dir=assets</span><br><span class="line"></span><br><span class="line"><span class="comment"># 跳板机上运行了 httpd, 提供静态文件服务</span></span><br><span class="line"><span class="comment"># 后面节点可以通过 http://static.svc.vopsdev.com/ocp/bootstrap.ign 下载</span></span><br><span class="line">sudo cp -f assets/bootstrap.ign /var/www/static/ocp/</span><br><span class="line"></span><br><span class="line">cat &gt; ./assets/append-bootstrap.ign &lt;&lt;<span class="string">EOF</span></span><br><span class="line"><span class="string">&#123;</span></span><br><span class="line"><span class="string">  &quot;ignition&quot;: &#123;</span></span><br><span class="line"><span class="string">    &quot;config&quot;: &#123;</span></span><br><span class="line"><span class="string">      &quot;append&quot;: [</span></span><br><span class="line"><span class="string">        &#123;</span></span><br><span class="line"><span class="string">          &quot;source&quot;: &quot;http://static.svc.vopsdev.com/ocp/bootstrap.ign&quot;, </span></span><br><span class="line"><span class="string">          &quot;verification&quot;: &#123;&#125;</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">      ]</span></span><br><span class="line"><span class="string">    &#125;,</span></span><br><span class="line"><span class="string">    &quot;timeouts&quot;: &#123;&#125;,</span></span><br><span class="line"><span class="string">    &quot;version&quot;: &quot;2.1.0&quot;</span></span><br><span class="line"><span class="string">  &#125;,</span></span><br><span class="line"><span class="string">  &quot;networkd&quot;: &#123;&#125;,</span></span><br><span class="line"><span class="string">  &quot;passwd&quot;: &#123;&#125;,</span></span><br><span class="line"><span class="string">  &quot;storage&quot;: &#123;&#125;,</span></span><br><span class="line"><span class="string">  &quot;systemd&quot;: &#123;&#125;</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string">EOF</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># base64 编码的 ignition</span></span><br><span class="line">base64 -w0 assets/master.ign &gt; assets/master.64</span><br><span class="line">base64 -w0 assets/worker.ign &gt; assets/worker.64</span><br><span class="line">base64 -w0 assets/append-bootstrap.ign &gt; assets/append-bootstrap.64</span><br></pre></td></tr></table></figure>
<h2 id="部署节点">部署节点</h2>
<p>对于 vSphere 环境下的部署, ignition 数据需要通过 guestinfo.ignition.config.data 虚拟机属性注入, 手工操作显然不可接受. 我这里采用的方法是 ovftool + govc. 用 ansible 或者 terraform 之类应该也可以做, 但是在这儿显得杀鸡用牛刀了</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">BOOTSTRAP_B64_CONF=<span class="string">&quot;<span class="subst">$(cat assets/append-bootstrap.64)</span>&quot;</span></span><br><span class="line">MASTER_B64_CONF=<span class="string">&quot;<span class="subst">$(cat assets/master.64)</span>&quot;</span></span><br><span class="line">WORKER_B64_CONF=<span class="string">&quot;<span class="subst">$(cat assets/worker.64)</span>&quot;</span></span><br><span class="line">RHCOS_OVA=<span class="string">&quot;/mnt/driver/RedHat/openshift-4.3/rhcos-4.3.0-x86_64-vmware.ova&quot;</span></span><br><span class="line">USER=<span class="string">&#x27;administrator@vsphere.local&#x27;</span></span><br><span class="line">PASS=<span class="string">&#x27;xxx&#x27;</span></span><br><span class="line">VC=<span class="string">&quot;vc.int.vopsdev.com&quot;</span></span><br><span class="line">VMFOLDER=<span class="string">&quot;Lab/OCP&quot;</span></span><br><span class="line">NETWORK=<span class="string">&quot;SG-SVC-11&quot;</span></span><br><span class="line">DATASTORE=<span class="string">&quot;DS-SHARED&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># infoblox api access</span></span><br><span class="line">IBUSER=<span class="string">&quot;xxx&quot;</span></span><br><span class="line">IBPASS=<span class="string">&quot;xxx&quot;</span></span><br><span class="line">IBAPI_BASE=<span class="string">&quot;https://infoblox.int.vopsdev.com/wapi/v2.1&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> GOVC_URL=<span class="string">&quot;<span class="variable">$USER</span>:<span class="variable">$PASS</span>@<span class="variable">$VC</span>&quot;</span></span><br><span class="line"><span class="built_in">export</span> GOVC_DATACENTER=HDC</span><br><span class="line"></span><br><span class="line">OCP_MASTERS=()</span><br><span class="line">OCP_WORKERS=()</span><br><span class="line"></span><br><span class="line"><span class="comment"># bootstrap node: 4 vCPU + 16G RAM</span></span><br><span class="line">VM=<span class="string">&quot;ocp-bootstrap&quot;</span></span><br><span class="line">ovftool --vmFolder=<span class="string">&quot;<span class="variable">$VMFOLDER</span>&quot;</span> --datastore=<span class="string">&quot;<span class="variable">$DATASTORE</span>&quot;</span> --diskMode=<span class="string">&quot;thin&quot;</span> --net:<span class="string">&quot;VM Network&quot;</span>=<span class="string">&quot;<span class="variable">$NETWORK</span>&quot;</span> --name=<span class="string">&quot;<span class="variable">$VM</span>&quot;</span>  --numberOfCpus:<span class="string">&#x27;*&#x27;</span>=4 --memorySize:<span class="string">&#x27;*&#x27;</span>=16384 --allowExtraConfig --extraConfig:disk.EnableUUID=<span class="literal">true</span> --extraConfig:guestinfo.ignition.config.data.encoding=<span class="string">&quot;base64&quot;</span> --extraConfig:guestinfo.ignition.config.data=<span class="string">&quot;<span class="variable">$BOOTSTRAP_B64_CONF</span>&quot;</span> <span class="variable">$RHCOS_OVA</span> <span class="string">&quot;vi://<span class="variable">$USER</span>:<span class="variable">$PASS</span>@<span class="variable">$VC</span>/HDC/host/compute-cluster&quot;</span></span><br><span class="line">govc vm.disk.change -vm <span class="variable">$VM</span> -disk.label <span class="string">&quot;Hard disk 1&quot;</span> -size 120G</span><br><span class="line"></span><br><span class="line"><span class="comment"># master nodes: 4 vCPU + 16G RAM</span></span><br><span class="line"><span class="keyword">for</span> idx <span class="keyword">in</span> 0 1 2; <span class="keyword">do</span></span><br><span class="line">  VM=<span class="string">&quot;ocp-node-<span class="variable">$idx</span>&quot;</span></span><br><span class="line">  ovftool --vmFolder=<span class="string">&quot;<span class="variable">$VMFOLDER</span>&quot;</span> --datastore=<span class="string">&quot;<span class="variable">$DATASTORE</span>&quot;</span> --diskMode=<span class="string">&quot;thin&quot;</span> --net:<span class="string">&quot;VM Network&quot;</span>=<span class="string">&quot;<span class="variable">$NETWORK</span>&quot;</span> --name=<span class="string">&quot;<span class="variable">$VM</span>&quot;</span>  --numberOfCpus:<span class="string">&#x27;*&#x27;</span>=4 --memorySize:<span class="string">&#x27;*&#x27;</span>=16384 --allowExtraConfig --extraConfig:disk.EnableUUID=<span class="literal">true</span> --prop:guestinfo.ignition.config.data.encoding=<span class="string">&quot;base64&quot;</span> --prop:guestinfo.ignition.config.data=<span class="string">&quot;<span class="variable">$MASTER_B64_CONF</span>&quot;</span> <span class="variable">$RHCOS_OVA</span> <span class="string">&quot;vi://<span class="variable">$USER</span>:<span class="variable">$PASS</span>@<span class="variable">$VC</span>/HDC/host/compute-cluster&quot;</span></span><br><span class="line">  govc vm.disk.change -vm <span class="variable">$VM</span> -disk.label <span class="string">&quot;Hard disk 1&quot;</span> -size 120G</span><br><span class="line">  OCP_MASTERS=(<span class="string">&quot;<span class="variable">$&#123;OCP_MASTERS[@]&#125;</span>&quot;</span> <span class="variable">$VM</span>)</span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># worker nodes: 4 vCPU + 32G RAM</span></span><br><span class="line"><span class="keyword">for</span> idx <span class="keyword">in</span> 3 4 5; <span class="keyword">do</span></span><br><span class="line">  VM=<span class="string">&quot;ocp-node-<span class="variable">$idx</span>&quot;</span></span><br><span class="line">  ovftool --vmFolder=<span class="string">&quot;<span class="variable">$VMFOLDER</span>&quot;</span> --datastore=<span class="string">&quot;<span class="variable">$DATASTORE</span>&quot;</span> --diskMode=<span class="string">&quot;thin&quot;</span> --net:<span class="string">&quot;VM Network&quot;</span>=<span class="string">&quot;<span class="variable">$NETWORK</span>&quot;</span> --name=<span class="string">&quot;<span class="variable">$VM</span>&quot;</span>  --numberOfCpus:<span class="string">&#x27;*&#x27;</span>=4 --memorySize:<span class="string">&#x27;*&#x27;</span>=32768 --allowExtraConfig --extraConfig:disk.EnableUUID=<span class="literal">true</span> --prop:guestinfo.ignition.config.data.encoding=<span class="string">&quot;base64&quot;</span> --prop:guestinfo.ignition.config.data=<span class="string">&quot;<span class="variable">$WORKER_B64_CONF</span>&quot;</span> <span class="variable">$RHCOS_OVA</span> <span class="string">&quot;vi://<span class="variable">$USER</span>:<span class="variable">$PASS</span>@<span class="variable">$VC</span>/HDC/host/compute-cluster&quot;</span></span><br><span class="line">  govc vm.disk.change -vm <span class="variable">$VM</span> -disk.label <span class="string">&quot;Hard disk 1&quot;</span> -size 120G</span><br><span class="line">  OCP_WORKERS=(<span class="string">&quot;<span class="variable">$&#123;OCP_WORKERS[@]&#125;</span>&quot;</span> <span class="variable">$VM</span>)</span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据新的 mac 地址, 更新 dhcp fixed address 记录</span></span><br><span class="line">ALL_NODES=(ocp-bootstrap <span class="string">&quot;<span class="variable">$&#123;OCP_MASTERS[@]&#125;</span>&quot;</span> <span class="string">&quot;<span class="variable">$&#123;OCP_WORKERS[@]&#125;</span>&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> node <span class="keyword">in</span> <span class="string">&quot;<span class="variable">$&#123;ALL_NODES[@]&#125;</span>&quot;</span>; <span class="keyword">do</span></span><br><span class="line">  mac=$(govc device.info -vm <span class="variable">$node</span> -json ethernet-0 | jq -r .Devices[].MacAddress)</span><br><span class="line">  ip=$(host <span class="variable">$node</span> | cut -d<span class="string">&#x27; &#x27;</span> -f4)</span><br><span class="line">  <span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$node</span>: <span class="variable">$ip</span>, <span class="variable">$mac</span>&quot;</span></span><br><span class="line">  api_path=$(curl -s -k -u <span class="variable">$IBUSER</span>:<span class="variable">$IBPASS</span> -H <span class="string">&quot;Content-Type: application/json&quot;</span> -X GET <span class="string">&quot;<span class="variable">$IBAPI_BASE</span>/fixedaddress?ipv4addr=<span class="variable">$ip</span>&quot;</span> | jq -r <span class="string">&#x27;.[] | ._ref&#x27;</span>)  </span><br><span class="line">  curl -s -k -u <span class="variable">$IBUSER</span>:<span class="variable">$IBPASS</span> -H <span class="string">&quot;Content-Type: application/json&quot;</span> -X PUT  <span class="string">&quot;<span class="variable">$IBAPI_BASE</span>/<span class="variable">$&#123;api_path&#125;</span>&quot;</span> -d <span class="string">&quot;&#123;\&quot;mac\&quot;:\&quot;<span class="variable">$mac</span>\&quot;&#125;&quot;</span> &gt; /dev/null</span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 必要时 reload infoblox DHCP 服务</span></span><br><span class="line">grid_path=$(curl -s -k -u <span class="variable">$IBUSER</span>:<span class="variable">$IBPASS</span> -H <span class="string">&quot;Content-Type: application/json&quot;</span> <span class="string">&quot;<span class="variable">$IBAPI_BASE</span>/grid&quot;</span> | jq -r <span class="string">&#x27;.[] | ._ref&#x27;</span>)</span><br><span class="line">curl -s -k -u <span class="variable">$IBUSER</span>:<span class="variable">$IBPASS</span> -H <span class="string">&quot;Content-Type: application/json&quot;</span> -X POST <span class="string">&quot;<span class="variable">$IBAPI_BASE</span>/<span class="variable">$&#123;grid_path&#125;</span>?_function=restartservices&quot;</span> -d <span class="string">&#x27;&#123;&quot;member_order&quot;:&quot;SIMULTANEOUSLY&quot;,&quot;restart_option&quot;:&quot;RESTART_IF_NEEDED&quot;,&quot;service_option&quot;:&quot;DHCP&quot;&#125;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动 bootstrap</span></span><br><span class="line">govc vm.power -on <span class="string">&quot;ocp-bootstrap&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 观察负载均衡器, 下面两个没问题了再进行下去</span></span><br><span class="line"><span class="comment"># curl -v -k https://api-int.ocp.vopsdev.com:22623/config/master</span></span><br><span class="line"><span class="comment"># curl -v -k https://api-int.ocp.vopsdev.com:22623/config/worker </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动所有 master 节点. 节点会去 bootstrap 上下载配置</span></span><br><span class="line"><span class="keyword">for</span> node <span class="keyword">in</span> <span class="string">&quot;<span class="variable">$&#123;OCP_MASTERS[@]&#125;</span>&quot;</span>; <span class="keyword">do</span></span><br><span class="line">  govc vm.power -on <span class="variable">$node</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 观察负载均衡器, 3 个 master 都起来了再进行下去</span></span><br><span class="line"><span class="comment"># curl -v -k https://api-int.ocp.vopsdev.com:22623/config/master</span></span><br><span class="line"><span class="comment"># curl -v -k https://api-int.ocp.vopsdev.com:22623/config/worker </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动所有 worker 节点. 节点会去 master 上下载配置</span></span><br><span class="line"><span class="keyword">for</span> node <span class="keyword">in</span> <span class="string">&quot;<span class="variable">$&#123;OCP_WORKERS[@]&#125;</span>&quot;</span>; <span class="keyword">do</span></span><br><span class="line">  govc vm.power -on <span class="variable">$node</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 到这儿这步一般已经结束了</span></span><br><span class="line">./openshift-install --dir=assets wait-for bootstrap-complete --log-level=debug</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> KUBECONFIG=<span class="string">&#x27;./assets/auth/kubeconfig&#x27;</span></span><br><span class="line">oc whoami</span><br><span class="line">oc version</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果有 pending 的证书请求就批准</span></span><br><span class="line">oc get csr</span><br><span class="line">oc get csr -o json | jq -r <span class="string">&#x27;.items[] | select(.status == &#123;&#125; ) | .metadata.name&#x27;</span> | xargs oc adm certificate approve</span><br><span class="line"></span><br><span class="line"><span class="comment"># 等待 worker 节点起来, 确保所有节点都是 Ready</span></span><br><span class="line">oc get nodes</span><br><span class="line"></span><br><span class="line"><span class="comment"># 等所有 cluster operator 状态 AVAILABLE: TRUE</span></span><br><span class="line">oc get clusteroperators</span><br><span class="line"></span><br><span class="line"><span class="comment"># 等待安装结束, 获取集群管理员密码</span></span><br><span class="line">./openshift-install --dir=assets wait-for install-complete</span><br></pre></td></tr></table></figure>
<p>使用集群管理员 kubeadmin 登录 web console <img src="ocp-console-initial.png" /></p>
<p>至此集群初步搭建完成, 后面将针对各个基础服务进行完善.</p>
]]></content>
      <categories>
        <category>RedHat</category>
        <category>Openshift</category>
      </categories>
      <tags>
        <tag>Openshift</tag>
        <tag>RedHat</tag>
      </tags>
  </entry>
  <entry>
    <title>离线部署 Openshift Container Platform 4.3 - 4: 配置 ingress</title>
    <url>/2332552694/</url>
    <content><![CDATA[<p>本文简单配置一下 ingress router, 解决默认设置的一些问题</p>
<ul>
<li>固定 router pod 的放置节点: 默认没有 placement 规则, ingress router pod 可以跑在任意 worker 节点上, 给外部负载均衡器的配置带来麻烦</li>
<li>替换默认证书为私有 CA 签发证书, 避免内网访问时提示证书不受信任的警告</li>
</ul>
<span id="more"></span>
<h2 id="ingress-router-placement">Ingress Router Placement</h2>
<p>我这个环境里只有三个 worker 节点, 配置外部负载均衡器访问 ingress 暴露的应用时也就是用这三个节点作为后端 server pool. 但是实际情况中可能有很多 worker 节点, 因此需要对 ingress router pod 的放置策略加以限制, 便于配置外部负载均衡器.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看 IngressController 的 API spec. 可以看到放置策略支持 nodeSelector 模式</span></span><br><span class="line">oc explain ingresscontroller.spec.nodePlacement --api-version=operator.openshift.io/v1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 给目标 worker 打标签, 标记其角色为 infra</span></span><br><span class="line">oc label node ocp-node-3.int.vopsdev.com node-role.kubernetes.io/worker=infra --overwrite</span><br><span class="line">oc label node ocp-node-4.int.vopsdev.com node-role.kubernetes.io/worker=infra --overwrite</span><br><span class="line">oc label node ocp-node-5.int.vopsdev.com node-role.kubernetes.io/worker=infra --overwrite</span><br><span class="line"></span><br><span class="line"><span class="comment"># 给 ingresscontroller 设置放置策略</span></span><br><span class="line">oc patch ingresscontroller default --<span class="built_in">type</span>=merge -p <span class="string">&#x27;&#123;&quot;spec&quot;:&#123;&quot;nodePlacement&quot;:&#123;&quot;nodeSelector&quot;:&#123;&quot;matchLabels&quot;:&#123;&quot;node-role.kubernetes.io/worker&quot;:&quot;infra&quot;&#125;&#125;&#125;&#125;&#125;&#x27;</span> -n openshift-ingress-operator</span><br><span class="line"></span><br><span class="line"><span class="comment"># router pod 进行了重建, 并且调度到目标节点上</span></span><br><span class="line">oc get pod -n openshift-ingress -o wide</span><br></pre></td></tr></table></figure>
<h2 id="替换-ingress-默认证书">替换 ingress 默认证书</h2>
<p>使用私有 PKI 的二级 CA (issuing CA) 签发一个 wildcard 证书作为 ingress 默认证书, 包含下面名字</p>
<ul>
<li>ocp.vopsdev.com</li>
<li>*.ocp.vopsdev.com</li>
<li>*.apps.ocp.vopsdev.com</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 让 haproxy router 信任私有根 CA</span></span><br><span class="line">oc create configmap vopsdev-root-ca --from-file=ca-bundle.crt=/etc/pki/ca-trust/<span class="built_in">source</span>/anchors/vopsdev-root-ca-g1.crt -n openshift-config</span><br><span class="line">oc patch proxy/cluster --<span class="built_in">type</span>=merge --patch=<span class="string">&#x27;&#123;&quot;spec&quot;:&#123;&quot;trustedCA&quot;:&#123;&quot;name&quot;:&quot;vopsdev-root-ca&quot;&#125;&#125;&#125;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里的证书使用证书链形式, 包含服务器证书 + 二级 CA 证书</span></span><br><span class="line"><span class="comment"># 由于内网机器都已经信任了根 CA, 结合这里的服务器证书链可以构成完整的 TLS 验证路径</span></span><br><span class="line">oc create secret tls wildcard-ocp-tls --cert=private.wildcard.vopsdev.com.chained.crt --key=private.wildcard.vopsdev.com.key -n openshift-ingress</span><br><span class="line">oc patch ingresscontroller default --<span class="built_in">type</span>=merge -p <span class="string">&#x27;&#123;&quot;spec&quot;:&#123;&quot;defaultCertificate&quot;:&#123;&quot;name&quot;:&quot;wildcard-ocp-tls&quot;&#125;&#125;&#125;&#x27;</span> -n openshift-ingress-operator</span><br></pre></td></tr></table></figure>
<p>等待 router pod 重建完成后, 测试登录 web console, 可以发现证书替换已经生效 <img src="ingress-default-certificate.png" /></p>
<p>对于内网环境, 这种 wildcard 形式的默认证书基本够用. 签发一个时效足够长的证书后也不用操心续期的问题. 但是如果想把应用发布到公网上, 还是得需要公网 CA 签发的证书. 目前用的比较多的是 Let's Encrypt, 此时可以结合 Openshift ACME Controller 实现为 ingress 资源自动申请 Let's Encrypt 证书并自动续期.</p>
]]></content>
      <categories>
        <category>RedHat</category>
        <category>Openshift</category>
      </categories>
      <tags>
        <tag>Openshift</tag>
        <tag>RedHat</tag>
      </tags>
  </entry>
  <entry>
    <title>离线部署 Openshift Container Platform 4.3 - 3: 配置本地 ImageStream 和 OperatorHub</title>
    <url>/2056397848/</url>
    <content><![CDATA[<p>本文将使用前面保存私有镜像库的离线资源替换默认的 ImageStream 和 OperatorHub</p>
<span id="more"></span>
<h2 id="替换-sample-images">替换 Sample Images</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 私有镜像库使用私有 PKI 签发的证书提供 https 服务, 需要让 samples operator 信任根 CA</span></span><br><span class="line">oc create configmap registry-config --from-file=/etc/pki/ca-trust/<span class="built_in">source</span>/anchors/vopsdev-root-ca-g1.crt -n openshift-config</span><br><span class="line">oc patch image.config.openshift.io cluster --patch <span class="string">&#x27;&#123;&quot;spec&quot;:&#123;&quot;additionalTrustedCA&quot;:&#123;&quot;name&quot;:&quot;registry-config&quot;&#125;&#125;&#125;&#x27;</span> --<span class="built_in">type</span>=merge</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加私有镜像库地址</span></span><br><span class="line">oc patch configs.samples.operator.openshift.io cluster --patch <span class="string">&#x27;&#123;&quot;spec&quot;:&#123;&quot;samplesRegistry&quot;:&quot;quay.svc.vopsdev.com&quot;&#125;&#125;&#x27;</span> --<span class="built_in">type</span>=merge</span><br><span class="line"></span><br><span class="line"><span class="comment"># 清除现有的 imagestreams</span></span><br><span class="line">oc patch configs.samples.operator.openshift.io cluster --patch <span class="string">&#x27;[&#123;&quot;op&quot;: &quot;replace&quot;, &quot;path&quot;: &quot;/spec/managementState&quot;, &quot;value&quot;:&quot;Removed&quot;&#125;]&#x27;</span> --<span class="built_in">type</span>=json</span><br><span class="line">oc get is -n openshift</span><br><span class="line"></span><br><span class="line"><span class="comment"># 再重新创建出来</span></span><br><span class="line">oc patch configs.samples.operator.openshift.io cluster --patch <span class="string">&#x27;[&#123;&quot;op&quot;: &quot;replace&quot;, &quot;path&quot;: &quot;/spec/managementState&quot;, &quot;value&quot;:&quot;Managed&quot;&#125;]&#x27;</span> --<span class="built_in">type</span>=json</span><br><span class="line">oc get is -n openshift</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查 dockerImageReference 可以发现已经指向私有镜像库了  </span></span><br><span class="line">oc get is apicast-gateway -n openshift -o yaml</span><br></pre></td></tr></table></figure>
<p>在 openshift namespace 下的这几个 imagestream 不受 samples operator 管理, 而是来自离线安装介质</p>
<ul>
<li>cli</li>
<li>cli-artifacts</li>
<li>installer</li>
<li>installer-artifacts</li>
<li>must-gather</li>
<li>tests</li>
<li>jenkins</li>
<li>jenkins-agent-maven</li>
<li>jenkins-agent-nodejs</li>
</ul>
<h2 id="替换-operatorhub">替换 OperatorHub</h2>
<p>通过 <code>oc get catalogsource -n openshift-marketplace</code> 可以发现在 openshift-marketplace namespace 下有 redhat-operators, community-operators 和 certified-operators 三个 catalog source. 先禁用这些默认的源, 然后再添加本地离线资源</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">oc patch OperatorHub cluster --<span class="built_in">type</span> json -p <span class="string">&#x27;[&#123;&quot;op&quot;: &quot;add&quot;, &quot;path&quot;: &quot;/spec/disableAllDefaultSources&quot;, &quot;value&quot;: true&#125;]&#x27;</span></span><br></pre></td></tr></table></figure>
<p>此时在 web console 上查看 operatorHub 时可以发现已经没有内容了.</p>
<p><img src="ocp-operatorhub-empty.png" /></p>
<p>通过 <code>oc get catalogsource -n openshift-marketplace</code> 查看显示内容为空.</p>
<div class="note info"><p>此处发现各个节点 kubelet 会依次重启. 如果遇到这个情况, 等各个节点都 Ready 再进行后面的操作</p>
</div>
<p>然后添加本地 catalog source. 这里需要用到前面 <code>oc adm catalog mirror</code> 产生的目录 redhat-operators-manifests</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建 imageContentSourcePolicy</span></span><br><span class="line">oc apply -f redhat-operators-manifests/</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 catalogSource 引用离线资源</span></span><br><span class="line">oc apply -f - &lt;&lt;<span class="string">EOF</span></span><br><span class="line"><span class="string">apiVersion: operators.coreos.com/v1alpha1</span></span><br><span class="line"><span class="string">kind: CatalogSource</span></span><br><span class="line"><span class="string">metadata:</span></span><br><span class="line"><span class="string">  name: vopsdev-redhat-operators</span></span><br><span class="line"><span class="string">  namespace: openshift-marketplace</span></span><br><span class="line"><span class="string">spec:</span></span><br><span class="line"><span class="string">  sourceType: grpc</span></span><br><span class="line"><span class="string">  image: quay.svc.vopsdev.com/devinfra/redhat-operators:v1</span></span><br><span class="line"><span class="string">  displayName: VOPSDEV RedHat Operator Catalog</span></span><br><span class="line"><span class="string">  publisher: grpc</span></span><br><span class="line"><span class="string">EOF</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 会看到自定义的 catalog pod 和 catalog source</span></span><br><span class="line">oc get pods -n openshift-marketplace</span><br><span class="line">oc get catalogsource -n openshift-marketplace</span><br></pre></td></tr></table></figure>
<p>再登录 web console 就可以看到 OperatorHub 下的内容了</p>
<p><img src="ocp-operatorhub-customized.png" /></p>
]]></content>
      <categories>
        <category>RedHat</category>
        <category>Openshift</category>
      </categories>
      <tags>
        <tag>Openshift</tag>
        <tag>RedHat</tag>
      </tags>
  </entry>
  <entry>
    <title>离线部署 Openshift Container Platform 4.3 - 5: 配置 registry 使用持久存储</title>
    <url>/1074344158/</url>
    <content><![CDATA[<p>默认安装完成时 OCP 集成的镜像仓库没有配置持久存储, registry pod 会被自动清理, 管理状态为 Removed. 本文将为 OCP 集成镜像仓库配置持久存储. 这里使用基于 minio 的对象存储</p>
<span id="more"></span>
<h2 id="关于-registry-持久存储">关于 registry 持久存储</h2>
<p>OCP <a href="https://docs.openshift.com/container-platform/4.3/scalability_and_performance/optimizing-storage.html">官方文档</a>建议使用对象存储作为 registry 的后端, 并且一再强调不建议在生产环境中使用 NFS. 然而在 OCP 官方文档的 vSphere 和 Baremetal 的<a href="https://docs.openshift.com/container-platform/4.3/registry/configuring_registry_storage/configuring-registry-storage-vsphere.html">配置样例</a>里又一直采用 NFS 做为 registry 的持久存储, 实在是让人无语.</p>
<p>我的环境有 minio 集群, 通过 endpoint https://s3.svc.vopsdev.com 对外提供服务. 这里将使用 minio 提供的对象存储作为 registry 的后端.</p>
<h2 id="配置-minio">配置 minio</h2>
<h3 id="创建-bucket">创建 bucket</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mc mb minio/ocp-registry</span><br></pre></td></tr></table></figure>
<h3 id="创建用户和策略">创建用户和策略</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># IAM policy</span></span><br><span class="line">cat &gt; ocp-registry-policy.json &lt;&lt;<span class="string">EOF</span></span><br><span class="line"><span class="string">&#123;</span></span><br><span class="line"><span class="string">  &quot;Version&quot;: &quot;2012-10-17&quot;,</span></span><br><span class="line"><span class="string">  &quot;Statement&quot;: [</span></span><br><span class="line"><span class="string">    &#123;</span></span><br><span class="line"><span class="string">      &quot;Action&quot;: [</span></span><br><span class="line"><span class="string">        &quot;s3:GetObject&quot;,</span></span><br><span class="line"><span class="string">        &quot;s3:PutObject&quot;,</span></span><br><span class="line"><span class="string">        &quot;s3:DeleteObject&quot;,</span></span><br><span class="line"><span class="string">        &quot;s3:ListMultipartUploadParts&quot;,</span></span><br><span class="line"><span class="string">        &quot;s3:AbortMultipartUpload&quot;</span></span><br><span class="line"><span class="string">      ],</span></span><br><span class="line"><span class="string">      &quot;Effect&quot;: &quot;Allow&quot;,</span></span><br><span class="line"><span class="string">      &quot;Resource&quot;: [</span></span><br><span class="line"><span class="string">        &quot;arn:aws:s3:::ocp-registry/*&quot;</span></span><br><span class="line"><span class="string">      ],</span></span><br><span class="line"><span class="string">      &quot;Sid&quot;: &quot;&quot;</span></span><br><span class="line"><span class="string">    &#125;,</span></span><br><span class="line"><span class="string">    &#123;</span></span><br><span class="line"><span class="string">      &quot;Action&quot;: [</span></span><br><span class="line"><span class="string">        &quot;s3:ListBucket&quot;,</span></span><br><span class="line"><span class="string">        &quot;s3:GetBucketLocation&quot;,</span></span><br><span class="line"><span class="string">        &quot;s3:ListBucketMultipartUploads&quot;</span></span><br><span class="line"><span class="string">      ],</span></span><br><span class="line"><span class="string">      &quot;Effect&quot;: &quot;Allow&quot;,</span></span><br><span class="line"><span class="string">      &quot;Resource&quot;: [</span></span><br><span class="line"><span class="string">        &quot;arn:aws:s3:::ocp-registry&quot;</span></span><br><span class="line"><span class="string">      ],</span></span><br><span class="line"><span class="string">      &quot;Sid&quot;: &quot;&quot;</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">  ]</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string">EOF</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建用户, 关联策略</span></span><br><span class="line">mc admin policy add minio ocp-registry ocp-registry-policy.json</span><br><span class="line">mc admin policy info minio ocp-registry</span><br><span class="line">mc admin user add minio ocp-registry-id ocp-registry-secret</span><br><span class="line">mc admin policy <span class="built_in">set</span> minio ocp-registry user=ocp-registry-id</span><br><span class="line">mc admin user info minio ocp-registry-id</span><br></pre></td></tr></table></figure>
<h2 id="配置-ocp-registry">配置 OCP registry</h2>
<p>初始状态, 在 openshift-image-registry 下只有 registry operator pod 在运行, 并没有 registry pod</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">oc get pod -n openshift-image-registry</span><br><span class="line">NAME                                               READY   STATUS    RESTARTS   AGE</span><br><span class="line">cluster-image-registry-operator-5d7d64d769-q4tw2   2/2     Running   1          2d1h</span><br></pre></td></tr></table></figure>
<p>在 openshift-image-registry 项目下创建 secret 保存访问对象存储所使用的 access_key 和 secret_key</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">oc create secret generic image-registry-private-configuration-user --from-literal=REGISTRY_STORAGE_S3_ACCESSKEY=ocp-registry-id --from-literal=REGISTRY_STORAGE_S3_SECRETKEY=ocp-registry-secret -n openshift-image-registry</span><br></pre></td></tr></table></figure>
<p>通过<code>oc edit configs.imageregistry.operator.openshift.io/cluster</code>编辑 image registry 的配置, 将管理状态设置为 Managed, 并指定后端对象存储信息:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">spec:</span><br><span class="line">  managementState: Managed</span><br><span class="line">  storage:</span><br><span class="line">    s3:</span><br><span class="line">      bucket: ocp-registry</span><br><span class="line">      region: local</span><br><span class="line">      regionEndpoint: https://s3.svc.vopsdev.com</span><br><span class="line">  routes:</span><br><span class="line">  - name: internal-registry-route</span><br><span class="line">    hostname: registry-int.apps.ocp.vopsdev.com</span><br></pre></td></tr></table></figure>
<p>同时这里也指定了 route 将集成镜像库的服务通过 ingress 暴露出来. 然后就可以看到 image-registry pod 已经起来了</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">oc get pod -n openshift-image-registry</span><br><span class="line">NAME                                               READY   STATUS    RESTARTS   AGE</span><br><span class="line">cluster-image-registry-operator-5d7d64d769-q4tw2   2/2     Running   1          2d1h</span><br><span class="line">image-registry-5cdb47886d-cq6l5                    1/1     Running   0          115s</span><br><span class="line">node-ca-2w9jf                                      1/1     Running   0          116s</span><br><span class="line">node-ca-5mccq                                      1/1     Running   0          116s</span><br><span class="line">node-ca-5vv95                                      1/1     Running   0          116s</span><br><span class="line">node-ca-fqk8q                                      1/1     Running   0          116s</span><br><span class="line">node-ca-gfhvl                                      1/1     Running   0          116s</span><br><span class="line">node-ca-hmr4w                                      1/1     Running   0          116s</span><br></pre></td></tr></table></figure>
<h2 id="测试">测试</h2>
<h3 id="创建本地账户">创建本地账户</h3>
<p>为了测试方便, 给 OCP 配置 HTPasswd 的认证源, 创建几个本地账户</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建 htpasswd 文件</span></span><br><span class="line">htpasswd -cBb /tmp/htpasswd admin admin</span><br><span class="line">htpasswd -Bb /tmp/htpasswd user1 user1</span><br><span class="line">htpasswd -Bb /tmp/htpasswd user2 user2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建包含此 htpsswd 的 secret</span></span><br><span class="line">oc create secret generic htpass-secret --from-file=htpasswd=/tmp/htpasswd -n openshift-config</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置 HTPasswd IDP</span></span><br><span class="line">oc apply -f - &lt;&lt;<span class="string">EOF</span></span><br><span class="line"><span class="string">apiVersion: config.openshift.io/v1</span></span><br><span class="line"><span class="string">kind: OAuth</span></span><br><span class="line"><span class="string">metadata:</span></span><br><span class="line"><span class="string">  name: cluster</span></span><br><span class="line"><span class="string">spec:</span></span><br><span class="line"><span class="string">  identityProviders:</span></span><br><span class="line"><span class="string">  - name: htpasswd_provider</span></span><br><span class="line"><span class="string">    mappingMethod: claim</span></span><br><span class="line"><span class="string">    type: HTPasswd</span></span><br><span class="line"><span class="string">    htpasswd:</span></span><br><span class="line"><span class="string">      fileData:</span></span><br><span class="line"><span class="string">        name: htpass-secret</span></span><br><span class="line"><span class="string">EOF</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 给用户授予角色. 允许 user1 往 sandbox-1 名字空间下推送镜像</span></span><br><span class="line">oc adm policy add-cluster-role-to-user cluster-admin admin</span><br><span class="line">oc new-project sandbox-1</span><br><span class="line">oc policy add-role-to-user registry-editor user1</span><br><span class="line">oc login -u user1</span><br></pre></td></tr></table></figure>
<h3 id="测试-registry">测试 registry</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker login -u user1 -p $(oc whoami -t) registry-int.apps.ocp.vopsdev.com</span><br><span class="line">docker pull alpine</span><br><span class="line">docker tag alpine:latest registry-int.apps.ocp.vopsdev.com/sandbox-1/alpine:latest</span><br><span class="line">docker push registry-int.apps.ocp.vopsdev.com/sandbox-1/alpine:latest</span><br></pre></td></tr></table></figure>
<p>在 minio 的浏览器界面上也可以看到结果</p>
<p><img src="integrated-registry-test.png" /></p>
<p>使用本地帐号 admin 登录 OCP web console, 可以发现针对 image registry 被删除的状态提示已经消失. 目前还提示 Alertmanager 的 notification 没有正确设置, 这部分会在后面监控配置中解决.</p>
<p><img src="ocp-status-after-registry-setup.png" /></p>
]]></content>
      <categories>
        <category>RedHat</category>
        <category>Openshift</category>
      </categories>
      <tags>
        <tag>Openshift</tag>
        <tag>RedHat</tag>
      </tags>
  </entry>
  <entry>
    <title>离线部署 Openshift Container Platform 4.3 - 6: 配置持久存储</title>
    <url>/1571144678/</url>
    <content><![CDATA[<p>本文为 OCP 平台配置动态持久存储, 使用 vSphere Volume 和 Ceph RBD CSI <span id="more"></span></p>
<h2 id="vsphere-volume">vSphere Volume</h2>
<p>如果前面配置都正确, vSphere Volume 此时已经可用. 目前系统里应该只有一种 storageclass: thin(default), 其 provisioner 为 kubernetes.io/vsphere-volume. 建一个 pod 测试一下 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看现有的 storageclass</span></span><br><span class="line">oc get sc</span><br><span class="line">NAME             PROVISIONER                    AGE</span><br><span class="line">thin (default)   kubernetes.io/vsphere-volume   2d19h</span><br><span class="line"></span><br><span class="line">oc project sandbox-1</span><br><span class="line"><span class="comment"># 我这里用的是私有仓库里的离线镜像</span></span><br><span class="line">oc apply -f - &lt;&lt;<span class="string">EOF</span></span><br><span class="line"><span class="string">---</span></span><br><span class="line"><span class="string">apiVersion: v1</span></span><br><span class="line"><span class="string">kind: Pod</span></span><br><span class="line"><span class="string">metadata:</span></span><br><span class="line"><span class="string">  name: vsphere-volume-demo-pod</span></span><br><span class="line"><span class="string">spec:</span></span><br><span class="line"><span class="string">  containers:</span></span><br><span class="line"><span class="string">    - name: web-server</span></span><br><span class="line"><span class="string">      image: quay.svc.vopsdev.com/devinfra/nginx:latest</span></span><br><span class="line"><span class="string">      volumeMounts:</span></span><br><span class="line"><span class="string">        - name: www-data</span></span><br><span class="line"><span class="string">          mountPath: /var/lib/www/html</span></span><br><span class="line"><span class="string">  volumes:</span></span><br><span class="line"><span class="string">    - name: www-data</span></span><br><span class="line"><span class="string">      persistentVolumeClaim:</span></span><br><span class="line"><span class="string">        claimName: vsphere-demo-pvc</span></span><br><span class="line"><span class="string">        readOnly: false</span></span><br><span class="line"><span class="string">---</span></span><br><span class="line"><span class="string">apiVersion: v1</span></span><br><span class="line"><span class="string">kind: PersistentVolumeClaim</span></span><br><span class="line"><span class="string">metadata:</span></span><br><span class="line"><span class="string">  name: vsphere-demo-pvc</span></span><br><span class="line"><span class="string">spec:</span></span><br><span class="line"><span class="string">  accessModes:</span></span><br><span class="line"><span class="string">    - ReadWriteOnce</span></span><br><span class="line"><span class="string">  volumeMode: Filesystem</span></span><br><span class="line"><span class="string">  resources:</span></span><br><span class="line"><span class="string">    requests:</span></span><br><span class="line"><span class="string">      storage: 1Gi</span></span><br><span class="line"><span class="string">  storageClassName: thin</span></span><br><span class="line"><span class="string">EOF</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查一下结果是否正常</span></span><br><span class="line">oc get pod,pvc</span><br><span class="line">NAME                          READY   STATUS    RESTARTS   AGE</span><br><span class="line">pod/vsphere-volume-demo-pod   1/1     Running   0          20s</span><br><span class="line"></span><br><span class="line">NAME                                     STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE</span><br><span class="line">persistentvolumeclaim/vsphere-demo-pvc   Bound    pvc-ae6dbae7-bb1f-460c-87d8-79ccbc01173d   1Gi        RWO            thin           20s</span><br></pre></td></tr></table></figure></p>
<p>前往 vSphere Datastore 下查看产生的 vmdk <img src="ocp-vsphere-volume.png" /></p>
<p>Provisioner 会在 install-config.yaml 文件中指定的 platform.vsphere.defaultDatastore 下创建 kubevols 目录, 并在此目录下创建 vmdk.</p>
<p>如果 pvc 出现长时间 pending, <code>oc describe</code> 提示</p>
<blockquote>
<p>Failed to provision volume with StorageClass "thin": folder 'xxx' not found</p>
</blockquote>
<p>请检查是否在 vSphere Datacenter 下创建了 metadata.name 对应的目录.</p>
<p>vSphere volume 的性能取决于你的 datastore 的性质.</p>
<h2 id="ceph-rbd">Ceph RBD</h2>
<div class="note info"><p>此处仅仅展示 OCP 和现有 Ceph 集群通过 CSI 对接的配置过程. 如果你在裸机上部署 OCP 同时又有现存的 Ceph 集群, 可以考虑这种方式整合. 如果你的 OCP 本身跑在 vSphere 虚拟化平台上, 那还是请使用 vSphere Volume 以确保稳定和性能</p>
</div>
<p>原先在 OCP 3.11, Atomic Host 自带 ceph-common 工具可以访问 Ceph 集群. 但是到了 OCP 4, RHCOS 上不再提供 ceph-common, 此时需要通过 CSI 来和现有的 Ceph 集群对接.</p>
<h3 id="ceph-端准备工作">Ceph 端准备工作</h3>
<p>现有的 Ceph 集群版本 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph version 14.2.4 (75f4de193b3ea58512f204623e6c5a16e6c1e1ba) nautilus (stable)</span><br></pre></td></tr></table></figure> 建一个 rbd pool 给 Openshift 使用, 同时创建 cephx 用户和密钥 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph osd pool create okd 32</span><br><span class="line">ceph osd pool application <span class="built_in">enable</span> okd rbd</span><br><span class="line">ceph auth get-or-create client.okd mon <span class="string">&#x27;profile rbd&#x27;</span> osd <span class="string">&#x27;profile rbd pool=okd&#x27;</span> -o ceph.client.okd.keyring</span><br></pre></td></tr></table></figure></p>
<h3 id="ocp-端">OCP 端</h3>
<p>下载 ceph-csi.git 根据实际环境信息配置后部署 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/ceph/ceph-csi.git</span><br><span class="line"><span class="built_in">cd</span> ceph-csi</span><br><span class="line"><span class="comment"># 当前最新 release 为 2.0.0</span></span><br><span class="line">git checkout v2.0.0</span><br><span class="line"><span class="built_in">cd</span> deploy/rbd/kubernetes</span><br><span class="line"></span><br><span class="line">oc new-project ceph-csi</span><br><span class="line"><span class="comment"># 参考 https://docs.ceph.com/docs/master/rbd/rbd-kubernetes/</span></span><br><span class="line"><span class="comment"># clusterID 可以使用 ceph mon dump 里的 fsid</span></span><br><span class="line">oc apply -f - &lt;&lt;<span class="string">EOF</span></span><br><span class="line"><span class="string">apiVersion: v1</span></span><br><span class="line"><span class="string">kind: ConfigMap</span></span><br><span class="line"><span class="string">data:</span></span><br><span class="line"><span class="string">  config.json: |-</span></span><br><span class="line"><span class="string">    [</span></span><br><span class="line"><span class="string">        &#123;</span></span><br><span class="line"><span class="string">            &quot;clusterID&quot;: &quot;2e44e736-d686-4c86-8cf5-ecb7d91ccf2b&quot;,</span></span><br><span class="line"><span class="string">            &quot;monitors&quot;: [</span></span><br><span class="line"><span class="string">                &quot;192.168.11.21:6789&quot;,</span></span><br><span class="line"><span class="string">                &quot;192.168.11.22:6789&quot;,</span></span><br><span class="line"><span class="string">                &quot;192.168.11.23:6789&quot;</span></span><br><span class="line"><span class="string">            ]</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">    ]</span></span><br><span class="line"><span class="string">metadata:</span></span><br><span class="line"><span class="string">  name: ceph-csi-config</span></span><br><span class="line"><span class="string">  namespace: ceph-csi</span></span><br><span class="line"><span class="string">EOF</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 secret 保存 cephx keyring 认证方式的 user/key </span></span><br><span class="line">oc apply -f - &lt;&lt;<span class="string">EOF</span></span><br><span class="line"><span class="string">apiVersion: v1</span></span><br><span class="line"><span class="string">kind: Secret</span></span><br><span class="line"><span class="string">metadata:</span></span><br><span class="line"><span class="string">  name: csi-rbd-secret</span></span><br><span class="line"><span class="string">  namespace: ceph-csi</span></span><br><span class="line"><span class="string">stringData:</span></span><br><span class="line"><span class="string">  userID: okd</span></span><br><span class="line"><span class="string">  userKey: xxx</span></span><br><span class="line"><span class="string">EOF</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意调整 namespace</span></span><br><span class="line">oc apply -f csi-provisioner-rbac.yaml</span><br><span class="line">oc apply -f csi-nodeplugin-rbac.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调整 scc</span></span><br><span class="line">oc adm policy add-scc-to-user privileged system:serviceaccount:ceph-csi:rbd-csi-nodeplugin</span><br><span class="line">oc adm policy add-scc-to-user privileged system:serviceaccount:ceph-csi:rbd-csi-provisioner</span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意调整 namespace</span></span><br><span class="line"><span class="comment"># 如果是隔离环境, 需要把下面几个镜像离线到私有镜像仓库, 然后调整镜像的位置</span></span><br><span class="line"><span class="comment">#  quay.io/cephcsi/cephcsi:v2.0.0</span></span><br><span class="line"><span class="comment">#  quay.io/k8scsi/csi-provisioner:v1.4.0</span></span><br><span class="line"><span class="comment">#  quay.io/k8scsi/csi-snapshotter:v1.2.2</span></span><br><span class="line"><span class="comment">#  quay.io/k8scsi/csi-attacher:v2.1.0</span></span><br><span class="line"><span class="comment">#  quay.io/k8scsi/csi-resizer:v0.4.0</span></span><br><span class="line"><span class="comment">#  quay.io/k8scsi/csi-node-driver-registrar:v1.2.0</span></span><br><span class="line">oc apply -f csi-rbdplugin-provisioner.yaml</span><br><span class="line">oc apply -f csi-rbdplugin.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 确定 csi-rbdplugin 和 csi-rbdplugin-provisioner pod 都正常起来了</span></span><br><span class="line">oc get all</span><br><span class="line">NAME                                             READY   STATUS    RESTARTS   AGE</span><br><span class="line">pod/csi-rbdplugin-7qqtp                          3/3     Running   0          5m</span><br><span class="line">pod/csi-rbdplugin-f8grj                          3/3     Running   0          5m</span><br><span class="line">pod/csi-rbdplugin-provisioner-77497b775c-25624   6/6     Running   0          5m</span><br><span class="line">pod/csi-rbdplugin-provisioner-77497b775c-2crxf   6/6     Running   0          5m</span><br><span class="line">pod/csi-rbdplugin-provisioner-77497b775c-lhgtl   6/6     Running   0          5m</span><br><span class="line">pod/csi-rbdplugin-x59fc                          3/3     Running   0          5m</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p>
<h3 id="测试">测试</h3>
<p>先创建 storageclass</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">oc apply -f - &lt;&lt;<span class="string">EOF</span></span><br><span class="line"><span class="string">apiVersion: storage.k8s.io/v1</span></span><br><span class="line"><span class="string">kind: StorageClass</span></span><br><span class="line"><span class="string">metadata:</span></span><br><span class="line"><span class="string">   name: csi-rbd-sc</span></span><br><span class="line"><span class="string">provisioner: rbd.csi.ceph.com</span></span><br><span class="line"><span class="string">parameters:</span></span><br><span class="line"><span class="string">   clusterID: 2e44e736-d686-4c86-8cf5-ecb7d91ccf2b</span></span><br><span class="line"><span class="string">   pool: okd</span></span><br><span class="line"><span class="string">   csi.storage.k8s.io/provisioner-secret-name: csi-rbd-secret</span></span><br><span class="line"><span class="string">   csi.storage.k8s.io/provisioner-secret-namespace: ceph-csi</span></span><br><span class="line"><span class="string">   csi.storage.k8s.io/node-stage-secret-name: csi-rbd-secret</span></span><br><span class="line"><span class="string">   csi.storage.k8s.io/node-stage-secret-namespace: ceph-csi</span></span><br><span class="line"><span class="string">reclaimPolicy: Delete</span></span><br><span class="line"><span class="string">mountOptions:</span></span><br><span class="line"><span class="string">   - discard</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure>
<p>然后再创建一个使用 pvc 的 pod 测试 ceph rbd storageclass</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">oc project sandbox-1</span><br><span class="line"></span><br><span class="line">oc apply -f - &lt;&lt;EOF</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: csi-rbd-demo-pod</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">    - name: web-server</span><br><span class="line">      image: quay.svc.vopsdev.com/devinfra/nginx:latest</span><br><span class="line">      volumeMounts:</span><br><span class="line">        - name: www-data</span><br><span class="line">          mountPath: /var/lib/www/html</span><br><span class="line">  volumes:</span><br><span class="line">    - name: www-data</span><br><span class="line">      persistentVolumeClaim:</span><br><span class="line">        claimName: rbd-pvc</span><br><span class="line">        readOnly: false</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">metadata:</span><br><span class="line">  name: rbd-pvc</span><br><span class="line">spec:</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteOnce</span><br><span class="line">  volumeMode: Filesystem</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 1Gi</span><br><span class="line">  storageClassName: csi-rbd-sc</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">oc get pod,pvc</span><br><span class="line">NAME                   READY   STATUS    RESTARTS   AGE</span><br><span class="line">pod/csi-rbd-demo-pod   1/1     Running   0          43s</span><br><span class="line"></span><br><span class="line">NAME                            STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE</span><br><span class="line">persistentvolumeclaim/rbd-pvc   Bound    pvc-57efa24d-acc8-40f5-bb0c-18b77f0d5ec6   1Gi        RWO            csi-rbd-sc     43s</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>RedHat</category>
        <category>Openshift</category>
      </categories>
      <tags>
        <tag>Openshift</tag>
        <tag>RedHat</tag>
      </tags>
  </entry>
  <entry>
    <title>解决 Intel(R) I350-T4 在 vSphere 7 上的高延时问题</title>
    <url>/1151338401/</url>
    <content><![CDATA[<p>记录下如何解决 ESXi 7 主机上的 Intel(R) I350-T4 网卡周期性出现的高延迟丢包问题.</p>
<span id="more"></span>
<h2 id="问题现象">问题现象</h2>
<p>实验环境有四台 Lenovo M920X 运行 ESXi 7.0U1 组成的集群, 使用的是 Intel(R) I350-T4 四口千兆网卡. 差不多每运行一两周后某台主机的某个网口就会出现超高延迟 (~1s) 并伴随丢包.</p>
<p>从 <code>vmkernel.log</code> 中可以发现下面的日志</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">2022-01-09T16:31:39.224Z cpu13:1049182)igbn: igbn_CheckRxHang:1382: vmnic1: false hang detected on RX queue 0</span><br><span class="line">2022-01-09T16:31:45.246Z cpu13:1049182)igbn: igbn_CheckRxHang:1382: vmnic1: false hang detected on RX queue 0</span><br><span class="line">2022-01-09T16:31:51.260Z cpu13:1049182)igbn: igbn_CheckRxHang:1382: vmnic1: false hang detected on RX queue 0</span><br><span class="line">2022-01-09T16:31:57.270Z cpu13:1049182)igbn: igbn_CheckRxHang:1382: vmnic1: false hang detected on RX queue 0</span><br><span class="line">2022-01-09T16:32:03.293Z cpu13:1049182)igbn: igbn_CheckRxHang:1382: vmnic1: false hang detected on RX queue 0</span><br></pre></td></tr></table></figure>
<p>此时插拔网线, 禁用/启用网卡均无效, 只能重启主机才能使该网口恢复正常.</p>
<h2 id="失败的尝试">失败的尝试</h2>
<h3 id="禁用节能模式-energy-efficient-ethernet">禁用节能模式 Energy Efficient Ethernet</h3>
<p>网上搜索到过一个<a href="https://blog.csdn.net/weixin_38623994/article/details/98939579">类似的问题</a>, 不过是在 ESXi 6.5 上的, 由 igbn 1.3.1 版本驱动的 bug 导致. 但是文中提到的选项参数在 ESXi 7 中已经不可用. 尝试在对端物理交换机上禁用节能模式后, 状况并没有显著差别.</p>
<h3 id="替换官方驱动">替换官方驱动</h3>
<p>将 inbox igbn 驱动替换为 Intel 提供的驱动. 使用 <code>vmkchdev -l</code> 来获取设备的 <code>VID:DID SVID:SDID</code>, 然后去 <a href="https://customerconnect.vmware.com">https://customerconnect.vmware.com</a> 搜索适用的驱动下载. 替换驱动并重启主机后, 问题依旧.</p>
<h3 id="调整-tsolro">调整 TSO/LRO</h3>
<p>尝试禁用/启用 TSO/LRO <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">esxcli system settings advanced set -o /Net/UseHwTSO -i 0</span><br><span class="line">esxcli system settings advanced set -o /Net/UseHwTSO6 -i 0</span><br><span class="line">esxcli system settings advanced set -o /Net/TcpipDefLROEnabled -i 0</span><br></pre></td></tr></table></figure></p>
<p>然而结果并没有什么显著差别 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">esxcli system settings advanced set -o /Net/UseHwTSO -i 1</span><br><span class="line">esxcli system settings advanced set -o /Net/UseHwTSO6 -i 1</span><br><span class="line">esxcli system settings advanced set -o /Net/TcpipDefLROEnabled -i 1</span><br></pre></td></tr></table></figure></p>
<h3 id="替换同型号网卡">替换同型号网卡</h3>
<p>从另外一个渠道买入同型号的 Intel(R) I350-T4, 给其中一台主机替换上, 经过一段时间后问题依然重现. 至此可以排除网卡硬件本身的问题.</p>
<h2 id="解决方法">解决方法</h2>
<p>偶然一次检查网卡统计信息时发现有比较大的 Receive FIFO error: <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">esxcli network nic stats get -n vmnic1</span><br><span class="line"></span><br><span class="line">NIC statistics for vmnic1</span><br><span class="line"> ...</span><br><span class="line"> Receive FIFO errors: 834656</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<p>结合 <a href="https://kb.vmware.com/s/article/50121760">KB 50121760</a> 尝试调整 ring buffer</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">esxcli network nic stats get -n vmnic1</span><br><span class="line">esxcli network nic ring preset get -n vmnic1</span><br><span class="line">esxcli network nic ring current get -n vmnic1</span><br><span class="line">esxcli network nic ring current set -n vmnic1 -r 4096 -t 4096</span><br></pre></td></tr></table></figure>
<p>调整完重启主机. 持续运行半年没再出现过类似状况, 至此问题解决.</p>
]]></content>
      <categories>
        <category>VMware</category>
      </categories>
      <tags>
        <tag>VMware</tag>
      </tags>
  </entry>
  <entry>
    <title>从 AWS Landing Zone 迁移至 Control Tower 1: 规划和实施</title>
    <url>/2508560204/</url>
    <content><![CDATA[<p>AWS Landing Zone 方案已经进入长期维护期, 不会再引入任何新的功能. AWS 建议用户使用 Control Tower 方案来管理企业多账户环境. 本文将示例如何从现有的 Landing Zone 方案迁移至 Control Tower 并列举迁移中的注意事项</p>
<span id="more"></span>
<h2 id="迁移之前的状态">迁移之前的状态</h2>
<p>现存的组织单元 OU 结构 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Root</span><br><span class="line">└── vopsdev-landing-zone</span><br><span class="line">    ├── core</span><br><span class="line">    │   ├── yl-master</span><br><span class="line">    │   ├── yl-security</span><br><span class="line">    │   └── yl-log-archive</span><br><span class="line">    ├── production</span><br><span class="line">    ├── test  </span><br><span class="line">    └── department</span><br><span class="line">        ├── yl-itdev</span><br><span class="line">        └── yl-network</span><br></pre></td></tr></table></figure> 由于存在一些 OU 级别的资源共享设置 (Resource Access Manager), 在迁移过程中想尽量保证 OU 结构不变</p>
<p>现存的 Service Control Policy/SCP: <img src="alz-scp.png" /></p>
<p>用于部署核心资源 + 基线资源的 AWS Landing Zone StackSets <img src="alz-stacksets.png" /></p>
<p>Service Catalog 下 Provisioned Product 对应每一个受控的账号 <img src="alz-provisioned-products.png" /></p>
<h2 id="迁移的原理">迁移的原理</h2>
<p>虽然 Control Tower 是从 AWS Landing Zone 发展而来, 部分概念一脉相承, 但是经过多年的发展, 区别也越来越大, 很多资源命名也不一致. 因此这里所谓的 "迁移" 本质上就是删了重建.</p>
<p>原则上先完全删除清理现有的 AWS Landing Zone, 然后创建 Control Tower 是可行的. 但是这样会存在一段时间, 组织既不受 AWS Landing Zone 的管控, 也不受 Control Tower 的管控. 理想的情况是在 AWS Landing Zone 存在的同时, 启用 Control Tower, 然后再清理掉 AWS Landing Zone. 但是由于有一些全局资源会导致冲突 (主要是 config recorder), 因此合理的迁移步骤是:</p>
<ul>
<li>检查 trusted access 设置</li>
<li>删除会导致冲突的 AWS Landing Zone 资源</li>
<li>启用 Control Tower Landing Zone</li>
<li>注册组织单元 (OU) 和账号到 Control Tower Landing Zone</li>
<li>清理 AWS Landing Zone 资源</li>
<li>重建定制的基线资源</li>
</ul>
<h2 id="检查-trusted-access">检查 trusted access</h2>
<p>到 AWS Organization, Services 下检查是否启用了 config 和 cloudtrail 的 trusted access. 如果启用了, 则禁用</p>
<p><img src="org-disable-trusted-access.png" /></p>
<h2 id="删除-config-recorder">删除 config recorder</h2>
<p>在 Cloudformation stackset 下找到 AWS-Landing-Zone-Baseline-EnableConfig, 删除其下的 stack instance,</p>
<p><img src="alz-delete-config-1.png" /></p>
<p>然后删除这个 stackset:</p>
<p><img src="alz-delete-config-2.png" /></p>
<h2 id="部署-control-tower-landing-zone">部署 Control Tower Landing Zone</h2>
<p>在现存的组织单元结构中, AWS Landing Zone 核心账号位于 vopsdev-landing-zone:core 之下. 在 Control Tower 中核心账号改称为 shared accounts, 包含 audit account 和 log archive account, 其所在 OU 被称为 Fundational OU, 只能放在顶级. 可以重用现存的账号作为 shared accounts, 此处将重用 yl-security 作为 audit account, 重用 yl-log-archive 作为 log archive account. Control Tower Landing Zone 创建完成后, shared accounts 会被自动移动到 Fundational OU 下.</p>
<p>通常情况 audit account 和 log archive account 为专用账号, 不建议其中跑其他的工作负载. 但是如果这些账号中跑了额外的工作负载, 又引用了基于 OU 共享的资源, 则应该确保这些资源要共享给新的 Foundational OU 以避免造成对工作负载的影响.</p>
<p>部署 Control Tower Landing Zone 之前, 先从相关 OU 上 detach AWS Landing Zone 的 SCP 以避免干扰. 然后在 Control Tower 控制台下选择 Set up landing zone</p>
<p><img src="act-setup-lz-1.png" /></p>
<p>选择 home region, 这里是 us-west-2</p>
<p><img src="act-setup-lz-home-region.png" /></p>
<p>保持 region deny setting 默认, 不启用. 再添加额外的 workload region, 此处添加了 eu-west-1</p>
<p><img src="act-setup-lz-additional-regions.png" /></p>
<p>下一步设置 OU. 首先是 Foundational OU, 这里使用了默认的名字 Security. 选择不创建其他 OU, 因为后面会将现存的 OU 注册进来.</p>
<p><img src="act-setup-lz-ou.png" /></p>
<p>下一步设置 shared accounts. 可以使用现存的账号, 选择 Use existing account 填写 account id 即可. 需要说明的是, 虽然账号可以重用, 资源都是新建的, 即新的 organization-level trail, 新的 logging S3 bucket 等.</p>
<p><img src="act-setup-lz-shared-accounts.png" /></p>
<p>Additional configurations 中按需要设置 AWS IAM Identity Center (SSO), organization level trail, logging retention, KMS 加密. 由于此环境中的 SSO 为单独配置, 选择 self-managed 跳过 Control Tower 的配置即可, 其他保持默认.</p>
<p>最后检查一遍配置, 没问题了勾选 I understand ... 后启动 Set up landing zone. 等待设置完成:</p>
<p><img src="act-setup-lz-wait.png" /></p>
<p>Control Tower Landing Zone 设置成功以后, 浏览一下 controls library 中提供的 controls (guardrails). 相比于 AWS Landing Zone, Control Tower 根据最佳实践和各种安全标准提供了大量预定义 SCP/Config Rules. 按照 Guidance 类型, controls 可以分为 Mandatory, Elective 和 Strongly recommended 三类. 默认只启用了 Mandatory 类型, 用来保护 Control Tower 本身资源的完整性. 可以按照需要从 Elective 和 Strongly recommended 中选择部分启用.</p>
<h2 id="注册组织单元和账号">注册组织单元和账号</h2>
<p>Control Tower Landing Zone 设置完成后, 可以将现有的 OU 注册进来. 现在 Control Tower Landing Zone 已经支持 nested OU 树形结构, 但是需要注意:</p>
<ul>
<li>注册 OU 并不是递归的, 先注册 parent OU 后才能注册 child OU</li>
<li>注册 OU 会自动注册其下直属账号</li>
</ul>
<p>因此这里先注册 vopsdev-landing-zone OU, 到 Control Tower 的 Organization 下</p>
<p><img src="act-register-ou.png" /></p>
<p>然后同样的方法注册其下的 department/test/production OUs.</p>
<h2 id="清理-aws-landing-zone-资源">清理 AWS Landing Zone 资源</h2>
<p>将现存的 OU 注册到 Control Tower Landing Zone 之后, 启用的 controls 就会应用到受管控的 OU 及其下属账户上. 接着就可以清理之前 AWS Landing Zone 的资源了. 如果之前使用 AWS Landing Zone 来设置各个账号的 primary VPC, 则需要先将这些 VPC 内的资源迁移掉, 其复杂程度完全取决于这些 VPC 所承载的工作负载. 这里的示例环境并没有这种情形, 因此直接可以清理其他资源</p>
<p>为了避免在清理过程中有人误操作把 AWS Landing Zone 的资源重新部署出来, 可以到 Codepipeline 中禁用 AWS Landing Zone 流水线 source 阶段到 build 阶段的转换.</p>
<h3 id="终止-service-catalog-中-avm-的-provisioned-products">终止 Service Catalog 中 AVM 的 provisioned products</h3>
<p>到 Service Catalog, Provisioned products 下面, 找到 production name 为 AWS-Landing-Zone-Account-Vending-Machine 那些条目, 选择 action, terminate.</p>
<p><img src="alz-cleanup-provisioned-product.png" /></p>
<p>Production name 为 AWS Control Tower Account Factory 的那些 provisioned product 条目来自 Control Tower 的 Account Factory. 每一个注册进来的非 shared service account 会对应一个. 不要误删了.</p>
<h3 id="清理-aws-landing-zone-baseline-resource-stacksets">清理 AWS Landing Zone Baseline resource stacksets</h3>
<p>到 Cloudformation stacksets 下逐个清理名为 AWS-Landing-Zone-Baseline-* 的 stackset. 先删除下面的 stack instances, 再删除 stackset. 如果之前终止 provisioned products 运行顺利, 所有的 stack instances 应该都已经被删除, 此时直接删除 stackset 即可.</p>
<h3 id="删除-service-catalog-中的-baseline-portfolio-与-avm-product">删除 Service Catalog 中的 Baseline portfolio 与 AVM product</h3>
<p>到 Service Catalog, Product list 下选中 AWS-Landing-Zone-Account-Vending-Machine, action 选择 delete product</p>
<p><img src="alz-cleanup-avm.png" /></p>
<p>到 Portfolios 下面, 选择 AWS Landing Zone - Baseline, 先删除其下关联的 product/constraints/access, 然后删除该 portfolio.</p>
<p><img src="alz-cleanup-baseline-portfolio.png" /></p>
<p>其他的 portfolio 会在后面通过删除 landing zone initiation stack 删除</p>
<h3 id="清理-aws-landing-zone-core-resource-stacksets">清理 AWS Landing Zone Core resource stacksets</h3>
<p>继续到 Cloudformation stacksets 下清理名为 AWS-Landing-Zone-* 的 stackset. 同样先删除下面的 stack instances, 再删除 stackset.</p>
<p>清理 AWS-Landing-Zone-SharedBucket 这个 stackset 并不会自动删除 AWS Landing Zone 的 log archive bucket + access log bucket, 需要去 log archive account 下手工清理.</p>
<h3 id="删除-aws-landing-zone-initiation-stack">删除 AWS Landing Zone initiation stack</h3>
<p>在 Cloudformation stacks 下删除 AWS Landing Zone 的 initiation stack. 完成后 AWS Landing Zone 相关的 codebuild, codepipeline, step function, service catalog portfolio 等资源都会被删除.</p>
<h3 id="清理其他剩余的资源">清理其他剩余的资源</h3>
<p>到 Systems Manager 下的 Parameter Store 根据描述信息删除属于 AWS Landing Zone 的参数. 到 Cloudwatch Log groups 下删除 AWS Landing Zone 相关的日志组.</p>
<h2 id="重建定制的基线资源">重建定制的基线资源</h2>
<p>如果在 AWS Landing Zone 中添加了自定义的基线资源, 则需要通过 Control Tower Landing Zone 的定制化重建出来, 这部分将放到下一篇中.</p>
<h2 id="参考">参考</h2>
<ul>
<li><a href="https://docs.aws.amazon.com/prescriptive-guidance/latest/aws-control-tower/introduction.html" class="uri">https://docs.aws.amazon.com/prescriptive-guidance/latest/aws-control-tower/introduction.html</a></li>
<li><a href="https://docs.aws.amazon.com/controltower/latest/userguide/alz-to-control-tower.html" class="uri">https://docs.aws.amazon.com/controltower/latest/userguide/alz-to-control-tower.html</a></li>
<li><a href="https://aws.amazon.com/blogs/mt/migrate-aws-landing-zone-solution-to-aws-control-tower/" class="uri">https://aws.amazon.com/blogs/mt/migrate-aws-landing-zone-solution-to-aws-control-tower/</a></li>
</ul>
]]></content>
      <categories>
        <category>AWS</category>
        <category>Landing Zone</category>
        <category>Control Tower</category>
      </categories>
      <tags>
        <tag>AWS</tag>
        <tag>Landing Zone</tag>
        <tag>Control Tower</tag>
      </tags>
  </entry>
  <entry>
    <title>从 AWS Landing Zone 迁移至 Control Tower 2: 重建定制资源</title>
    <url>/3318020405/</url>
    <content><![CDATA[<p>AWS Control Tower 定制化有 CfCT (customization for control tower) 和 AFT (Account Factory for Terraform) 的方式. 这里将介绍如何使用 CfCT 方式为 Control Tower Landing Zone 添加资源.</p>
<span id="more"></span>
<h2 id="对比-cfct-和-alz-自定义基线资源">对比 CfCT 和 ALZ 自定义基线资源</h2>
<p>通过 CfCT 可以为 Control Tower Landing Zone 添加 preventive/detective control 和其他资源. 其中 preventive control 通过 service control policy 的 json 来定义. 而 detective control 和其他资源则通过 cloudformation stackset 来定义.</p>
<div class="note info"><p>Control Tower 一共有三种 control: preventive control 由 service control policy 实现, detective control 由 config rule 实现, proactive control 由 cloudformation hook 实现.</p>
</div>
<p>CfCT 定制化配置的目录结构如下: <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">.</span><br><span class="line">├── manifest.yaml       # tempates 的输入参数直接写在这里, 不再需要 parameters 目录了</span><br><span class="line">├── policies/           # SCP policy jsons</span><br><span class="line">│    ├── xxx.json       </span><br><span class="line">│    └── yyy.json</span><br><span class="line">└── templates/          # cloudformation stacksets</span><br><span class="line">    ├── aaa.template 		    </span><br><span class="line">    └── bbb.template</span><br></pre></td></tr></table></figure></p>
<p>如果熟悉 AWS Landing Zone 的定制化配置, 可以发现这个目录结构几乎完全一致. ALZ 的 stackset 输入参数需要写入 json 文件放到一个单独的 parameters 目录结构中. 而 CfCT 简化了这部分, 可以将输入参数直接写在 manifest.yaml 中, 不再需要 parameters 目录.</p>
<p>下图来自 AWS CfCT 官方文档</p>
<p><img src="customizations-for-aws-control-tower-architecture-diagram.png" /></p>
<p>从这里也可以看到其基本架构/实现原理和 ALZ 也是类似, 都是通过 codepipeline 触发 step functions 创建 stackset 或者 scp.</p>
<h2 id="部署-customizations-for-aws-control-tower-模板">部署 customizations-for-aws-control-tower 模板</h2>
<p>下载 <a href="https://github.com/aws-solutions/aws-control-tower-customizations/blob/main/customizations-for-aws-control-tower.template">customizations-for-aws-control-tower.template</a>, 使用这个模板文件创建 cloudformation stack, 来生成 CfCT 所需要的基础资源:</p>
<ul>
<li>codebuild</li>
<li>codepipeline</li>
<li>event bridge rule</li>
<li>lambda</li>
<li>sqs queue</li>
<li>s3 bucket</li>
<li>step function</li>
</ul>
<p>输入参数中将 AWS CodePipeline Source 从默认的 AWS S3 改为 AWS CodeCommit, 使得将来可以通过配置代码的变更来触发流水线.</p>
<h2 id="编写并提交资源定义清单">编写并提交资源定义清单</h2>
<p>从 AWS Codecommit 中将 custom-control-tower-configuration 项目 clone 下来, 添加自定义配置. 这里用一个简单的例子示例如何添加 preventive control + detective control + IAM role 资源.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> custom-control-tower-configuration</span><br><span class="line">tree .</span><br><span class="line"></span><br><span class="line">.</span><br><span class="line">├── manifest.yaml</span><br><span class="line">├── policies</span><br><span class="line">│   └── preventive-guardrails.json</span><br><span class="line">└── templates</span><br><span class="line">    ├── access-keys-rotated.template</span><br><span class="line">    └── describe-regions-iam-role.template</span><br></pre></td></tr></table></figure>
<p><code>manifest.yaml</code> 文件内容 <figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">region:</span> <span class="string">us-west-2</span></span><br><span class="line"><span class="attr">version:</span> <span class="number">2021-03-15</span></span><br><span class="line"></span><br><span class="line"><span class="attr">resources:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">test-preventive-guardrails</span></span><br><span class="line">  <span class="attr">description:</span> <span class="string">Prevent</span> <span class="string">deleting</span> <span class="string">or</span> <span class="string">disabling</span> <span class="string">resources</span> <span class="string">in</span> <span class="string">member</span> <span class="string">accounts</span></span><br><span class="line">  <span class="attr">resource_file:</span> <span class="string">policies/preventive-guardrails.json</span></span><br><span class="line">  <span class="attr">deploy_method:</span> <span class="string">scp</span></span><br><span class="line">  <span class="attr">deployment_targets:</span></span><br><span class="line">    <span class="attr">organizational_units:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">vopsdev-landing-zone:department</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">vopsdev-landing-zone:production</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">vopsdev-landing-zone:test</span></span><br><span class="line"></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">create-iam-role</span></span><br><span class="line">  <span class="attr">resource_file:</span> <span class="string">templates/describe-regions-iam-role.template</span></span><br><span class="line">  <span class="attr">deploy_method:</span> <span class="string">stack_set</span></span><br><span class="line">  <span class="attr">deployment_targets:</span></span><br><span class="line">    <span class="attr">organizational_units:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">vopsdev-landing-zone:department</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">vopsdev-landing-zone:production</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">vopsdev-landing-zone:test</span></span><br><span class="line">  <span class="attr">regions:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">us-west-2</span></span><br><span class="line"></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">rotate-access-keys-guardrail</span></span><br><span class="line">  <span class="attr">resource_file:</span> <span class="string">templates/access-keys-rotated.template</span></span><br><span class="line">  <span class="attr">parameters:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">parameter_key:</span> <span class="string">maxAccessKeyAge</span></span><br><span class="line">    <span class="attr">parameter_value:</span> <span class="string">&#x27;24&#x27;</span></span><br><span class="line">  <span class="attr">deploy_method:</span> <span class="string">stack_set</span></span><br><span class="line">  <span class="attr">deployment_targets:</span></span><br><span class="line">    <span class="attr">organizational_units:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">vopsdev-landing-zone:department</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">vopsdev-landing-zone:production</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">vopsdev-landing-zone:test</span></span><br><span class="line">  <span class="attr">regions:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">us-west-2</span></span><br></pre></td></tr></table></figure> 其结构和 ALZ manifest.yaml 的 baseline_resources 部分很相似</p>
<ul>
<li>各个资源会进一步使用 resource_file 引用其定义清单文件: scp 定义保存在 policies 目录下, stackset 保存在 templates 目录下. 实际上 resource_file 还可以引用保存在 s3 上的文件.</li>
<li>deploy method 有 scp 和 stackset 两种. preventive control 使用 scp, detective control 和其他资源使用 stackset</li>
<li>deoployment_targets 除了指定 organizational_units 外还可以指定 accounts</li>
<li>region 指定 stackset 资源需要往那些 region 创建 stack instance. 对于全局资源, 例如 IAM role, 只需要在 home region 创建.</li>
</ul>
<p><code>policies/preventive-guardrails.json</code> 是一个 SCP 的 json 定义文件. 它将创建用户定制的 preventive control 来阻止对某些资源的特定操作: <figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;Version&quot;</span>: <span class="string">&quot;2012-10-17&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;Statement&quot;</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">&quot;Sid&quot;</span>: <span class="string">&quot;GuardPutAccountPublicAccessBlock&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;Effect&quot;</span>: <span class="string">&quot;Deny&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;Action&quot;</span>: <span class="string">&quot;s3:PutAccountPublicAccessBlock&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;Resource&quot;</span>: <span class="string">&quot;arn:aws:s3:::*&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">&quot;Sid&quot;</span>: <span class="string">&quot;GuardEMRPutBlockPublicAccess&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;Effect&quot;</span>: <span class="string">&quot;Deny&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;Action&quot;</span>: <span class="string">&quot;elasticmapreduce:PutBlockPublicAccessConfiguration&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;Resource&quot;</span>: <span class="string">&quot;*&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">&quot;Sid&quot;</span>: <span class="string">&quot;GuardGlacierDeletion&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;Effect&quot;</span>: <span class="string">&quot;Deny&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;Action&quot;</span>: [</span><br><span class="line">        <span class="string">&quot;glacier:DeleteArchive&quot;</span>,</span><br><span class="line">        <span class="string">&quot;glacier:DeleteVault&quot;</span></span><br><span class="line">      ],</span><br><span class="line">      <span class="attr">&quot;Resource&quot;</span>: <span class="string">&quot;arn:aws:glacier:*:*:vaults/*&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">&quot;Sid&quot;</span>: <span class="string">&quot;GuardKMSActions&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;Effect&quot;</span>: <span class="string">&quot;Deny&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;Action&quot;</span>: [</span><br><span class="line">        <span class="string">&quot;kms:DeleteAlias&quot;</span>,</span><br><span class="line">        <span class="string">&quot;kms:DeleteImportedKeyMaterial&quot;</span>,</span><br><span class="line">        <span class="string">&quot;kms:ScheduleKeyDeletion&quot;</span></span><br><span class="line">      ],</span><br><span class="line">      <span class="attr">&quot;Resource&quot;</span>: <span class="string">&quot;*&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><code>templates/access-keys-rotated.template</code> 是一个 stackset 定义文件. 它将创建用户定制的 detective control (即 config rule) 来检查未定期轮滚的密钥: <figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">AWSTemplateFormatVersion:</span> <span class="string">&#x27;2010-09-09&#x27;</span></span><br><span class="line"><span class="attr">Description:</span> <span class="string">Checks</span> <span class="string">whether</span> <span class="string">the</span> <span class="string">active</span> <span class="string">access</span> <span class="string">keys</span> <span class="string">are</span> <span class="string">rotated</span> <span class="string">within</span> <span class="string">the</span> <span class="string">number</span> <span class="string">of</span> <span class="string">days</span> <span class="string">specified</span> <span class="string">in</span> <span class="string">maxAccessKeyAge.</span></span><br><span class="line"><span class="attr">Parameters:</span></span><br><span class="line">  <span class="attr">maxAccessKeyAge:</span></span><br><span class="line">    <span class="attr">Type:</span> <span class="string">&#x27;String&#x27;</span></span><br><span class="line">    <span class="attr">Description:</span> <span class="string">&#x27;Maximum number of days within which the access keys must be rotated. The default value is 90 days.&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="attr">Resources:</span></span><br><span class="line">  <span class="attr">CustomConfigRule:</span></span><br><span class="line">    <span class="attr">Type:</span> <span class="string">AWS::Config::ConfigRule</span></span><br><span class="line">    <span class="attr">Properties:</span></span><br><span class="line">      <span class="attr">ConfigRuleName:</span> <span class="string">ACCESS_KEYS_ROTATED</span></span><br><span class="line">      <span class="attr">Description:</span> <span class="string">Custom</span> <span class="string">Config</span> <span class="string">Rule</span> <span class="bullet">-</span> <span class="string">Checks</span> <span class="string">whether</span> <span class="string">the</span> <span class="string">active</span> <span class="string">access</span> <span class="string">keys</span> <span class="string">are</span> <span class="string">rotated</span> <span class="string">within</span> <span class="string">the</span> <span class="string">number</span> <span class="string">of</span> <span class="string">days</span> <span class="string">specified</span> <span class="string">in</span> <span class="string">maxAccessKeyAge</span></span><br><span class="line">      <span class="attr">InputParameters:</span></span><br><span class="line">        <span class="attr">maxAccessKeyAge :</span> <span class="type">!Ref</span> <span class="string">maxAccessKeyAge</span></span><br><span class="line">      <span class="attr">Source:</span></span><br><span class="line">        <span class="attr">Owner:</span> <span class="string">AWS</span></span><br><span class="line">        <span class="attr">SourceIdentifier:</span> <span class="string">ACCESS_KEYS_ROTATED</span></span><br><span class="line">      <span class="attr">Scope:</span></span><br><span class="line">        <span class="attr">ComplianceResourceTypes:</span> []</span><br></pre></td></tr></table></figure></p>
<p><code>templates/describe-regions-iam-role.template</code> 是一个 stackset 定义文件. 它将创建一个定制资源 (IAM role): <figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">Resources:</span></span><br><span class="line">  <span class="attr">TestLambdaRole:</span></span><br><span class="line">    <span class="attr">Type:</span> <span class="string">AWS::IAM::Role</span></span><br><span class="line">    <span class="attr">Metadata:</span></span><br><span class="line">      <span class="attr">cfn_nag:</span></span><br><span class="line">        <span class="attr">rules_to_suppress:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">id:</span> <span class="string">W28</span></span><br><span class="line">            <span class="attr">reason:</span> <span class="string">&quot;for demo&quot;</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">id:</span> <span class="string">F3</span></span><br><span class="line">            <span class="attr">reason:</span> <span class="string">&quot;for demo&quot;</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">id:</span> <span class="string">W11</span></span><br><span class="line">            <span class="attr">reason:</span> <span class="string">&quot;for demo&quot;</span></span><br><span class="line">    <span class="attr">Properties:</span></span><br><span class="line">      <span class="attr">AssumeRolePolicyDocument:</span></span><br><span class="line">        <span class="attr">Version:</span> <span class="string">&#x27;2012-10-17&#x27;</span></span><br><span class="line">        <span class="attr">Statement:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">Effect:</span> <span class="string">Allow</span></span><br><span class="line">            <span class="attr">Principal:</span></span><br><span class="line">              <span class="attr">Service:</span> <span class="string">lambda.amazonaws.com</span></span><br><span class="line">            <span class="attr">Action:</span> <span class="string">sts:AssumeRole</span></span><br><span class="line">      <span class="attr">Path:</span> <span class="string">/</span></span><br><span class="line">      <span class="attr">RoleName:</span> <span class="string">TestLambdaRole</span></span><br><span class="line">      <span class="attr">Policies:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">PolicyName:</span> <span class="string">DescribeRegions</span></span><br><span class="line">          <span class="attr">PolicyDocument:</span></span><br><span class="line">            <span class="attr">Version:</span> <span class="string">&#x27;2012-10-17&#x27;</span></span><br><span class="line">            <span class="attr">Statement:</span></span><br><span class="line">              <span class="bullet">-</span> <span class="attr">Effect:</span> <span class="string">Allow</span></span><br><span class="line">                <span class="attr">Action:</span></span><br><span class="line">                  <span class="bullet">-</span> <span class="string">ec2:DescribeRegions</span></span><br><span class="line">                <span class="attr">Resource:</span> <span class="string">&#x27;*&#x27;</span></span><br></pre></td></tr></table></figure></p>
<h2 id="测试-cfct-定制流水线和资源可用性">测试 CfCT 定制流水线和资源可用性</h2>
<p>提交配置代码, 推送到 Codecommit 后即可触发 Custom-Control-Tower-CodePipeline 流水线. 等其执行成功:</p>
<p><img src="cfct-pipeline.png" /></p>
<p>然后可以检查新定义的资源是否创建成功. 到 Organization 下指定的 OU 上检查定制 SCP:</p>
<p><img src="cfct-custom-preventive-control.png" /></p>
<p>在 Cloudformation 中可以看到相关的 stacksets:</p>
<p><img src="cfct-stacksets.png" /></p>
<p>到目标账户下查看定制的 config rule:</p>
<p><img src="cfct-custom-detective-control.png" /></p>
<p>到目标账户下查看定制的 IAM role</p>
<p><img src="cfct-custom-resource.png" /></p>
<h2 id="参考">参考</h2>
<ul>
<li><a href="https://docs.aws.amazon.com/controltower/latest/userguide/customize-landing-zone.html" class="uri">https://docs.aws.amazon.com/controltower/latest/userguide/customize-landing-zone.html</a></li>
<li><a href="https://catalog.workshops.aws/control-tower/en-US/customization/cfct" class="uri">https://catalog.workshops.aws/control-tower/en-US/customization/cfct</a></li>
</ul>
]]></content>
      <categories>
        <category>AWS</category>
        <category>Landing Zone</category>
        <category>Control Tower</category>
      </categories>
      <tags>
        <tag>AWS</tag>
        <tag>Landing Zone</tag>
        <tag>Control Tower</tag>
        <tag>CfCT</tag>
      </tags>
  </entry>
  <entry>
    <title>迁移 NSX-V 到 NSX-T: 方案设计</title>
    <url>/2005802497/</url>
    <content><![CDATA[<p>NSX-V 将在 2024 年 1 月停止支持, 在 NSX-V 上运行工作负载的企业会被建议尽快迁移到 NSX-T. 在一个长期稳定运行的企业环境中更换其网络底座通常是一个相当麻烦的过程. 如何尽量减少网络迁移对业务的影响, 将会是很多企业需要考量的问题. 这里将使用一个实际的例子给出不同的迁移方案, 并分析各个方案的复杂性和宕机时间.</p>
<span id="more"></span>
<h2 id="设计和现有-nsx-v-等价的-nsx-t-拓扑">设计和现有 NSX-V 等价的 NSX-T 拓扑</h2>
<p>下图左侧是一个现存的 NSX-V 环境</p>
<p><img src="V2T.png" /></p>
<p>简单介绍一下这个拓扑:</p>
<ul>
<li>有三个承载业务的逻辑网段 (Seg-A, Seg-B 和 Seg-C)</li>
<li>逻辑网段 Seg-C 直接连接到 Edge Service Gateway (nsx-esg)
<ul>
<li>在 nsx-esg 上启用了 inline 模式的四层负载均衡器, 将流量负载到 Seg-C 上的服务器. 因为这些服务器上运行的应用需要能直接看到客户端 IP, 所以这个负载衡器使用了透明模式. 使用透明模式后, 后端服务器的网关得指向负载均衡器</li>
</ul></li>
<li>逻辑网段 Seg-A 和 Seg-B 连接到 Logical Distributed Router (nsx-ldr)</li>
<li>第二个 Edge Service Gateway (nsx-internal-lb) 上启用了 one-arm 模式的七层负载均衡器, 连接到 Seg-B, 将流量负载到 Seg-B 上的应用服务器</li>
<li>nsx-esg 的默认路由指向物理路由器, 并且 nsx-esg 上启用了基于路由的 ipsec VPN 和企业的 VPN hub 连接来接入企业内网</li>
<li>nsx-esg 和 nsx-ldr 连接到 Transit 逻辑网段, 通过 ospf 进行路由交换/重分布</li>
</ul>
<p>NSX-T 和 NSX-V 之间有很大区别, 其设计相当灵活. 如何将现有的 NSX-V 拓扑转化为功能上等价的 NSX-T 拓扑将会是迁移过程中一项最重要的工作, 相当考验架构师对这两个产品和业务环境的理解. 这里不再展开, 右侧就是等价的 NSX-T 拓扑</p>
<ul>
<li>Seg-C 连接到 tier-1 gateway (corp-t1-ext) 将其作为网关, inline 模式的四层透明负载均衡器将关联到 corp-t1-ext. corp-t1-ext 上联到 tier-0 gateway (corp-t0-gw)</li>
<li>Seg-A 和 Seg-B 连接到 corp-t0-gw 将其作为网关</li>
<li>第二个 tier-1 gateway (corp-t1-standalone) 连接到 Seg-B, one-arm 模式的七层负载均衡器关联到 corp-t1-standalone</li>
<li>corp-t0-gw 使用 HA VIP 连接到物理世界, 其默认路由指向物理路由器. 基于路由的 ipsec VPN 服务将关联到 corp-t0-gw, 接入企业内网</li>
</ul>
<div class="note info"><p>注意: 在当前 NSX-T 版本 (4.1) 中, 负载均衡器必须连接到 tier-1 gateway</p>
</div>
<h2 id="可选的迁移方案">可选的迁移方案</h2>
<p>整体来说有两类方案. 第一类是就地迁移 (in-place migration), 在原有的基础设施上直接将 NSX-V 替换为 NSX-T. VMware 提供了 Migration Coordinator 工具来协助用户完成这种迁移. 第二类是另外准备一套 NSX-T 环境, 然后使用 enhanced vMotion 将工作负载迁移过去 (lift and shift). 这两类方法都有一些技术细节需要考量.</p>
<p>对于就地迁移, Migration Coordinator 的全自动迁移模式仅支持有限几个非常简单的拓扑结构, 基本上没法使用到实际的企业环境. 即使后来 Migration Coordinator 支持了用户自定义拓扑, 它需要用户指定 ESG 和 tier-X gateway 之间的映射关系. 然而从上面的例子可以看出 ESG 和 tier-X gateway 并不见得就能对应起来 (nsx-esg 的 LB 和 VPN 分别拆到了 tier-1 和 tier-0 上). 此时就需要用户在使用 Migration Coordinator 的同时, 自己也要完成一部分迁移工作. 另外在 Migration Coordinator 迁移 Edge 和 Host 阶段有流量中断. 取决于 Host 数量的多少和并行度设置, 这个停机时间可能会达到几十分钟.</p>
<p>对于 lift and shift 方式, NSX-T 和 NSX-V 的逻辑网段互相独立, 因此为了保证业务的持续性, 需要将 NSX-V 逻辑网段桥接到 NSX-T, 并保证初始时网关只存在于 NSX-V. 在最终迁移完成后, 切换网关到 NSX-T. 这种方法需要准备额外的基础设施, 但是停机时间仅存在于最后切换网关阶段, 这个时间可以很短.</p>
<p>后面将针对 Migration Coordinator 和 lift and shift 迁移方式分别展开</p>
]]></content>
      <categories>
        <category>VMware</category>
        <category>NSX-T</category>
        <category>NSX-V</category>
      </categories>
      <tags>
        <tag>VMware</tag>
        <tag>NSX-T</tag>
        <tag>NSX-V</tag>
      </tags>
  </entry>
  <entry>
    <title>迁移 NSX-V 到 NSX-T: Migration Coordinator 方案 - 1 准备</title>
    <url>/3271280561/</url>
    <content><![CDATA[<p>本文将介绍如何使用 Migration Coordinator 的用户自定义拓扑模式来迁移现有的 NSX-V 到 NSX-T 环境的第一部分, 迁移前的准备工作.</p>
<span id="more"></span>
<h2 id="迁移中需要考虑的要点">迁移中需要考虑的要点</h2>
<p>迁移前后的拓扑</p>
<p><img src="V2T.png" /></p>
<p>关于物理网络的一些额外的配置信息</p>
<ul>
<li>overlay transport vlan: 12</li>
<li>NSX-V ESG 通过 vlan 11 连接到外部物理世界</li>
</ul>
<p>这里将进行完全的"就地迁移", 保持外部 ip 地址/dns条目/vpn peering 地址不变. 即使使用用户自定义拓扑, 仅依靠 Migration Coordinator 本身依旧无法完成 NSX-V 到 NSX-T 的转化:</p>
<ul>
<li>有部分 NSX-V 特性不支持通过 MC 进行迁移, 具体可以查看 NSX Migration Guide</li>
<li>MC 要求用户指定 ESG 到 Tier-X gateway 的映射, 而这样的映射关系并不一定存在</li>
</ul>
<p>因此对于 MC 无法完成的部分, 或者不想让 MC 自动完成的部分, 用户都可以自行配置:</p>
<ul>
<li>创建 transport zones</li>
<li>创建/配置 edge cluster</li>
<li>创建连接到外部物理 vlan 的逻辑网段</li>
<li>创建/配置 tier-X gateways</li>
<li>创建/配置负载均衡器</li>
</ul>
<p>MC 本身可以自动创建 edge cluster, 这里选择自行部署仅仅是为了获得更大的灵活性 (使用专用的 host 来承载 edge cluster). 根据需要可以在启动 MC 流程之前就配置一部分, 然后在 MC 执行过程中可以暂停流程, 进行额外手工配置后继续 MC 流程.</p>
<h2 id="提前准备-nsx-t-的部分配置">提前准备 NSX-T 的部分配置</h2>
<p>本环境将使用专门的主机来部署 edge cluster, 即承载 edge node 的 ESXi 主机本身不作为传输节点, 因此 edge node TEP 和传输节点可以使用相同 vlan (12) 的地址池.</p>
<h3 id="准备-edge-cluser">准备 Edge Cluser</h3>
<p>先创建 Edge TEP 地址池</p>
<p><img src="01-edge-pool.png" /></p>
<p>创建 Edge Uplink Profile</p>
<p><img src="02-edge-uplink.png" /></p>
<p>创建 VLAN 传输区域和 Overlay 传输区域</p>
<p><img src="03-transport-zone.png" /></p>
<p>单独部署 Edge 节点</p>
<p><img src="04-deploy-edge-nodes.png" /></p>
<p>配置 Edge 节点</p>
<p><img src="05-configure-edge-node.png" /></p>
<p>创建 Edge Cluster</p>
<p><img src="07-edge-cluster.png" /></p>
<h3 id="创建连接到物理世界的逻辑网段">创建连接到物理世界的逻辑网段</h3>
<p>正常情况下虚拟环境使用 vlan 11 连接到物理世界, 需要创建一个对应的 vlan-backed segment. 由于这里想实现完全的就地迁移, 保留所有的外部地址, 为了防止迁移过程对外部环境的影响 (地址冲突), 初始故意设置了错误的 vlan id (0) 将 NSX-T 环境和物理环境隔离开, 迁移完成后再改回来</p>
<p><img src="08-svc-vlan-segment.png" /></p>
<h3 id="创建-tier-0tier-1-gateways">创建 Tier-0/Tier-1 Gateways</h3>
<p>创建 tier-0 gateway: corp-t0-gw, A/S 模式</p>
<p><img src="09-create-corp-t0-gw.png" /></p>
<p>创建 tier-1 gateway: corp-t1-ext, A/S 模式, 连接到 corp-t0-gw</p>
<p><img src="10-create-corp-t1-ext.png" /></p>
<p>创建 tier-1 gateway: corp-t1-standalone, A/S 模式, 不连接到任何 tier-0</p>
<p><img src="11-create-corp-t1-standalone.png" /></p>
<h3 id="配置-tier-0-gateway">配置 Tier-0 Gateway</h3>
<p>能提前配置的部分可以尽量先提前配置出来.</p>
<p>配置 corp-t0-gw 外部接口</p>
<p><img src="12-1-config-corp-t0-gw-external-interfaces.png" /></p>
<p>配置 HA-VIP</p>
<p><img src="12-2-config-corp-t0-gw-havip.png" /></p>
<p>配置默认路由</p>
<p><img src="12-3-config-corp-t0-gw-default-route.png" /></p>
<p>Tier-0 上的 NAT 规则</p>
<p><img src="19-1-nat-corp-t0-gw.png" /></p>
<p>Tier-0 上的防火墙规则</p>
<p><img src="19-2-corp-t0-gw-firewall.png" /></p>
<p>还可以提前配置之后 route-based ipsec VPN 需要的 bgp 部分. 例如, 本地 AS:</p>
<p><img src="16-config-corp-t0-gw-bgp.png" /></p>
<p>bgp 邻居:</p>
<p><img src="15-config-corp-t0-gw-bgp-neighbor.png" /></p>
<p>正常更新源地址应该是 VPN tunnel interface 地址, 由于 VPN 配置还没迁移过来, 这个地址还不可用, 因此这里先空着.</p>
<p>bgp 重分布:</p>
<p><img src="17-config-corp-t0-gw-bgp-redistribute.png" /></p>
<p>下一篇将开始使用 MC 进行正式迁移.</p>
]]></content>
      <categories>
        <category>VMware</category>
        <category>NSX-T</category>
        <category>NSX-V</category>
      </categories>
      <tags>
        <tag>VMware</tag>
        <tag>NSX-T</tag>
        <tag>NSX-V</tag>
      </tags>
  </entry>
  <entry>
    <title>迁移 NSX-V 到 NSX-T: Migration Coordinator 方案 - 2 迁移</title>
    <url>/1379010243/</url>
    <content><![CDATA[<p>本文将完成使用 Migration Coordinator 基于用户自定义拓扑迁移现有的 NSX-V 到 NSX-T 环境的剩余部分</p>
<span id="more"></span>
<h2 id="启动-migration-coordinator-服务">启动 Migration Coordinator 服务</h2>
<p>默认 MC 服务并没有启动, 使用 ssh 登入 NSX-T Manager, 执行命令 <code>start service migration-coordinator</code> 将其启动起来. 然后就可以在 NSX-T Manager 的 Web UI 上访问了.</p>
<p>选择 "User Defined Topology"</p>
<p><img src="20-start-user-defined-topology-migration.png" /></p>
<p>选择迁移模式 "Complete Migration"</p>
<p><img src="21-complete-migration.png" /></p>
<h2 id="将-nsx-v-环境注册到-nsx-t">将 NSX-V 环境注册到 NSX-T</h2>
<p>添加 vCenter 和 NSX-V 的认证信息, 添加到 NSX-T 环境中</p>
<p><img src="22-add-vcenter-and-nsxv.png" /></p>
<h2 id="migration-coordinator-流程---导入拓扑">Migration Coordinator 流程 - 导入拓扑</h2>
<p>导入 NSX-V 拓扑</p>
<p><img src="23-import-configuration.png" /></p>
<p>然后可以查看导入的拓扑, 确认没有问题</p>
<p><img src="24-view-nsxv-topology.png" /></p>
<h2 id="migration-coordinator-流程---转化二层">Migration Coordinator 流程 - 转化二层</h2>
<p>然后转化二层:</p>
<p><img src="25-translate-layer-2-configuration.png" /></p>
<p>转化完会检测配置冲突, 有冲突需要用户手工消解的部分会提示:</p>
<p><img src="26-resolve-layer-2-issues.png" /></p>
<p>大部分情况下使用默认的冲突消解建议即可, 这里 Migrate or skip VLAN DVPGs missing segments 部分选择了 skip. 因为连接到物理世界的 segment 已经提前创建出来, 并且这个环境里其他的 DVPG 并不需要在 NSX-T 中使用</p>
<p><img src="27-skip-missed-vlan-segments.png" /></p>
<p>完成所有的冲突消解:</p>
<p><img src="28-resolve-layer-2-issues-done.png" /></p>
<p>然后就可以进行二层迁移:</p>
<p><img src="29-migration-layer-2-configuration.png" /></p>
<p>检查二层迁移结果:</p>
<p><img src="30-check-layer-2.png" /></p>
<p>二层迁移完成之后, transport node uplink profile, segment 之类都会自动创建出来.</p>
<h2 id="手工迁移负载均衡器配置">手工迁移负载均衡器配置</h2>
<p>接着我们可以从 migration coordinator 中退出来, 手工迁移负载均衡器相关的配置.</p>
<p>首先在 corp-t1-standalone 添加 service port 连接到 segment B:</p>
<p><img src="31-config-corp-t1-standalone-csp.png" /></p>
<p>为 corp-t1-standalone 添加默认路由:</p>
<p><img src="32-config-corp-t1-standalone-default-route.png" /></p>
<p>将 segment C 连接到 Tier-1 gateway (corp-t1-ext) 并设置网关</p>
<p><img src="33-attach-seg-c-to-corp-t1-ext.png" /></p>
<p>创建外部负载均衡器, 连接到 Tier-1 gateway (corp-t1-ext)</p>
<p><img src="34-create-ext-inline-lb.png" /></p>
<p>创建内部负载均衡器, 连接到 Tier-1 gateway (corp-t1-standalone)</p>
<p><img src="35-create-internal-lb.png" /></p>
<p>按照原有环境的配置, 创建 server pools, monitors, application profiles, application rules 和 virtual servers. 这里不再一一详细列出</p>
<p><img src="45-lb-all-virtualservers.png" /></p>
<h2 id="migration-coordinator-流程---转化三到七层">Migration Coordinator 流程 - 转化三到七层</h2>
<p>接着回到 migration coordinator 界面, 继续三到七层的配置. 首先是定义拓扑, 选择 "Select a Tier-0/Tier-1 Gateway for each entity that needs to be migrated":</p>
<p><img src="46-topology-mapping.png" /></p>
<p>将 nsx-ldr 对应到 corp-t0-gw, 这是因为我们需要将 segment A 和 segment B 连接到 Tier-0 上. 将 nsx-esg 对应到 corp-t0-gw, 这是因为我们需要将 IPSec VPN 的配置迁移过去. nsx-internal-lb 不设置映射, 因为我们已经手动配置好了负载均衡器.</p>
<p>确认不迁移未设置映射的 ESG</p>
<p><img src="47-skip-unmapped.png" /></p>
<p>然后转化三到七层配置:</p>
<p><img src="48-translate-L3-L4-L7.png" /></p>
<p>和二层类似, 用户也需要手工消解三到七层配置转化中发现的冲突:</p>
<p><img src="53-resolve-l3-l4-l7-issues.png" /></p>
<p>非建议选项: 跳过 vIDM, 这个环境里面不需要</p>
<p><img src="54-skip-vIDM.png" /></p>
<p>其他基本上都可以选择建议的配置, 完成所有冲突消解:</p>
<p><img src="56-resolve-l3-l4-l7-done.png" /></p>
<p>开始迁移三到七层配置:</p>
<p><img src="57-migrate-l3-l4-l7-configuration.png" /></p>
<p>完成后检查三到七层配置:</p>
<p><img src="58-check-l3-l4-l7-realization.png" /></p>
<p>此时 segment A 和 B 会连接到 Tier-0 gateway, IPSec VPN 的配置也会产生. 到此为止, 现存 NSX-V 环境的流量还不会受到影响</p>
<h2 id="migration-coordinator-流程---迁移-edge">Migration Coordinator 流程 - 迁移 Edge</h2>
<p>接着开始迁移 Edge. 从这个阶段开始, 现存环境的流量将开始中断.</p>
<p><img src="59-migrate-edge.png" /></p>
<p>迁移完 Edge 后, 就可以断开 NSX-V 环境 nsx-esg 的网络连接, 并修正 NSX-T VLAN segment 的 VLAN ID 了.</p>
<h2 id="migration-coordinator-流程---迁移-host">Migration Coordinator 流程 - 迁移 Host</h2>
<p>根据需要选择主机迁移的并行度</p>
<p><img src="59-migrate-host-plan.png" /></p>
<p>开始迁移主机, 并等待迁移结束</p>
<p><img src="62-migrate-host-done.png" /></p>
<p>迁移时会先卸载主机上的 NSX-V 模块, 然后再安装 NSX-T 模块, 进行主机准备. 而且只有在迁移主机时, 才会将其上运行的虚拟机的 vNIC 重新连接到 NSX-T 的 segment, 因此虚拟机东西向流量完全恢复要等到所有主机迁移完成. 根据主机数量的多少和并行度的设置, 这个过程可能会长达几十分钟甚至数小时.</p>
<h2 id="手工修复环境">手工修复环境</h2>
<p>最后手工修复剩下的部分</p>
<ul>
<li>刷新物理路由器上 NSX-T 外部地址的 ARP 缓存条目. 因为我们保留了相同的外部地址, 物理路由器上可能还留着 NSX-V 对应的条目. 可以触发 NSX-T Tier-0 gateway 的 failover 来发送 GARP 刷新缓存或者直接在物理路由器上清除 ARP 缓存</li>
<li>修复 corp-t0-gw 上 bgp 邻居配置中的更新源地址, 此时 VPN tunnel 正常应该已经起来了, 使用 VPN tunnel 地址作为更新源</li>
<li>检查 MC 自动产生的迁移对象, 如果不需要的可以删除</li>
</ul>
<p>最后检查迁移完成后的环境, 确保路由/负载均衡器/VPN等基本功能都没有问题:</p>
<p><img src="63-post-check-vpn-ok.png" /></p>
<p><img src="64-post-check-vs-ok.png" /></p>
<p>最后清除剩下的 NSX-V 环境.</p>
]]></content>
      <categories>
        <category>VMware</category>
        <category>NSX-T</category>
        <category>NSX-V</category>
      </categories>
      <tags>
        <tag>VMware</tag>
        <tag>NSX-T</tag>
        <tag>NSX-V</tag>
      </tags>
  </entry>
  <entry>
    <title>迁移 NSX-V 到 NSX-T: Lift &amp; Shift - 1 部署目标环境</title>
    <url>/302121944/</url>
    <content><![CDATA[<p>本系列将介绍如何使用 Lift &amp; Shift + Edge 桥接的方式来迁移现存的 NSX-V 环境到 NSX-T. 第一部分将先创建新的 NSX-T 环境. 和前面类似, 此迁移将保证所有的外部地址/DNS条目/VPN peer 配置保持不变. <span id="more"></span></p>
<h2 id="迁移步骤概要">迁移步骤概要</h2>
<p>在同一个 vCenter 下创建一个新的 vDC, 配置好基础的主机/存储/网络. 然后在这个新的 vDC 下创建和 NSX-V 功能等价的 NSX-T 拓扑. 为了避免对外部环境和现有环境的影响, 部分配置需要细微调整:</p>
<ul>
<li>连接到物理环境的 VLAN backed segment 可暂时使用错误的 VLAN id (0) 来实现和物理环境的隔离</li>
<li>所有 NSX-T overlay segment 配置网关, 但是不连接到任何 gateway. 因为桥接之后网关在 NSX-V 那端</li>
<li>内部负载均衡器暂时不连接到 gateway</li>
</ul>
<p><img src="V2T-bridge.png" /></p>
<p>这里有些做法和 NSX migration guide 并不完全一致. 实际上现实环境各种各样, 文档无法面面俱到. 理解自己的环境, 选择最合适的方案.</p>
<p>对于桥接, 需要在 NSX-V 所在主机上单独部署 NSX-T Edge. 此时该主机将进行 vxlan 封装/解封装, 其上运行的 Edge 能再进行 geneve 封装/解封装, 以完成两个环境的对接. 每一个逻辑网段的桥接需要一个单独的 NSX-T Edge 集群.</p>
<p>配置完桥接以后就可以使用 enhanced vMotion 将工作负载从原先的 vDC 迁移到新的 vDC 并使用 NSX-T 的逻辑网段. 这个过程网络流量不会受到影响. 迁移完后将前面临时调整的部分进行修正:</p>
<ul>
<li>修正 NSX-T 连接到物理世界 VLAN segment 的 VLAN id</li>
<li>将 NSX-T overlay segment 连接到 gateway</li>
<li>将内部负载均衡器连接到 gateway</li>
</ul>
<h2 id="创建-nsx-t-拓扑">创建 NSX-T 拓扑</h2>
<h3 id="准备传输节点和承载东西向流量的-edge-集群">准备传输节点和承载东西向流量的 Edge 集群</h3>
<p>常规步骤添加 compute manager, 创建 transport node TEP pool, edge node TEP pool, transport node uplink profile, edge node uplink profile, overlay transport zone, vlan transport zone. 然后部署并配置用于东西向流量的 Edge 集群, 创建 transport node profile 完成主机准备. 不再赘述.</p>
<p><img src="11-prepare-transport-node.png" /></p>
<h3 id="创建并配置-tier-0tier-1">创建并配置 Tier-0/Tier-1</h3>
<p>先创建连接到物理环境的 VLAN-backed segment, 暂时使用 VLAN id 0 来隔离 NSX-T 和外部物理环境</p>
<p><img src="12-physical-segment.png" /></p>
<p>创建 Tier-0: corp-t0-gw</p>
<p><img src="14-create-corp-t0-gw.png" /></p>
<p>配置 Tier-0 的外部接口</p>
<p><img src="15-corp-t0-gw-external-interfaces.png" /></p>
<p>配置 Tier-0 的 HA VIP</p>
<p><img src="16-corp-t0-gw-ha-vip.png" /></p>
<p>配置 Tier-0 的默认路由</p>
<p><img src="18-corp-t0-gw-default-route.png" /></p>
<p>之后 route-based IPSec VPN 所使用的 bgp 配置也可以提前做出来:</p>
<p><img src="21-corp-t0-gw-bgp.png" /></p>
<p><img src="22-corp-t0-gw-bgp-neighbor.png" /></p>
<p>更新源地址暂时不可用, 可以先空着. Tier-0 上的 gateway 防火墙和 NAT 规则也可以先创建出来:</p>
<p><img src="24-gateway-firewall.png" /></p>
<p><img src="25-corp-t0-gw-nat-rule.png" /></p>
<p>然后创建 overlay segments, 设置网关地址, 但是不连接到任何 Tier-0/Tier-1 gateway:</p>
<p><img src="26-all-segments.png" /></p>
<p>创建 Tier-1: corp-t1-ext 连接到 corp-t0-gw, 路由发布 connected subnets + LB VIP:</p>
<p><img src="23-create-corp-t1-ext.png" /></p>
<p>创建 Tier-1: corp-t1-standalone 不连接到任何 Tier-0:</p>
<p><img src="27-create-corp-t1-standalone.png" /></p>
<p>在 corp-t1-standalone 上创建 service interface 连接到 Seg-B, 并设置默认路由:</p>
<p><img src="28-corp-t1-standalone-csp.png" /></p>
<p><img src="29-corp-t1-standalone-default-route.png" /></p>
<p>这里注意一下, 因为 NSX-V 的 Seg-B 和 NSX-T 的 Seg-B 后面会桥接起来, 这个 service interface 地址不要和现有环境中的地址冲突了. 后面切换完成后, 如果想改成之前使用的地址也是可以的.</p>
<h3 id="创建并配置负载均衡器">创建并配置负载均衡器</h3>
<p>创建外部负载均衡器, 连接到 corp-t1-ext:</p>
<p><img src="30-create-external-lb.png" /></p>
<p>创建内部负载均衡器, 不连接到任何 Tier-1:</p>
<p><img src="32-create-internal-lb-detached.png" /></p>
<p>然后参照现有的环境创建 virtual servers, monitors, server pools, application profiles, application rules 等, 不再赘述</p>
<p><img src="42-lb-virtual-servers.png" /></p>
<p>这里内部负载均衡器上的 virtual servers 可以直接使用原始地址. 因为内部负载均衡器没有附加到 corp-t1-standalone, 后面即使做了桥接也不会有地址冲突的问题. 外部负载均衡器上的 virtual servers 也可以直接使用原始地址, 这是因为 Seg-C 还没有连接到 corp-t1-ext, 因此即使 Seg-C 做了桥接也不会有地址冲突.</p>
<h3 id="创建并配置-ipsec-vpn">创建并配置 IPSec VPN</h3>
<p>参照现有的 VPN 配置, 对应地在 Tier-0 上创建 IPSec VPN service:</p>
<p><img src="43-create-ipsec-vpn-service.png" /></p>
<p>创建 VPN endpoint, 使用和 NSX-V 一致的地址来保证 VPN peer 配置不变:</p>
<p><img src="44-create-ipsec-vpn-endpoint.png" /></p>
<p>创建和 NSX-V 环境一致的 IKE profile + IPSec profile</p>
<p><img src="45-create-ike-profile.png" /></p>
<p><img src="46-create-ipsec-profile.png" /></p>
<p>最后创建 VPN 会话, 选择前面自定义的 IKE profile + IPSec profile. 由于目前是隔离状态, VPN 会话是 down 的状态</p>
<p><img src="47-create-ipsec-session.png" /></p>
]]></content>
      <categories>
        <category>VMware</category>
        <category>NSX-T</category>
        <category>NSX-V</category>
      </categories>
      <tags>
        <tag>VMware</tag>
        <tag>NSX-T</tag>
        <tag>NSX-V</tag>
      </tags>
  </entry>
  <entry>
    <title>迁移 NSX-V 到 NSX-T: Lift &amp; Shift - 2 部署 Edge Bridge 并迁移</title>
    <url>/3148157594/</url>
    <content><![CDATA[<p>继续介绍如何使用 Lift &amp; Shift + Edge 桥接的方式来迁移现存的 NSX-V 环境到 NSX-T. 这部分将准备新的 NSX-T Edge 集群将 NSX-V 和 NSX-T 的逻辑网段进行桥接, 然后迁移工作负载. <span id="more"></span></p>
<h2 id="创建并配置桥接所用的-nsx-t-edge-集群">创建并配置桥接所用的 NSX-T Edge 集群</h2>
<h3 id="准备主机端口组">准备主机端口组</h3>
<p>在原始 NSX-V 环境创建 VDS Trunk 端口组:</p>
<p><img src="48-prepare-trunking-port-group.png" /></p>
<p>并且在需要桥接的 NSX-V virtualwire 端口组安全设置打开 Promiscuous mode + Forged transmit:</p>
<p><img src="49-prepare-segment-security.png" /></p>
<p>实际上也可以开启 MAC learning 而不使用 Promiscuous. 只是在 vSphere 7 环境下开启 MAC learning 只能通过 API 完成, 而 vSphere 8 则提供了图形界面的方式来启用 MAC learning.</p>
<div class="note info"><p>开启 Promiscuous 会对网络性能产生一定影响, 如果无法接受则需使用 MAC learning</p>
</div>
<h3 id="部署并配置-bridge-edge-集群">部署并配置 Bridge Edge 集群</h3>
<p>在原始启用了 NSX-V 的主机集群上部署 NSX-T Edge 节点, 第一个 vNIC 连接管理网络, 第二个 vNIC 连接 Trunk VDS 端口组, 第三个 vNIC 连接目标 NSX-V 逻辑网段:</p>
<p><img src="50-bridge-edge-networking-seg-a.png" /></p>
<p>每一个需要桥接的逻辑网段需要一个 Bridge Edge Cluster. 这里总共有三个逻辑网段, 因此需要三个 Bridge Edge Cluster. 每个 Cluster 里至少有一个 Edge Node (如果做高可用则需要两个)</p>
<p>然后配置 Bridge Edge 节点. 每个 Bridge Edge 里将运行两个 NVDS, 一个用于 NSX-T 的 overlay 一个用于桥接, 因此对应的需要两种不同的上行链路配置.</p>
<p>首先是 overlay 的上行链路配置:</p>
<p><img src="53-bridge-overlay-uplink.png" /></p>
<p>然后是桥接的上行链路配置, transport vlan id 设置为 0:</p>
<p><img src="54-bridge-uplink.png" /></p>
<p>配置 Bridge Edge 节点, 每个 Edge 节点内创建两个 NVDS: 一个用于 NSX-T 的 overlay 流量, 对应使用的是 overlay 的上行配置</p>
<p><img src="55-edge-overlay-nvds.png" /></p>
<p>另一个用于桥接流量, 对应使用桥接的上行</p>
<p><img src="56-edge-bridge-nvds.png" /></p>
<p>然后为每个需要桥接的逻辑网段创建 Bridge Edge Cluster. 这里创建了三个单节点集群.</p>
<h3 id="创建桥接连接-nsx-v-和-nsx-t-逻辑网段">创建桥接连接 NSX-V 和 NSX-T 逻辑网段</h3>
<p>创建 bridge profile, 关联对应的 Edge cluster, 并选择 primary node</p>
<p><img src="59-bridge-profile.png" /></p>
<p>每个需要桥接的逻辑网段都需要一个 bridge profile 关联到对应的 Edge cluster. 最后一共是三个配置:</p>
<p><img src="60-all-bridge-profiles.png" /></p>
<p>然后到 NSX-T 的各个 segment 下配置桥接: Additional Settings -&gt; Edge Bridges, ADD EDGE BRIDGE</p>
<p><img src="61-bridge-to-segment.png" /></p>
<p>选择对应的 bridge profile, 选择 vlan transport zone, vlan id 使用 0.</p>
<p>桥接配置到此就完成了, 可以用 ssh 登入到各个 Bridge Edge 节点检查服务是否正常:</p>
<p><img src="63-get-bridge.png" /></p>
<p>然后可以迁移几个测试虚拟机来验证桥接的连通性. 到此为止, 所有的准备和验证工作都可以在迁移之前完成, 而不会影响到现有的工作负载.</p>
<h2 id="迁移工作负载并切换网络">迁移工作负载并切换网络</h2>
<p>到了真正迁移的时间, 使用 enhanced vMotion 将所有工作负载迁移到新的 vDC 下, 迁移时目的地选择 NSX-T 的逻辑网段. 取决于工作负载的规模, 这个过程可能会很长. 但是如果前面桥接配置正确, 则这段时间内业务流量不会受到影响.</p>
<p>然后就需要做网关的切换和最后的调整, 这部分配置修改会导致流量短暂中断</p>
<ul>
<li>断开 NSX-V 侧所有 ESG/LDR 的连接</li>
<li>将 NSX-T 所有 overlay segment 连接到网关: Seg-A/B 连接到 corp-t0-gw, Seg-C 连接到 corp-t1-ext. 将 internal-lb 连接到 corp-t1-standalone</li>
<li>修正 NSX-T VLAN segment 的 vlan id: 0 -&gt; 11</li>
<li>刷新物理路由器上 Tier-0 的 ARP 缓存</li>
<li>修改 Tier-0 上 bgp 邻居更新源, 使用 VPN tunnel 地址</li>
</ul>
<p>这些步骤即使手工操作也可以在五分钟内完成. 如果提前准备了 API 调用脚本则可以进一步缩短流量中断时间. 迁移完成并验证后就可以清理 NSX-V 环境了.</p>
<h2 id="关于特定工作负载的考量">关于特定工作负载的考量</h2>
<p>相对于就地迁移方案, Lift &amp; Shift + 桥接的方案虽然更加复杂, 但是需要的停机时间会显著缩短. 对维护停机时间有高度要求的企业可能会更倾向于这种方案. 然而企业在选择迁移方案时还需要考虑迁移方案对上层工作负载带来的影响.</p>
<p>Lift &amp; Shift + 桥接的方法会将工作负载迁移到一个新的 vDC 上, 对应的就是一套新的主机/存储/网络环境. 大部分情况下这种变化对上层工作负载是透明的, 然而却并不总是这样. 现在很多容器平台和底层云/虚拟化平台有深度的整合. 如果在容器平台之外修改了底层云/虚拟化环境, 那么容器平台的 cloud provider 配置可能会处于不一致/不可用的状态. 此外对于容器平台某些类型的持久卷, 直接迁移其底层的存储也可能会引起持久卷丢失. 凡此种种都是迁移规划和方案选择时需要考虑的.</p>
]]></content>
      <categories>
        <category>VMware</category>
        <category>NSX-T</category>
        <category>NSX-V</category>
      </categories>
      <tags>
        <tag>VMware</tag>
        <tag>NSX-T</tag>
        <tag>NSX-V</tag>
      </tags>
  </entry>
</search>
